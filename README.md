# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

# ON WORKING

# ML news: 

## Research
|Link|description|
|---|---|
|[LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs.](https://arxiv.org/abs/2406.15319) | claims to achieve 64.3% on HotpotQA (full-wiki), which is on par with the state-of-the-art model. proposes LongRAG, which combines RAG with long-context LLMs to enhance performance; uses a long retriever to significantly reduce the number of extracted units by operating on longer retrieval units; the long reader takes in the long retrieval units and leverages the zero-shot answer extraction capability of long-context LLMs to improve performance of the overall system. |
|[From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data.](https://arxiv.org/abs/2406.19292) |suggests a fine-tuning strategy to increase the precision of information retrieval in LLMs while preserving reasoning abilities over long-context inputs; the fine-tuning dataset consists of 350 sample numerical dictionary key-value retrieval tasks; results show that this strategy reduces the "lost-in-the-middle" effect and enhances performance on both long-context reasoning and information retrieval. |
|[GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models.](https://arxiv.org/abs/2406.14550v1) |enhances the long-context capabilities of LLMs by proposing a graph-based agent system that organizes long text into a graph and uses an agent to explore the graph (using predefined functions guided by a step-by-step rational plan) to efficiently generate answers to questions; consistently outperforms GPT-4-128k across context lengths ranging from 16k to 256k. |
|[Following Length Constraints in Instructions.](https://arxiv.org/abs/2406.17744) |explains a method for addressing length bias and training language models that adhere to length constraints more closely; it refines a model using DPO using a dataset that has been augmented with length instructions and demonstrates fewer length constraint violations while maintaining a high response quality. |
|[Adam-mini: Use Fewer Learning Rates To Gain More.](https://arxiv.org/abs/2406.16793) | a new optimizer that carefully divides parameters into blocks and assigns a single high-quality learning that outperforms Adam; it achieves consistent results on language models sized from 125M -7B for pre-training, SFT, and RLHF. It uses fewer learning rates, which results in a 45%–50% reduction in memory footprint while still performing on par or even better than AdamW.|
|[MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data.](https://arxiv.org/abs/2406.18790) |generative image model with better performance than pure text conditioned models due to its ability to interleave text and images. |
|[Scaling Synthetic Data Creation with 1,000,000,000 Personas.](https://arxiv.org/abs/2406.20094) |By treating web text as originating from a persona, this approach can significantly enhance job performance downstream by conditioning on that persona. The researchers find a jump of 20% points on MATH. |
|[Odd-One-Out: Anomaly Detection by Comparing with Neighbors.](https://arxiv.org/abs/2406.20099v1) | A novel anomaly detection challenge has been presented by researchers that focuses on things that appear unusual in comparison to other objects in the scene. In contrast to conventional techniques, anomalies in this case are distinctive to the scene and can be determined from several angles.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[An Update to Adept.](https://www.adept.ai/blog/adept-update) |The founders of Adept are heading to Amazon to license some of their technology. |
|[Time strikes a deal to funnel 101 years of journalism into OpenAI's gaping maw.](https://www.engadget.com/time-strikes-a-deal-to-funnel-101-years-of-journalism-into-openais-gaping-maw-144058426.html) | Time has joined a growing number of publications to sign a licensing deal with OpenAI. The ChatGPT creator will legally be able to train its large language models on 101 years worth of the storied publication's journalism, as Axios first reported.|
|[Amazon Investigates Perplexity AI Over Potential Data-Scraping Violations.](https://www.pcmag.com/news/amazon-investigates-perplexity-ai-over-potential-data-scraping-violations) | Amazon Web Services is looking into whether Perplexity is breaking its rules after Wired said the AI startup is swiping its web archives without consent. Perplexity, however, says it's following the rules.|
|[Apple could announce a Google Gemini deal this fall .](https://www.theverge.com/2024/6/30/24189262/apple-intelligence-google-gemini-deal-iphone-mac-ipad-openai-chatgpt) | If you’re disappointed that the only AI model that will integrate with Apple devices so far will be ChatGPT, it sounds like you won’t have to wait long for that to change. Apple will announce “at least” one other deal — to add Google Gemini, too — this fall.|
|[Meta accused of breaking EU digital law by charging for ad-free social networks.](https://www.theguardian.com/technology/article/2024/jul/01/meta-facebook-instagram-eu-digital-markets-act) |European Commission objects to ‘pay or consent’ model for users of Facebook and Instagram |
|[Microsoft’s Mustafa Suleyman says he loves Sam Altman, believes he’s sincere about AI safety.](https://techcrunch.com/2024/06/25/microsofts-mustafa-suleyman-says-he-loves-sam-altman-believes-hes-sincere-about-ai-safety/) |In an interview at the Aspen Ideas Festival on Tuesday, Mustafa Suleyman, CEO of Microsoft AI, made it very clear that he admires OpenAI CEO Sam Altman. |
|[When the Terms of Service Change to Make Way for A.I. Training.](https://www.nytimes.com/2024/06/26/technology/terms-service-ai-training.html) | As they negotiate a complicated web of privacy regulations and user consent, tech giants like Google and Meta are revising their privacy rules to allow the use of public and potentially private user data to train AI systems. There have been backlash since consumers and content creators are afraid that their work will be used to train AI that may eventually replace them. The conflicts draw attention to new issues in data privacy, AI development, and striking a balance between innovation and morality in the IT sector.|
|[Meet Figma AI.](https://www.figma.com/blog/introducing-figma-ai) |Designers may get assistance with tasks like visual search, asset search, text editing, image editing, prototyping, layer renaming, and design generation with Figma AI, a new suite of AI-powered capabilities for Figma. During the beta phase, these features—which are driven by AI models from third parties—are free to use. |
|[Google’s emissions climb nearly 50% in five years due to AI energy demand.](https://www.theguardian.com/technology/article/2024/jul/02/google-ai-emissions) |Tech giant’s goal of reducing climate footprint at risk as it grows increasingly reliant on energy-hungry data centres |
|[Amazon beefs up AI development, hiring execs from startup Adept and licensing its technology.](https://www.cnbc.com/2024/06/28/amazon-hires-execs-from-ai-startup-adept-and-licenses-its-technology.html) |Amazon has hired top executives from AI agent startup Adept, the company confirmed. As part of the deal, Amazon will license technology from Adept, including some of its AI models and datasets. Amazon has been trying to keep pace with competitors in AI by developing services and through its investment in OpenAI competitor Anthropic.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees.](https://arxiv.org/abs/2406.16858) |improves the long-context capabilities of LLMs by putting forth a graph-based agent system that efficiently generates answers to questions by organizing long text into a graph and employing an agent to explore the graph (using predefined functions guided by a step-by-step reasonable plan); surpasses GPT-4-128k with consistency in context lengths between 16k and 256k. |
|[On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey.](https://arxiv.org/abs/2406.15126) |survey on LLM-based synthetic data generation, curation, and evaluation. |
|[Text2Bricks: Fine-tuning Open-Sora in 1,000 GPU Hours.](https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky) | Lambda Labs trained the Open Sora video model on its 1-click cluster to create Lego movies.|
|[Laplace Neural Operator.](https://github.com/qianyingcao/Laplace-Neural-Operator) |One architecture for approximating PDEs that is based on neural networks is the Laplace operator. |
|[llama-agents.](https://github.com/run-llama/llama-agents) |llama-agents is an async-first framework for building, iterating, and productionizing multi-agent systems, including multi-agent communication, distributed tool execution, human-in-the-loop, and more! |
|[Suri: Multi-constraint Instruction Following for Long-form Text Generation.](https://chtmp223.github.io/suri/) |A collection of 20,000 lengthy documents and intricate instructions is called Suri. Its goal is to enhance AI's capacity to adhere to intricate writing requirements. The Suri development team has presented Instructional ORPO (I-ORPO), an alignment technique that provides feedback through artificially damaged instructions. |
|[Cambrian-1.](https://cambrian-mllm.github.io/) | High-performing, fully open vision model from NYU with significant improvements over text encoders and data mixtures.|
|[DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling on Time Variability.](https://arxiv.org/abs/2406.19135v1) |A novel expressive text-to-speech (TTS) model called DEX-TTS makes use of reference speech to enhance style representation and model generalization. |
|[Debugging in PyTorch.](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide3/Debugging_PyTorch.html) |PyTorch is an excellent modeling tool. Nonetheless, a few prevalent issues have the ability to significantly lower model performance. Examining this list will aid you when debugging your model code. |
|[vision-agent.](https://github.com/landing-ai/vision-agent) | Vision Agent is a library that helps you utilize agent frameworks to generate code to solve your vision task.|
|[What to do to scale up?](https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5) | An amazing and surprisingly understandable post about fine-tuning hyperparameters as model and dataset sizes increase.|
|[Web2Code.](https://mbzuai-llm.github.io/webpage2code/) | A novel procedure that researchers have created will enhance Web2Code instruction tweaking. It entails generating new text question-answer pairs, generating new webpage image-code pairs, improving webpage understanding data, and developing new webpage code generation pairs.|
|[Block Transformer: Global-to-Local Language Modeling for Fast Inference.](https://github.com/itsnamgyu/block-transformer) | This repository presents a brand-new Transformer type with a significantly smaller KV cache size. Although it hasn't been tested in large quantities, it should be able to perform on par with typical Transformers.|
|[Composio.](https://github.com/ComposioHQ/composio) | Equip your agent with high-quality tools & integrations without worrying about authentication, accuracy, and reliability in a single line of code!|
|[Segment Anything without Supervision.](https://github.com/frank-xwang/unsam) |Unsupervised SAM (UnSAM) is a 'segment anything' model for promptable and automatic whole-image segmentation which does not require human annotations. |
|[Following Length Constraints in Instructions.](https://github.com/facebookresearch/RAM/tree/main/projects/length_instruct) | Most models don't adhere to length specifications (less than 40 words, for example). This piece demonstrates how to tune them to do that.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[Smudgy chins, weird hands, dodgy numbers: seven signs you’re watching a deepfake.](https://www.theguardian.com/technology/article/2024/jul/01/seven-signs-deepfake-artificial-intelligence-videos-photographs) | Look out for surplus fingers, compare mannerisms with real recordings and apply good old-fashioned common sense and scepticism, experts advise|
|[Training MoEs at Scale with PyTorch.](https://pytorch.org/blog/training-moes/) |In order to write about scaling their MoE models to thousands of GPUs, the Mosaic team has teamed up with PyTorch. |
|[Investing in the Age of Generative AI.](https://eastwind.substack.com/p/investing-in-the-age-of-generative) |Though there is currently a "euphoria" surrounding investment, the generative AI business is already showing signs of fragility. |
|[Can AI boom drive Nvidia to a $4tn valuation despite investor doubt?](https://www.theguardian.com/technology/article/2024/jul/02/can-ai-boom-drive-nvidia-to-a-4tn-valuation-despite-investor-doubt) |Powerful new chips are on the way but there are questions over whether tech firm’s growth can be sustained |
|[AI scaling myths.](https://www.aisnakeoil.com/p/ai-scaling-myths) |It is improbable that LLMs will ever be able to achieve AGI through scaling on its own. Although scaling has been found to improve model capabilities, it largely improves confusion instead of emergent skills. Getting hold of high-quality training data is getting harder and harder. |
|[A discussion of discussions on AI bias.](https://danluu.com/ai-bias/) | The nature of AI bias has come under more scrutiny, with detractors claiming that biases in machine learning are demonstrated by the way models like as Playground AI occasionally change a user's ethnicity in photos. Some users refute this as a flaw or pertinent prejudice, pointing to instances in which Asian traits are overrepresented. The discussion touches on the wider ramifications of AI bias in many businesses. There is no easy answer to this complicated problem.|
|[The shape of information.](https://kucharski.substack.com/p/the-shape-of-information) |This article describes how to use binary logic to maximize scarce resources. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

# ML news: Week 24 - 30 June

## Research
|Link|description|
|---|---|
|[Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?](https://arxiv.org/abs/2406.13121) |reports that long-context LLMs can compete with state-of-the-art retrieval and RAG systems without explicit training on the tasks; suggests that compositional reasoning (needed in SQL-like tasks) is still challenging for these LLMs; and encourages further research on advanced prompting strategies. performs a thorough performance analysis of long-context LLMs on in-context retrieval and reasoning. first presents a benchmark with real-world tasks requiring 1M token context. |
|[PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers.](https://arxiv.org/abs/2406.12430) | improves decision-making using the iterative plan-then-RAG (PlanRAG) technique, which consists of two steps: The last phase determines whether a new plan for additional analysis is required and repeats earlier steps or makes a decision based on the data. 1) An LM creates the plan for decision making by reviewing the questions and data schema, and 2) the retriever creates the queries for data analysis; It is discovered that PlanRAG performs better than iterative RAG on the suggested Decision QA tasks. |
|[Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs.](https://arxiv.org/abs/2406.10209) | demonstrates how the goldfish loss resists memorization and keeps the model useful, but it may need to train for longer to more effectively learn from the training data. It is a modification of the next-token prediction objective called goldfish loss, which helps mitigate the verbatim generation of memorized training data. It uses a simple technique that excludes a pseudorandom subset of training tokens at training time.|
|[Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B.](https://arxiv.org/abs/2406.07394v2) | report having used an approach that combines LLMs with Monte Carlo Tree Search to achieve a mathematical Olympiad solution at the GPT-4 level. This approach aims to improve the system's performance in mathematical reasoning by enabling features like systematic exploration, self-refinement, and self-evaluation.|
|[From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries.](https://arxiv.org/abs/2406.12824) |aims to better understand how LLMs use external knowledge in place of parametric information when responding to factual queries. It finds that in a RAG pipeline, LLMs take a "shortcut" and exhibit a strong bias toward using only the context information and their parametric memory to answer the question. |
|[Tree Search for Language Model Agents.](https://jykoh.com/search-agents/paper.pdf) | reveals that performance scales with increased test-time computing. It is tested on interactive online environments and applied to GPT-4o to dramatically enhance performance. It suggests an inference-time tree search technique for LM agents to do exploration and enable multi-step reasoning.|
|[Evidence of a log scaling law for political persuasion with large language models.](https://arxiv.org/abs/2406.14508) |Super persuasion is the worry that models may become noticeably more persuasive as they get bigger. The idea that larger models aren't significantly more compelling than smaller models isn't supported by strong data. They might, nevertheless, be able to be adjusted to be more convincing. |
|[MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading.](https://arxiv.org/abs/2406.14537v1) |Reinforcement learning is used in MacroHFT, a novel method of high-frequency trading (HFT) in cryptocurrency markets, to enhance profitability and decision-making. |
|[Soft-QMIX: Integrating Maximum Entropy For Monotonic Value Function Factorization.](https://arxiv.org/abs/2406.13930v1) |Researchers have included a local Q-value learning method within a maximum entropy framework to enhance QMIX, a well-liked multi-agent reinforcement learning technique. |
|[eaL: Efficient RLHF Training for LLMs with Parameter Reallocation.](https://github.com/openpsi-project/realhf) |ReaLHF is a unique method that optimizes parallelization during training and dynamically redistribute parameters to improve reinforcement learning from human input (RLHF). |
|[AlphaFold2 structures guide prospective ligand discovery.](https://www.science.org/doi/10.1126/science.adn6354) | AlphaFold2 (AF2) models have had wide impact but mixed success in retrospective ligand recognition. We prospectively docked large libraries against unrefined AF2 models of the σ2 and serotonin 2A (5-HT2A) receptors, testing hundreds of new molecules and ...|
|[GPTs are GPTs: Labor market impact potential of LLMs.](https://www.science.org/doi/10.1126/science.adj0998) |OWe propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs. When accounting for current and likely future software developments that complement LLM capabilities, this share jumps to just over 46% of jobs. |
|[Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models.](https://arxiv.org/abs/2406.14848v1) | PE-Rank is a novel passage ranking method that leverages context compression through single passage embeddings to increase performance.|
|[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression.](https://github.com/thu-nics/moa) |By customizing sparse attention configurations for each head and layer, the Mixture of Attention (MoA) method maximizes sparse attention in large language models. |
|[GeoMFormer: A General Architecture for Geometric Molecular Representation Learning.](https://arxiv.org/abs/2406.16853v1) | A new Transformer-based model called GeoMFormer learns both equivariant and invariant properties to enhance molecular modeling.|
|[Making my local LLM voice assistant faster and more scalable with RAG.](https://johnthenerd.com/blog/faster-local-llm-assistant/) | Researchers classified data, precomputed embeddings, and dynamically generated examples to improve the efficiency and scalability of an LLM voice assistant.|
|[Retrieval Augmented Instruction Tuning for Open NER with Large Language Models.](https://arxiv.org/abs/2406.17305v1) | Using big language models, Retrieval Augmented Instruction Tuning (RA-IT) enhances information extraction.|
|[Data curation via joint example selection further accelerates multimodal learning.](https://arxiv.org/abs/2406.17711) |In pre-training, actively choosing the next best batch is a difficult and open problem. This research from DeepMind investigates how to match SOTA for a variety of tasks while using only 10% of FLOPs and hard mining negative samples. |
|[Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text.](https://imlixinyang.github.io/director3d-page/) | A system called Director3D was created to improve camera trajectory modeling and 3D scene production in the real world. Director3D creates lifelike 3D scenes from text descriptions by using a Multi-view Latent Diffusion Model and a Trajectory Diffusion Transformer.|
|[Prompt Engineering Tool.](https://github.com/teknium1/Prompt-Engineering-Toolkit) |An excellent prompting toolset that helps evaluate the effectiveness of various prompts, nearly completely composed of Sonnet 3.5. |
|[Meta Large Language Model Compiler: Foundation Models of Compiler Optimization.](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/) | Two language models that can decompile to LLVM IR and compile code to assembly have been made available by Meta. They received additional training after being trained on 546 billion tokens of superior quality data. They can accomplish 45% round trip disassembly performance and 77% optimized assembling performance.|

## News
|Link|description|
|---|---|
|[Geologists raise concerns over possible censorship and bias in Chinese chatbot.](https://www.theguardian.com/technology/article/2024/jun/24/geologists-censorship-bias-chinese-chatbot-geogpt) | GeoGPT developed as part of Chinese-funded earth sciences programme aimed at researchers in global south|
|[OpenAI acquires Rockset.](https://openai.com/index/openai-acquires-rockset) |Rockset is a robust database that supports both indexing and querying. The startup was acquired by OpenAI in order to enhance its infrastructure for retrieval. |
|[Snapchat AI turns prompts into new lens.](https://www.theverge.com/2024/6/19/24181965/snapchat-ai-prompt-custom-lens) | Snapchat’s upcoming on-device AI model could transform your background — and your clothing — in real time.|
|[HeyGen Raises $60M Series A to Scale Visual Storytelling for Businesses.](https://www.heygen.com/article/announcing-our-series-a) |HeyGen, an AI video generating platform, has raised $60 million in Series A funding to improve its studio-quality video creation and localization capabilities quickly and affordably. HeyGen, which just generated $35 million in ARR, strives to democratize visual storytelling for companies of all sizes. |
|[AI candidate running for Parliament in the U.K. says AI can humanize politics.](https://www.nbcnews.com/tech/tech-news/ai-candidate-running-parliament-uk-says-ai-can-humanize-politics-rcna156991) |Voters can talk to AI Steve, whose name will be on the ballot for the U.K.'s general election next month, to ask policy questions or raise concerns. |
|[Anthropic has a fast new AI model — and a clever new way to interact with chatbots .](https://www.theverge.com/2024/6/20/24181961/anthropic-claude-35-sonnet-model-ai-launch) |Claude 3.5 Sonnet is apparently Anthropic’s smartest, fastest, and most personable model yet. |
|[AIs are coming for social networks.](https://www.theverge.com/2024/6/18/24181196/butterflies-app-ai-chatbots-social-media) | An app called Butterflies puts a new spin on how we interact with AI. With Meta and others making similar moves, social media is about to get a lot weirder.|
|[OpenAI walks back controversial stock sale policies, will treat current and former employees the same.](https://www.cnbc.com/2024/06/24/openai-changes-secondary-stock-sale-rules-treats-ex-staffers-equally.html) |OpenAI has changed its policies toward secondary share sales to allow current and former employees to participate equally in its annual tender offers, CNBC has learned. All current and former staffers “will have the same sales limit” and be able to participate at the same time, OpenAI said in documents shared with stakeholders.|
|[Report: Amazon developing AI chatbot that would compete with ChatGPT and others.](https://www.geekwire.com/2024/report-amazon-developing-ai-chatbot-that-would-compete-with-chatgpt-and-others/) |Amazon is developing its own consumer-focused AI chatbot that would compete with OpenAI’s ChatGPT and could be revealed later this year, according to a report from Business Insider. |
|[Multi is joining OpenAI.](https://multi.app/blog/multi-is-joining-openai) |OpenAI continues its purchase binge by purchasing additional desktop-related infrastructure. |
|[Artificial Marketing Intelligence at your fingertips: MarTech startup Ability AI secures $1.1M pre-seed round funding to automate the process.](https://www.linkedin.com/pulse/artificial-marketing-intelligence-your-fingertips-martech-startup-avnye/) | |
|[Claude 3.5 suggests AI’s looming ubiquity could be a good thing.](https://www.theguardian.com/technology/article/2024/jun/25/anthropic-claude-ai-chatbot) |If you don’t like chatbots popping up everywhere, get ready to be peeved. But the latest version of Anthropic’s shows AI is becoming more useful – and, crucially, affordable |
|[Apple found in breach of EU competition rules.](https://www.theguardian.com/technology/article/2024/jun/24/apple-breach-eu-competition-rules-digital-markets-act) | European Commission finds iPhone maker broke new laws designed to protect smaller competitors against big tech platforms|
|[Etched is building an AI chip that only runs one type of model.](https://techcrunch.com/2024/06/25/etched-is-building-an-ai-chip-that-only-runs-transformer-models/) |Etched is among the many, many alternative chip companies vying for a seat at the table — but it’s also among the most intriguing.  |
|[Stability AI Secures Significant New Investment.](https://stability.ai/news/stability-ai-secures-significant-new-investment) |Stability AI was able to obtain a "significant infusion of capital" from both new and existing investors in addition to hiring a new CEO. |
|[Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings.](https://imbue.com/research/70b-intro/) | Earlier this year, we pre-trained and fine-tuned a 70B-parameter model that outperforms GPT-4o zero-shot on a range of reasoning and coding-related benchmarks and datasets. Our fine-tuned model, pre-trained on 2T tokens, roughly matches a fine-tuned Llama 3 70B, which was pre-trained on more than seven times as much data.|
|[OpenAI Pushes Back Voice Mode.](https://threadreaderapp.com/thread/1805716393524183136.html) | The sophisticated Voice Mode that OpenAI showcased in its Spring Update will go live in alpha form in late July for a limited group of ChatGPT Plus subscribers.|
|[Meta’s AI translation model embraces overlooked languages.](https://www.nature.com/articles/d41586-024-00964-2) |More than 7,000 languages are in use throughout the world, but popular translation tools cannot deal with most of them. A translation model that was tested on under-represented languages takes a key step towards a solution. |
|[Researchers fool university markers with AI-generated exam papers.](https://www.theguardian.com/education/article/2024/jun/26/researchers-fool-university-markers-with-ai-generated-exam-papers) |University of Reading project poses questions for integrity of coursework and take-home student assignments |
|[YouTube tries convincing record labels to license music for AI song generator.](https://arstechnica.com/ai/2024/06/youtube-tries-convincing-record-labels-to-license-music-for-ai-song-generator/) | Video site needs labels’ content to legally train AI song generators.|
|[Evolutionary Scale Raises $142m series A.](https://www.evolutionaryscale.ai/blog/esm3-release) | A biology startup called Evolutionary Scale has came out of stealth with significant funding. Additionally, it declared the release of ESM 3, its foundation model, a 98B parameter model trained for 10^24 Flops on 771B biological tokens. Using the model, it found a new luminous green protein that is not found in nature.|
|[Waymo One is now open to everyone in San Francisco.](https://waymo.com/blog/2024/06/waymo-one-is-now-open-to-everyone-in-san-francisco/) |With its driverless cars, Waymo One now makes it possible for anybody in San Francisco to request a ride. After providing tens of thousands of trips per week, the company is expanding. Its all-electric fleet helps it achieve its sustainability goals and boosts the local economy. Waymo claims that its cars are much less likely to be involved in collisions than those driven by humans, citing increased safety. |
|[ChatGPT on your desktop.](https://openai.com/chatgpt/mac/) |Users can now download the ChatGPT desktop software for macOS. |
|[https://www.theguardian.com/technology/article/2024/jun/27/ai-bill-gates-climate-targets-datacentres-energy.](https://www.theguardian.com/technology/article/2024/jun/27/ai-bill-gates-climate-targets-datacentres-energy) | Microsoft co-founder says efficiencies for technology and electricity grids will outweigh energy use by data centres|
|[Snap Lense Studio 5.0.](https://ar.snap.com/blog/genai-suite-lens-studio-5.0) | The GenAI suite, which Snap introduced with Lens Studio 5.0, is a fantastic development and a huge help for creating augmented reality apps.|
|[Instagram Launching An AI Studio.](https://www.theverge.com/2024/6/27/24187502/instagram-ai-studio-versions-chatbot) |Instagram's "AI Studio" enables developers to create self-aware AI chatbots. In the US, an early test of it is presently underway. |
|[Dust raises $16m series A.](https://blog.dust.tt/dust-seriesa-sequoia-leading-ai-platform/) |Dust, one of the first modern-day chaining and agency companies, raised more money after surpassing $1 million in annual revenue. |
|[ElevenLabs launches iOS app that turns ‘any’ text into audio narration with AI.](https://venturebeat.com/ai/elevenlabs-launches-ios-app-that-turns-any-text-into-audio-narration-with-ai/) |"ElevenLabs Reader: AI Audio," the company's debut iOS app, enables users to listen on the go by turning text files or web links into audio narration. |

## Resources
|Link|description|
|---|---|
|[Open-Sora 1.2 Report.](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_03.md) |a 1.1B parameter model trained on over 30 million data points, this open-source video generation model can produce 16-second 720p videos. It also features an improved diffusion model and video compression network for both temporal and spatial compression, which lowers training costs and improves controllability of the generations. |
|[LLM101n: Let's build a Storyteller.](https://github.com/karpathy/LLM101n) |An outline for a new course that Andrej Karpathy is working on can be found in a new repository. It entails creating a narrative-capable aligned language model. Code, video lectures, and other learning resources are included in the course. |
|[AutoCodeRover: Autonomous Program Improvement.](https://github.com/nus-apr/auto-code-rover) | AutoCodeRover is a new technology that combines sophisticated code search methods with big language models to automate software enhancements, such as feature additions and problem fixes.|
|[NLUX.](https://github.com/nlkitai/nlux) | NLUX is React and JavaScript open-source library for building conversational AI interfaces. It makes it super simple to build web applications powered by Large Language Models (LLMs) and AI. With just a few lines of code, you can add conversational AI capabilities and interact with your favourite AI models.|
|[Claudette.](https://claudette.answer.ai/) |Claudette is a higher-level and easier-to-use way to interact with Claude. |
|[top CVPR 2024 papers.](https://github.com/SkalskiP/top-cvpr-2024-papers) |Computer Vision and Pattern Recognition is a massive conference. In 2024 alone, 11,532 papers were submitted, and 2,719 were accepted. I created this repository to help you search for crème de la crème of CVPR publications. |
|[TTS in 7000 Languages.](https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v3.0) |Recently, Toucan published a collection of new text-to-speech models that are now compatible with all ISO-639-3 standard languages. |
|[ParaLLM: 1300+ tok/s on a MacBook.](https://willcb.com/blog/parallm/) |When batch parallel KV cache is implemented in MLX, inference times for the creation of synthetic data and model completions are significantly sped up. |
|[Train vision models in TRL .](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) | Transformers can be trained using reinforcement learning with the help of TRL, a Hugging Face library. You may apply the same procedure for vision-based language models, such as LLaVA, using this example.|
|[Rethinking Remote Sensing Change Detection With A Mask View.](https://arxiv.org/abs/2406.15320v1) | Two new models for remote sensing change detection—CDMask and CDMaskFormer—are presented in this study.|
|[llama.ttf.](https://fuglede.github.io/llama.ttf/) | This article explains how to use a font file to run a little Llama language model.|
|[june.](https://github.com/mezbaul-h/june) |june is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers. |
|[Building a personalized code assistant with open-source LLMs using RAG Fine-tuning.](https://www.together.ai/blog/rag-fine-tuning) | AI and Morph Labs collaborated to create an excellent blog post about optimizing models for retrieval enhanced generation. They also demonstrate a few applications of generated data.|
|[EvalAlign: Evaluating Text-to-Image Models through Precision Alignment of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations.](https://arxiv.org/abs/2406.16562v1) |A novel metric called EvalAlign was created to enhance the assessment of generative models that convert text to images. EvalAlign provides fine-grained accuracy and stability in contrast to current measures. It emphasizes text-image alignment and image faithfulness. |
|[Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models.](https://huggingface.co/blog/finetune-florence2) | Florence-2, released by Microsoft in June 2024, is a foundation vision-language model. This model is very attractive because of its small size (0.2B and 0.7B) and strong performance on a variety of computer vision and vision-language tasks. Florence supports many tasks out of the box: captioning, object detection, OCR, and more.|
|[Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity.](https://pytorch.org/blog/accelerating-neural-network-training/) |Specifically designed kernels have been created by the PyTorch team to utilize sparse cores, which are typically exclusively used for inference. |
|[FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models.](http://haonanqiu.com/projects/FreeTraj.html) |Diffusion models are used in FreeTraj, a tuning-free technique for controlling motion trajectories in video creation. To direct the generated content, it adjusts the attention mechanisms and noise sampling. |
|[OpenGlass - Open Source Smart Glasses.](https://github.com/BasedHardware/OpenGlass) | Turn any glasses into hackable smart glasses with less than $25 of off-the-shelf components. Record your life, remember people you meet, identify objects, translate text, and more.|
|[An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability.](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) |The Golden Gate Claude served as a potent illustration of how to influence and evaluate models using SAEs. This work includes some sample code for training these models and an easy-to-understand explanation of how it operates. |
|[RES-Q.](https://github.com/qurrent-ai/res-q) | A new benchmark called RES-Q is designed to evaluate how well huge language models can modify code repositories using instructions in natural language.|
|[Balancing Old Tricks with New Feats: AI-Powered Conversion From Enzyme to React Testing Library at Slack.](https://slack.engineering/balancing-old-tricks-with-new-feats-ai-powered-conversion-from-enzyme-to-react-testing-library-at-slack/) |Using a hybrid method, Slack developers used AI Large Language Models with Abstract Syntax Tree transformations to automate the translation of more than 15,000 unit tests from Enzyme to React Testing Library. The team utilized Anthropic's Claude 2.1 AI model in conjunction with DOM tree capture for React components to achieve an 80% success rate in automatic conversions. This ground-breaking project demonstrates Slack's dedication to using AI to improve developer productivity and experience. It's a part of the continuous attempts to remain ahead of the always changing frontend scene. |
|[R2R.](https://github.com/SciPhi-AI/R2R) | R2R was designed to bridge the gap between local LLM experimentation and scalable, production-ready Retrieval-Augmented Generation (RAG). R2R provides a comprehensive and SOTA RAG system for developers, built around a RESTful API for ease of use.|
|[Internist.ai 7b.](https://huggingface.co/internistai/base-7b-v0.2) |Internist.ai 7b is a medical domain large language model trained by medical doctors to demonstrate the benefits of a physician-in-the-loop approach. The training data was carefully curated by medical doctors to ensure clinical relevance and required quality for clinical practice. |
|[Finding GPT-4’s mistakes with GPT-4.](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/) |CriticGPT, a model based on GPT-4, writes critiques of ChatGPT responses to help human trainers spot mistakes during RLHF |
|[ALPBench: A Benchmark for Active Learning Pipelines on Tabular Data.](https://arxiv.org/abs/2406.17322v1) |A program called ALPBench was created to standardize active learning query benchmarks. |
|[Introducing AuraSR - An open reproduction of the GigaGAN Upscaler.](https://blog.fal.ai/introducing-aurasr-an-open-reproduction-of-the-gigagan-upscaler-2/) |FAL recently made AuraSR, a high resolution picture upscaler, open-sourced. Even with repeated applications, it may upscale by 4x with just one forward pass. AuraSR performs admirably with created photos. |
|[Point-SAM: Promptable 3D Segmentation Model for Point Clouds.](https://point-sam.github.io/) |Point-SAM, a transformer-based 3D segmentation model, has been introduced by researchers in response to the increasing demand for comprehensive 3D data. |
|[GenIR-Survey.](https://github.com/ruc-nlpir/genir-survey) | This survey explores generative information retrieval (GenIR), a novel approach to information retrieval that shifts from conventional search techniques to ones that generate results dynamically.|
|[Gemma 2.](https://www.kaggle.com/models/google/gemma-2) | Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.|
|[MatText: Do Language Models Need More than Text & Scale for Materials Modeling?](https://arxiv.org/abs/2406.17295v1) |MatText is a collection of benchmarking tools and datasets intended to assess the effectiveness of language models in the field of materials science. |
|[mamba2.](https://github.com/okarthikb/state-space-models/blob/main/models/mamba2.py) |A quick implementation of Mamba 2 |

## Perspectives
|Link|description|
|---|---|
|[The Long View on AI.](https://www.maximum-progress.com/p/the-long-view-on-ai) | AI has the potential to cause tremendous growth rates and technological improvements, according to historical statistics. Society will probably be able to adjust to these rapid changes just as it has in the past.|
|[AI’s Hidden Opportunities: Shawn "swyx" Wang on New Use Cases and Career.](https://www.heavybit.com/library/article/ai-hidden-opportunities-for-software-developers-swyx) |Well-known developer Shawn "swyx" Wang discusses untapped potential for conventional software professionals wishing to go into artificial intelligence. In particular, examining how to enhance existing tools, use AI to summarization, and more. |
|[Apple Intelligence.](https://daringfireball.net/2024/06/wwdc24_apple_intelligence) | Rather than developing stand-alone AI products, Apple has incorporated generative AI into its core apps, improving services like Mail classification, Safari summaries, and Siri's functioning. This demonstrates the company's focus on user control and privacy.|
|[Apple intelligence and AI maximalism.](https://www.ben-evans.com/benedictevans/2024/06/20/apple-intelligence) |Apple has showed a bunch of cool ideas for generative AI, but much more, it is pointing to most of the big questions and proposing a different answer - that LLMs are commodity infrastructure, not platforms or products. |
|[How To Solve LLM Hallucinations.](https://morethanmoore.substack.com/p/how-to-solve-llm-hallucinations) | Lamini has created Memory Tuning, which effectively embeds particular facts into models without sacrificing general knowledge and reduces hallucinations by 95%.|
|[AI machine translation tools must be taught cultural differences too.](https://www.nature.com/articles/d41586-024-02091-4) |But to successfully preserve or revitalize minority languages, the scope of large-language-model (LLM) training needs to be broadened. |
|[Misinformation might sway elections — but not in the way that you think.](https://www.nature.com/articles/d41586-024-01696-z) | Rampant deepfakes and false news are often blamed for swaying votes. Research suggests it’s hard to change people’s political opinions, but easier to nudge their behaviour.|
|[How I’m using AI tools to help universities maximize research impacts.](https://www.nature.com/articles/d41586-024-02081-6) |Artificial-intelligence algorithms could identify scientists who need support with translating their work into real-world applications and more. Leaders must step up. |
|[The Future of LLM-Based Agents: Making the Boxes Bigger.](https://www.arcus.co/blog/ai-agents-pt-2) |Long-term planning and system-level resilience are two essential strategies that assist move Agents from the playground into the real world, and they are discussed in this post. These introduce the ability to create plans of a higher level for the Agents, allowing for adaptability in the middle of an episode. They also introduce systems techniques to intelligently orchestrate the models, resulting in increased performance and accuracy. |
|[Apple, Microsoft Shrink AI Models to Improve Them.](https://spectrum.ieee.org/small-language-models-apple-microsoft) | Large language models are becoming less popular as IT companies shift their focus to more efficient small language models (SLMs). Apple and Microsoft have introduced models with far fewer parameters that nonetheless perform comparably or even better in benchmarks. According to the CEO of OpenAI, we're past the LLM era since SLMs have benefits including greater accessibility for smaller entities, local device operation, and potential insights into human language acquisition. Even though SLMs are narrower in scope, their performance is enhanced by training them on high-quality, or "textbook-quality" data.|
|[Are Tech-Enabled Vertical Roll-Ups the Future or the Past?.](https://www.tidemarkcap.com/post/are-tech-enabled-vertical-roll-ups-the-future-or-the-past) | The ability to generate excess cash flows through operational efficiencies is a prerequisite for roll-up methods. It's possible that the development of AI offers a new lever that fully unlocks the rollup strategy. Are rollups for SMBs and verticals the future? Two different perspectives on this issue are presented in this post.|

# ML news: 16 - 23  June

## Research
|Link|description|
|---|---|
|[Discovering Preference Optimization Algorithms with and for Large Language Models.](https://arxiv.org/abs/2406.08414) |suggests an algorithm that adaptively combined logistic and exponential losses; this approach eliminates the need for human intervention by prompting an LLM to suggest and implement preference optimization loss functions based on previously assessed performance metrics. It also suggests an LLM-driven objective discovery of state-of-the-art preference optimization.  |
|[SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals.](https://arxiv.org/abs/2406.04784) |a framework to increase the high-level goal-achieving capabilities of an LLM-based agent; during interaction with the environment, the framework adaptively decomposes a high-level goal into a tree structure of useful subgoals; enhances performance on a variety of tasks, including cooperative, competitive, and deferred feedback environments. |
|[Mixture-of-Agents Enhances Large Language Model Capabilities.](https://arxiv.org/abs/2406.04692) | a strategy that beats GPT-4o on AlpacaEval 2.0, MT-Bench, and FLASK by utilizing the combined strengths of several LLMs through a Mixture-of-Agents methodology; layers are constructed with numerous LLM agents, and each agent builds on the outputs of other agents in the previous levels.|
|[Transformers meet Neural Algorithmic Reasoners.](https://arxiv.org/abs/2406.09308) | Tokens in the LLM can now cross-attend to node embeddings from a GNN-based neural algorithmic reasoner (NAR) thanks to a new hybrid design; the resulting model, named TransNAR, shows gains in OOD reasoning across algorithmic challenges.|
|[Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching.](https://arxiv.org/abs/2406.06326) | increases an LLM's capacity to learn new information from raw documents through self-teaching; the process consists of three steps: 1) a self-teaching component that enhances documents with a series of knowledge-intensive tasks emphasizing comprehension, memorization, and self-reflection; 2) the model is configured to continuously learn using only the new documents, aiding in the thorough acquisition of new knowledge; and 3) the deployed model is used to learn new information from new documents while evaluating its QA skills.|
|[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.](https://arxiv.org/abs/2406.09403) |a framework that gives a multimodal LLM access to a visual sketchpad and drawing tools; it can give a model, such as GPT-4, the ability to create intermediate sketches in order to reason over complex tasks; over strong base models without sketching, it performs better on many tasks; on all the tasks tested, GPT-4 equipped with SketchPad sets a new state of the art. |
|[Mixture of Memory Experts.](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf) |claims to enable scaling to a high number of parameters while keeping the inference cost fixed. It suggests a method to significantly reduce hallucination (10x) by tuning millions of expert adapters (e.g., LoRAs) to learn exact facts and retrieve them from an index at inference time. The memory experts are specialized to ensure faithful and factual accuracy on the data it was tuned on. |
|[Multimodal Table Understanding.](https://arxiv.org/abs/2406.08100) | presents Table-LLaVa 7B, a multimodal LLM for multimodal table understanding; it produces a large-scale dataset MMTab, comprising table images, instructions, and tasks; it is comparable with GPT-4V and greatly outperforms existing MLLMs on numerous benchmarks. |
|[Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement.](https://arxiv.org/abs/2406.07138) |suggests a training-efficient way to extend LLMs to longer context lengths (e.g., 4K -> 256K); it uses a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning; the approach helps to alleviate the so-called "Lost-in-the-Middle" problem in long-context LLMs. suggests a method to tune an LLM to effectively utilize information from the middle part of the context. |
|[Simple and Effective Masked Diffusion Language Models.](https://s-sahoo.com/mdlm/) | Easy diffusion model to model language. It functions fairly well and generates out of order.|
|[MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding.](https://arxiv.org/abs/2406.09297v1) |A novel technique that dramatically lowers memory consumption during auto-regressive inference in transformers is called Multi-Layer Key-Value (MLKV) sharing. |
|[Understanding Hallucinations in Diffusion Models through Mode Interpolation.](https://arxiv.org/abs/2406.09358v1) | This study looks into the reasons behind "hallucinations"—images that never were in the training set—that are produced by diffusion-based picture generation models.|
|[Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs.](https://arxiv.org/abs/2406.09136v1) |A technique called Chain of Preference Optimization (CPO) helps large language models (LLMs) become more adept at logical reasoning. CPO matches the reasoning steps of Chain-of-Thought (CoT) decoding with the optimal routes of ToT by fine-tuning LLMs using search trees from the Tree-of-Thought (ToT) technique. |
|[Language Modeling with Editable External Knowledge.](https://arxiv.org/abs/2406.11830v1) |ERASE is a novel approach to updating language models. Unlike conventional methods that emphasize enhancing retrieval during prediction, ERASE incrementally deletes or rewrites entries in the knowledge base as new documents are incorporated. |
|[Duoduo CLIP: Efficient 3D Understanding with Multi-View Images.](https://3dlg-hcvc.github.io/DuoduoCLIP) |Duoduo CLIP is a 3D representation learning model that utilizes multi-view images rather than point-clouds for its training and analysis. |
|[CAMixerSR: Only Details Need More "Attention".](https://arxiv.org/abs/2402.19289v2) |CAMixerSR enhances image resolution by intelligently applying convolution to simpler areas and using deformable window-attention for intricate textures. |
|[‘Fighting fire with fire’ — using LLMs to combat LLM hallucinations.](https://www.nature.com/articles/d41586-024-01641-0) | The number of errors produced by an LLM can be reduced by grouping its outputs into semantically similar clusters. Remarkably, this task can be performed by a second LLM, and the method’s efficacy can be evaluated by a third. Associate article is [here](https://www.nature.com/articles/s41586-024-07421-0)|
|[Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.](https://huggingface.co/microsoft/Florence-2-large-ft) | Microsoft has published a collection of tiny VLMs under an MIT license that perform noticeably better in captioning, bounding, and classification than much larger models.|
|[Logit Prisms: Decomposing Transformer Outputs for Mechanistic Interpretability.](https://neuralblog.github.io/logit-prisms/) |The logit lens approach has been improved by decomposing logit outputs into contributions from different model components. This aids in comprehending the decision-making process of transformer models. This method, which employs "prisms" for residual streams, attention layers, and MLP layers, demonstrates how these components affect predictions and offers insights into the tasks that the gemma-2b model does, such as factual retrieval and arithmetic. |
|[PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers.](https://arxiv.org/abs/2406.12430v1) |Using sophisticated data analysis, decision QA is a new role for LLMs that identifies the optimal decisions. |
|[ChangeViT: Unleashing Plain Vision Transformers for Change Detection.](https://arxiv.org/abs/2406.12847v1) | A methodology called ChangeViT makes use of vision transformers (ViTs) to identify significant environmental changes in remote sensing photos.|
|[LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging.](https://arxiv.org/abs/2406.12837v1) |LayerMerge is a novel technique that simultaneously prunes activation functions and convolution layers to increase neural network efficiency. |
|[Adversarial Attacks on Multimodal Agents.](https://chenwu.io/attack-agent/) |Vision-enabled language models (VLMs) such as Gemini and GPT-4o enable autonomous agents to perform activities like code editing and buying. This investigation demonstrates how susceptible these agents are to malevolent attacks. |
|[TimeSieve: Extracting Temporal Dynamics through Information Bottlenecks.](https://arxiv.org/abs/2406.05036v1) | A novel model called TimeSieve was created to address typical problems in time series forecasting.|

## News
|Link|description|
|---|---|
|[Apple to ‘Pay’ OpenAI for ChatGPT Through Distribution, Not Cash.](https://www.bloomberg.com/news/articles/2024-06-12/apple-to-pay-openai-for-chatgpt-through-distribution-not-cash) | The collaboration between Apple and OpenAI isn't anticipated to bring in a significant amount of money for either company, at least not right away. Apple is not paying OpenAI as part of the agreement because it feels that integrating OpenAI's technology and brand into its products is as valuable as or more valuable than financial compensation. The agreement isn't exclusive; Apple is already talking about providing additional chatbot choices. In the long run, Apple intends to profit from AI by entering into revenue-sharing contracts with AI partners.|
|[AI will make money sooner than you’d think, says Cohere CEO Aidan Gomez.](https://www.theverge.com/24173858/ai-cohere-aidan-gomez-money-revenue-llm-transformers-enterprise-stochastic-parrot) |Enterprise is the pathway to profit, Gomez says, but maybe don’t ask it to do medicine quite yet. |
|[Fake beauty queens charm judges at the Miss AI pageant.](https://www.npr.org/2024/06/09/nx-s1-4993998/the-miss-ai-beauty-pageant-ushers-in-a-new-type-of-influencer) |An AI model from Romania named Aiyana Rainbow is a finalist in the first Miss AI pageant, which showcases AI-generated models on social media. The event is a part of "The FanVue World AI Creator Awards," which is organized by FanVue and highlights the talent of AI creators who can create captivating content without having to be the face of the work. The $5,000 prize package for Miss AI will include mentorship and support from the public relations community. At the end of June, the outcomes will be made public. |
|[Elon Musk reconsiders phone project after Apple Intelligence OpenAI integration.](https://www.teslarati.com/elon-musk-reconsiders-phone-apple-intelligence-openai-chatgpt-integration/) | Elon Musk threatened to forbid any Apple devices from being used on the properties of his firms in response to Apple integrating OpenAI ChatGPT on a few of its devices.|
|[Microsoft’s star AI chief peers into OpenAI’s code, highlighting an unusual rivalry.](https://www.semafor.com/article/06/14/2024/microsoft-ai-ceo-mustafa-suleyman-audits-openais-code) |Primarily, OpenAI was established as a safety net against DeepMind, the AI startup that Google purchased in 2014. However, Mustafa Suleyman, a co-founder of DeepMind, has recently been taking on an unimaginable task: delving into OpenAI's crown jewels, the proprietary algorithms that power foundation models like GPT-4, according to people familiar with the situation. This is due to the fact that Suleyman is currently Microsoft's head of AI initiatives. As part of Microsoft's multibillion-dollar investment in OpenAI, the corporation possesses the intellectual property rights to its software. |
|[Amazon says it’ll spend $230 million on generative AI startups.](https://techcrunch.com/2024/06/13/amazon-says-itll-spend-230-million-on-generative-ai-startups/) |Amazon says that it will commit up to $230 million to startups building generative AI-powered applications. |
|[McDonald’s ends AI drive-thru trial as fast-food industry tests automation.](https://www.theguardian.com/business/article/2024/jun/17/mcdonalds-ends-ai-drive-thru) |Companies have touted AI as future of the industry, but technology has also resulted in viral videos of wrong orders |
|[Balance effects of AI with profits tax and green levy, says IMF.](https://www.theguardian.com/business/article/2024/jun/17/ai-profits-tax-green-levy-imf-carbon-emissions) | Governments faced with economic upheaval caused by artificial intelligence should consider fiscal policies including taxes on excess profits and a green levy to atone for AI-related carbon emissions, according to the International Monetary Fund.|
|[Introducing Gen-3 Alpha.](https://runwayml.com/blog/introducing-gen-3-alpha/) | Runway has developed a brand-new, incredibly potent video generation model. Many of the current functions on its platform will be powered by it. You can find examples at the given URL.|
|[DeepMind’s new AI generates soundtracks and dialogue for videos.](https://techcrunch.com/2024/06/17/deepminds-new-ai-generates-soundtracks-and-dialog-for-videos) | V2A is an AI system that DeepMind is developing to create synchronized soundtracks for videos. It generates music, sound effects, and dialogue using diffusion models trained on audio, dialogue transcripts, and video clips.|
|[Giant Chips Give Supercomputers a Run for Their Money .](https://spectrum.ieee.org/cerebras-wafer-scale-engine) |The California-based business Cerebras has proven in molecular dynamics calculations that their second-generation wafer-scale engine outperforms the fastest supercomputer in the world by a large margin. Additionally, it can infer sparse huge language models with no loss of accuracy at one-third of the energy cost of a complete model. The hardware of Cerebras allows for quick memory access and interconnects, which make both accomplishments possible. Cerebras aims to expand the scope of its wafer-scale engine applications to encompass a broader range of issues, such as airflow models surrounding cars and molecular dynamics simulations of biological processes. |
|[Nvidia becomes world’s most valuable company amid AI boom.](https://www.theguardian.com/technology/article/2024/jun/18/nvidia-valuation-most-valuable) | Chipmaker dethrones Microsoft and Apple as stock market surge boosts valuation above $3.34tn|
|[The ‘Godfather of AI’ quit Google a year ago. Now he’s emerged out of stealth to back a startup promising to use AI for carbon capture.](https://fortune.com/europe/2024/06/18/godfather-ai-geoffrey-hinton-quit-google-year-ago-emerged-stealth-back-startup-cuspai-ai-carbon-capture/) |Renowned AI researchers Geoff Hinton and Max Welling have gathered a talented team to develop AI systems aimed at advancing material science for carbon capture. |
|[Nvidia Conquers Latest AI Tests​.](https://spectrum.ieee.org/mlperf-nvidia-conquers) |Nvidia's Hopper architecture-based systems excelled in two recent MLPerf AI benchmark tests, which assess the fine-tuning of large language models and the training of graph neural networks. |
|[Perplexity AI searches for users in Japan, via SoftBank deal.](https://techcrunch.com/2024/06/17/softbank-ties-up-with-perplexity/) |Perplexity is capitalizing on its strategic partnership with SoftBank to broaden its presence in Japan. As part of this initiative, it is providing a free year of its premium AI-powered search engine, Perplexity Pro. SoftBank's goal is to draw users by offering AI services without creating internal solutions. With a valuation of $1 billion, Perplexity is expanding its funding and investor base, which features prominent tech leaders and venture firms. |
|[Introducing Local III.](https://changes.openinterpreter.com/log/local-iii) | The open-source local agent, Open Interpreter, has recently received a significant upgrade. It now has the capability to control the computer seamlessly and operates entirely offline and locally.|
|[Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs.](https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms) | LlamaIndex has launched the Property Graph Index, significantly improving knowledge graph capabilities with enhanced modeling, storage, and querying features. This new index enables flexible graph construction and supports schema-guided, implicit, and free-form entity extraction. It also integrates with vector databases for hybrid searches and offers querying options through keyword expansion, vector similarity, Cypher queries, and custom traversal.|
|[Decagon launches with $35m raised from Accel and a16z.](https://decagon.ai/blog/series-a) |Decagon is developing human-like AI agents for customer support and has recently secured $30 million in Series A funding from Accel, along with $5 million in seed funding from a16z. Decagon's product manages global support for companies such as Eventbrite, Rippling, Webflow, BILT, and Substack. |
|[London premiere of movie with AI-generated script cancelled after backlash.](https://www.theguardian.com/film/article/2024/jun/20/premiere-movie-ai-generated-script-cancelled-backlash-the-last-screenwriter-prince-charles-cinema) | Plans to show The Last Screenwriter, whose script is credited to ‘ChatGPT 4.0’, prompted complaints although the film-makers insist the feature is ‘a contribution to the cause’|
|[OpenAI’s former chief scientist is starting a new AI company.](https://www.theverge.com/2024/6/19/24181870/openai-former-chief-scientist-ilya-sutskever-ssi-safe-superintelligence) |Ilya Sutskever is launching Safe Superintelligence Inc., an AI startup that will prioritize safety over ‘commercial pressures.’ |
|[Claude 3.5 Sonnet.](https://www.anthropic.com/news/claude-3-5-sonnet) |At a fifth of the cost, Claude 3.5 Sonnet outperforms Opus in performance. Plus, it's the greatest vision model out there right now. This demonstrates how much the frontier models have progressed. |
|[Apple researchers add 20 more open-source models to improve text and image AI.](https://appleinsider.com/articles/24/06/19/apple-researchers-add-20-more-open-source-models-to-improve-text-and-image-ai) |With 20 Core Machine Learning models that Apple has added to the Hugging Face open-source AI repository, the repository now includes a wider selection of public models with improved image classification and depth segmentation. These donations come after Apple earlier in the year released the four OpenELMs to Hugging Face and the Ferret big language model. The action shows Apple's dedication to developing AI capabilities and its growing involvement with the AI research community. |
|[Factory Raises $15M Series A from Sequoia.](https://www.factory.ai/news/series-a-announcement) |Led by Sequoia Capital, Factory has raised $15 million in Series A funding to grow its workforce and improve its Droids software development toolset, which leverages artificial intelligence. Its products are rapidly expanding its customer base and setting new benchmarks on the SWE-bench AI coding benchmark. With Factory, software engineering will be increasingly automated, cutting down on laborious processes and speeding up development cycles. |
|[Optimizing AI Inference at Character.AI.](https://research.character.ai/optimizing-inference/) |Twenty percent of Google's search volume, or 20,000 questions per second, are answered by Character AI. It operates this effectively thanks to several advancements. |
|[Apple delays launch of AI-powered features in Europe, blaming EU rules.](https://www.theguardian.com/technology/article/2024/jun/21/apple-ai-europe-regulation) | Apple says competition rules that require functionality with rival products would compromise privacy and security|

## Resources
|Link|description|
|---|---|
|[Nemotron-4 340B.](https://research.nvidia.com/publication/2024-06_nemotron-4-340b) |offers a reward model to filter data based on many qualities and an instruct model to generate high-quality data; exhibits impressive results on widely-used benchmarks such as MMLU and GSM8K; It competes with GPT-4 in a number of activities, such as scoring highly in multi-turn chat; Together with the base model, a preference data is also made available. |
|[Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs.](https://arxiv.org/abs/2406.09324v1) |Determining ways to incorporate search into language model creation is now the Holy Grail of study. This work is quite encouraging as it demonstrates that on math performance, tiny models with search can match considerably more powerful models. |
|[MCTSr: Mathematic as a Blackbox for LLM.](https://github.com/trotsky1997/MathBlackBox) | |
|[VideoGPT.](https://github.com/mbzuai-oryx/videogpt-plus) | To improve video understanding, a model called VideoGPT+ combines image and video encoders. While video encoders offer temporal context, image encoders capture finely detailed spatial information.|
|[Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach.](https://linlin-dev.github.io/project/RSG.html) | In order to enhance Scene Graph Generation (SGG) for very-high-resolution satellite imaging (VHR SAI), this research introduces a new dataset and methodology.|
|[LLM.Mojo.](https://github.com/dorjeduck/llm.mojo) |This project is a port of Andrej Karpathy's llm.c to Mojo, currently in beta and subject to changes. |
|[Depth Anything V2.](https://arxiv.org/abs/2406.09414) | With the use of artificial data, the new Depth Anything model was trained, and its performance on intricate scenes has significantly increased.|
|[DeepSeek-Coder-V2.](https://github.com/deepseek-ai/DeepSeek-Coder-V2) |Robust DeepSeek Coder achieves scores of 90+ on HumanEval and matches GPT-4 Turbo on numerous other difficult benchmarks. It is free for business usage and accessible via an API. |
|[HelpSteer2: Open-source dataset for training top-performing reward models.](https://arxiv.org/abs/2406.08673) |Along with an excellent paper about training reward models to match model output to human preferences, Nvidia has made available a dataset and procedure. |
|[Differentiable rasterization.](https://srush.github.io/DiffRast) | Given a program that produces a vector representation of an image (think SVG), rasterization turns it into a pixel representation (think PNG). Everything ought to be adjustable. This article explains how to write SVG light that is differentiable.|
|[LARS - The LLM & Advanced Referencing Solution.](https://github.com/abgulati/LARS) |LARS is an application that enables you to run LLM's (Large Language Models) locally on your device, upload your own documents and engage in conversations wherein the LLM grounds its responses with your uploaded content. |
|[Beyond the Basics of Retrieval for Augmenting Generation.](https://parlance-labs.com/education/rag/ben.html) |The RAGatouille creator delivered a great discussion about COLBERT, some of the open issues, and how to significantly increase RAG performance. |
|[TokenCost.](https://github.com/AgentOps-AI/tokencost) |Tokencost helps calculate the USD cost of using major Large Language Model (LLMs) APIs by calculating the estimated cost of prompts and completions. |
|[GaiaNet node.](https://github.com/GaiaNet-AI/gaianet-node) | Install and run your own AI agent service.|
|[Meta Chameleon.](https://github.com/facebookresearch/chameleon) |Chameleon is an early fusion model that processes images and text tokens concurrently. The team published the paper a few weeks ago and has now released model checkpoints along with inference code. |
|[OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations.](https://github.com/princeton-vl/ogni-dc) | OGNI-DC is a new framework for depth completion that employs "Optimization-Guided Neural Iterations" (OGNI). This method refines a depth gradient field and incorporates the depth gradients into a depth map.|
|[Subobject-level Image Tokenization.](https://arxiv.org/abs/2402.14327v2) |Subobject tokenization is a novel approach for vision models to interpret images. Rather than dividing images into fixed square patches, this method allows models to analyze images by identifying meaningful segments, such as parts of objects. |
|[Introduction to Granite Code Models.](https://github.com/ibm-granite/granite-code-models/tree/main) |We introduce the Granite series of decoder-only code models for code generative tasks (e.g., fixing bugs, explaining code, documenting code), trained with code written in 116 programming languages. A comprehensive evaluation of the Granite Code model family on diverse tasks demonstrates that our models consistently reach state-of-the-art performance among available open source code LLMs.  |
|[FireFunction V2: Fireworks Function Calling Model.](https://huggingface.co/fireworks-ai/firefunction-v2) |Open model that matches GPT4-o on function calling benchmarks trained on top of Llama 3 70B. |
|[Argilla.](https://github.com/argilla-io/argilla) |For AI developers and subject matter experts who need complete data ownership, high-quality outputs, and overall efficiency, Argilla offers a platform for cooperation. |
|[TroL: Traversal of Layers for Large Language and Vision Models.](https://github.com/byungkwanlee/trol) |Large language and vision models (LLVMs) with sizes of 1.8B, 3.8B, and 7B parameters are part of the new TroL family of efficient LLVMs. |
|[Dot.](https://github.com/alexpinel/Dot) |A stand-alone open-source program designed to be simple to use for local LLMs, and specifically RAG, to interact with files and documents in a manner similar to Nvidia's Chat with RTX. |
|[WebCanvas: Benchmarking Web Agents in Online Environments.](https://github.com/imeanai/webcanvas) |WebCanvas is a pioneering online evaluation framework designed to address the dynamic nature of web interactions. It provides a realistic assessment of autonomous web agents by utilizing live web environments and emphasizing task completion through the identification of key nodes. |
|[CIFAR-10 Airbench.](https://github.com/KellerJordan/cifar10-airbench) |A benchmark for image classification is CIFAR-10. In a remarkably short amount of time, this algorithm offers a training setting that yields good performance. |
|[Cost Of Self Hosting Llama-3 8B-Instruct.](https://blog.lytix.co/posts/self-hosting-llama-3) |Compared to using ChatGPT, self-hosting an LLM such as Llama-3 8B-Instruct can be much more expensive, costing approximately $17 per million tokens, while ChatGPT just costs $1 per million tokens. It is possible to lower the cost of self-hosted hardware to less than $0.01 per million tokens, but it would take about 5.5 years for the initial investment to pay for itself. |
|[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models.](https://yongtaoge.github.io/projects/geobench/) |Modern surface normal estimate and depth models are assessed using a new benchmark. |
|[An Empirical Study of Mamba-based Language Models.](https://huggingface.co/nvidia/mamba2-hybrid-8b-3t-4k) |The Nvidia research that previously showcased the hybrid basic Mamba model is now available. |

## Perspectives
|Link|description|
|---|---|
|[Computer says yes: how AI is changing our romantic lives.](https://www.theguardian.com/technology/article/2024/jun/16/computer-says-yes-how-ai-is-changing-our-romantic-lives) | Artificial intelligence is creating companions who can be our confidants, friends, therapists and even lovers. But are they an answer to loneliness or merely another way for big tech to make money?|
|[Nvidia’s New Sales Booster: The Global Push for National AI Champions.](https://www.wsj.com/tech/ai/nvidias-new-sales-booster-the-global-push-for-domestic-ai-champions-6d005ab7) |Governments everywhere are increasing their spending to entice corporations and multinationals to construct new data centers and renovate existing ones so that AI can be developed locally and massive language models can be trained in the original languages using data from their own inhabitants. According to Nvidia, these independent AI initiatives should generate over $10 billion in revenue this year. The potential economic effects of generative AI are a source of concern for several governments. For their sensitive data and AI infrastructure, they want sovereign clouds, and US IT companies are happy to construct them for them. |
|[General Intelligence (2024).](https://nonint.com/2024/06/03/general-intelligence-2024/) | What is lacking and what would it take to create a generally intelligent agent? This essay suggests that we will be here in a few years and examines the three concepts required to create an agent. The writer is an OpenAI researcher.|
|[Human neuroscience is entering a new era — it mustn’t forget its human dimension.](https://www.nature.com/articles/d41586-024-02022-3) | The field is taking a leap forward thanks to innovative technologies, such as artificial intelligence. Researchers must improve consent procedures and public involvement.|
|[AI and Euro 2024: VAR is shaking up football — and it’s not going away.](https://www.nature.com/articles/d41586-024-01764-4) |Sports physicist Eric Goff explains how updates to the technology can help referees to make the toughest calls. |
|[How cutting-edge computer chips are speeding up the AI revolution.](https://www.nature.com/articles/d41586-024-01544-0) |Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence. |
|[Apple’s Intelligent Strategy.](https://sdw.space/apples-intelligent-strategy/) | Apple showed off an incredible strategic edge in the AI arms race - but some might have missed that the company hints at using its biggest weakness as a formidable weapon against competitors.|
|[How to Fix “AI’s Original Sin”.](https://www.oreilly.com/radar/how-to-fix-ais-original-sin/) |The copyright issues raised by AI models trained on protected content without authorization are discussed in this article. It advises AI developers to adhere to copyright signals, put in place safeguards to stop producing content that violates intellectual property rights, and design business plans that guarantee just compensation for content creators. These strategies include retrieval-augmented generation (RAG) and the development of collaborative AI content ecosystems. |
|[Takeaways from OpenAI and Google's May announcements.](https://www.tanayj.com/p/takeaways-from-openai-and-googles) |With the introduction of sophisticated AI models by OpenAI and Google, real-time multimodal understanding and answers are now possible, and enhanced AI assistants and advancements in speech agents are promised. Google's Gemini 1.5 Flash offers a notable reduction in latency and cost, while OpenAI's GPT-4o promises double the speed and half the cost of its predecessor. Both digital behemoths are incorporating AI into their ecosystems, with OpenAI focusing on consumer markets with partnerships and products that could potentially reach up to a billion consumers. |
|[Collection of AI Side Business Money-Making Information.](https://github.com/bleedline/aimoneyhunter/blob/main/README_en.md) | There are some respectable AI projects on this list that even beginners can work on.|
|[paramount.](https://github.com/ask-fini/paramount) |Paramount lets your expert agents evaluate AI chats |


# ML news: 10-16 June

## Research
|Link|description|
|---|---|
|[Scaling neural machine translation to 200 languages.](https://www.nature.com/articles/s41586-024-07335-x) |based on a sparsely Gated Mixture of Experts architecture and trained on data using a method designed for low-resource languages, presents a massive multilingual model that uses transfer learning across 200 languages. It evaluates on 40K translations and achieves an average 44% improvement in translation quality. |
|[MatMul-free LLMs.](https://arxiv.org/abs/2406.02528) | claims that memory consumption can be reduced by more than 10x by using an optimized kernel during inference; suggests an implementation that removes matrix multiplication operations from LLMs while maintaining performance at billion-parameter scales; the performance gap between full precision Transformers and the MatMul-free models narrows as the model size increases.|
|[Buffer of Thoughts .](https://arxiv.org/abs/2406.04271) |utilizes a meta-buffer containing high-level thoughts (thought templates) extracted from problem-solving processes to present a thought-augmented reasoning approach that improves the accuracy, efficiency, and robustness of LLM-based reasoning. The relevant thought template is then retrieved and instantiated with task-specific reasoning structures for the thought-augmented reasoning process. It shows SOTA performance on 10 difficult tasks at 12% of the cost of multi-query prompting methods such as Tree-of-Thoughts. |
|[SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales.](https://arxiv.org/abs/2405.20974) | supervised finetuning on a dataset containing summaries of the differences between multiple reasoning chains is performed by the training framework to teach LLMs to express more accurate fine-grained confidence estimates and self-reflective rationales. Reinforcement learning is then applied to calibrate confidence estimates, encouraging the LLM to produce accurate, high-confidence predictions and penalizing overconfidence in erroneous outputs.|
|[The Geometry of Categorical and Hierarchical Concepts in Large Language Models.](https://arxiv.org/abs/2406.01506) |investigates the geometry of categorical concepts and how the hierarchical relations between them are encoded in LLMs. It discovers that the hierarchical structure is reflected in the representation of complex concepts by polytopes made from direct sums of simplices, while simple categorical concepts are represented as simplices by the LLMs. |
|[Show, Don't Tell: Aligning Language Models with Demonstrated Feedback.](https://arxiv.org/abs/2406.00888) |suggests a technique that uses a very small number of demonstrations as feedback to align LLMs to a particular setting; it outperforms few-shot prompting, SFT, and self-play methods on the tested benchmarks and aligns LLM outputs to a user's demonstrated behaviors. Additionally, it can learn fine-grained style and task alignment across domains. |
|[Towards Scalable Automated Alignment of LLMs.](https://arxiv.org/abs/2406.01252) | gives a summary of the techniques used to align LLMs and examines the four orientations listed below: 1) Inductive bias alignment; 2) Behavior imitation alignment; 3) Model feedback alignment; and 4) Environment feedback alignment|
|[AgentGym: Evolving Large Language Model-based Agents across Diverse Environments.](https://arxiv.org/abs/2406.04151) | a novel framework with multiple tasks and contexts for wide-ranging, concurrent, and real-time agent exploration; constructs a generally competent LLM-based agent with the ability to self-evolve and investigates its potential beyond data that hasn't been seen before across tasks and environments.|
|[Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment.](https://arxiv.org/abs/2406.04295) |A Synthetic-Domain Alignment (SDA) framework has been developed by researchers to improve test-time adaptation (TTA) techniques. By fine-tuning pretrained models with synthetic data produced by a conditional diffusion model, SDA efficiently aligns source and synthetic domains. |
|[ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization.](https://arxiv.org/abs/2406.04312v1) | Reward-based Noise Optimization (ReNO) is a novel technique to improve Text-to-Image (T2I) models during inference by employing signals from reward models with human preferences to optimize the baseline noise.|
|[YOLO-World: Real-Time Open-Vocabulary Object Detection.](https://arxiv.org/abs/2401.17270v1) | With YOLO-World, researchers have improved the widely used YOLO object detectors and included open-vocabulary detection. This method, which combines large-scale dataset training with vision-language modeling, enables it to swiftly and accurately detect a wide range of objects, even in situations for which it was not designed.|
|[Improved Scene Landmark Detection for Camera Localization.](https://arxiv.org/abs/2401.18083v1) | Using distinctive scene landmarks, researchers have developed a novel, privacy-friendly technique for camera localization. This method, which does not rely on real 3D point clouds for localization, is very accurate and storage-efficient since it makes use of 3D scene landmarks and a CNN-based heatmap.|
|[Proofread: Fixes All Errors with One Tap.](https://arxiv.org/abs/2406.04523) | The Gboard team has described how they correct sentence- and paragraph-level problems in written text on device using SFT on a PaLM2-XS model. They discovered that latency optimizations led to significant gains in utilization.|
|[BitsFusion: 1.99 bits Weight Quantization of Diffusion Model.](https://snap-research.github.io/BitsFusion/) | Using a new quantization approach, the Snap Research team was able to increase speed while reducing the size of the Stable Diffusion UNet model from 1.72 GB to 219 MB. Although the quantization technique is a little complicated, it shows great promise for generative model execution on consumer hardware.|
|[Introducing Apple’s On-Device and Server Foundation Models.](https://machinelearning.apple.com/research/introducing-apple-foundation-models) | During WWDC 2024, Apple debuted "Apple Intelligence". Apple Intelligence is an AI system that is built into macOS Sequoia, iOS 18, and iPadOS 18. It has sophisticated generative models for a variety of commonplace activities, like text refinement, picture generation, and notification summary. With an emphasis on user privacy and responsible AI development, this system integrates cloud and on-device capabilities to improve the user experience across all Apple products.|
|[OVMR: Open-Vocabulary Recognition with Multi-Modal References.](https://arxiv.org/abs/2406.04675v1) | OVMR is a novel approach that combines textual descriptions with sample photos to improve open-vocabulary recognition.|
|[Predictive Dynamic Fusion.](https://arxiv.org/abs/2406.04802v1) |The Predictive Dynamic Fusion (PDF) architecture solves stability and reliability problems to improve multimodal learning. |
|[Compute Better Spent: Replacing Dense Layers with Structured Matrices.](https://arxiv.org/abs/2406.06248) |The Linear layers are where Transformer computation is primarily done. This approach creates a structured representation with better scaling laws than naive dense layers, using less CPU than muP and Monarch matrices. |
|[CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models.](https://arxiv.org/abs/2406.06007v1) |A thorough methodology called CARES is used to assess the reliability of Medical Large Vision Language Models (Med-LVLMs). |
|[Learning to Route Among Specialized Experts for Zero-Shot Generalization.](https://arxiv.org/abs/2402.05859v1) | PHATGOOSE is an approach that dramatically increases an AI's capacity to generalize and learn new tasks without prior exposure by efficiently routing between different specialized language models for each portion of a task.|
|[Diabetic Retinopathy Detection.](https://arxiv.org/abs/2406.06384v1) |A unique framework that enhances the grading of diabetic retinopathy (DR), a condition that can result in visual impairment, has been developed by researchers. |
|[BERTs are Generative In-Context Learners.](https://arxiv.org/abs/2406.04823) |In a different universe, BERT models—rather than their decoder-only GPT counterparts—would have been shown to be in-context learners. When that is the case, as this paper investigates, BERTs perform remarkably well in information retrieval but poorly in knowledge acquisition, most likely as a result of the bidirectional attention mechanism. |
|[TextGrad: Automatic "Differentiation" via Text.](https://arxiv.org/abs/2406.07496) | The concept of treating a language model that is capable of updating text as a backpropagation system is investigated in this study. The benchmark performance, not computationally matched against baseline models, shows significant increases, according to the researchers.|
|[Improve Mathematical Reasoning in Language Models by Automated Process Supervision.](https://arxiv.org/abs/2406.06592) |DeepMind found a great way to extend the labor-intensive process of process oversight that requires human intervention. With robust base models, it was able to automate a significant portion of the procedure, which resulted in significant mathematical reasoning performance on Gemini Pro tuned models. |
|[Autoregressive Model Beats Diffusion: 🦙 Llama for Scalable Image Generation.](https://github.com/FoundationVision/LlamaGen) | For image generation, Llama Gen is an autoregressive model that scales better than diffusion alternatives. By using ImageNet to train a class-conditioned model, its researchers were able to raise the bar for FID.|
|[When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models.](https://arxiv.org/abs/2406.07368v1) |In order to address the efficiency concerns in autoregressive big language models, researchers have looked into combining speculative decoding with linear attention techniques. In order to improve training and performance, this work presents an augmentation strategy for linear attention that is consistent with speculative decoding. |
|[What If We Recaption Billions of Web Images with LLaMA-3?](https://arxiv.org/abs/2406.08478) |Using a vision model to caption online scraped photos significantly enhances downstream model performance. This is particularly valid for models like CLIP. |
|[Hearing Anything Anywhere.](https://masonlwang.com/hearinganythinganywhere/) |This research presents DiffRIR, a new framework that uses a planar scene reconstruction with a limited number of room impulse response (RIR) recordings to recreate the spatial acoustic properties of environments. |
|[Simple and Effective Masked Diffusion Language Models.](https://github.com/kuleshov-group/mdlm) | By using an efficient training recipe and incorporating a simpler Rao-Blackwellized objective, researchers have shown that masked discrete diffusion models can compete with autoregressive approaches in language modeling.|

## News
|Link|description|
|---|---|
|[First NHS physiotherapy clinic run by AI to start this year.](https://www.theguardian.com/society/article/2024/jun/09/first-nhs-physiotherapy-clinic-run-by-ai-to-start-this-year) |New platform to provide same-day appointments with digital physiotherapist in effort to cut waiting times|
|[Apple to launch iOS 18 AI features marketed as ‘Apple Intelligence’.](https://9to5mac.com/2024/06/07/report-apple-to-launch-ios-18-ai-features-marketed-as-apple-intelligence/) |Bloomberg’s Mark Gurman today reports that Apple will launch its upcoming AI initiatives in iOS 18 and other operating systems under the brand name ‘Apple Intelligence’, which is obviously a convenient twist on the ‘AI’ acronym. |
|[Claude’s Character.](https://www.anthropic.com/research/claude-character) |Claude is not simply your average, sycophantic AI that nods in agreement with the user. A character version of Constitutional AI has been specifically used to create Claude's personality and character. This essay goes into great detail on how Claude uses post-training to control the kind of output that he typically produces in order to portray this desired character. |
|[Databricks + Tabular.](https://www.databricks.com/blog/databricks-tabular) |With the acquisition of Tabular, Databricks has brought together major players from Apache Iceberg and Delta Lake to concentrate on data format interoperability for its lakehouse architecture. With Delta Lake UniForm's compatibility solution at the forefront, the objective is to establish a single, open standard for data interoperability in order to prevent data silos. |
|[How the voices for ChatGPT were chosen.](https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/) |We worked with industry-leading casting and directing professionals to narrow down over 400 submissions before selecting the 5 voices. |
|[OpenAI and Apple announce partnership to integrate ChatGPT into Apple experiences.](https://openai.com/index/openai-and-apple-announce-partnership/) |Apple is integrating ChatGPT into experiences within iOS, iPadOS, and macOS, allowing users to access ChatGPT’s capabilities—including image and document understanding—without needing to jump between tools.  |
|[Apple Intelligence: every new AI feature coming to the iPhone and Mac.](https://www.theverge.com/2024/6/10/24175405/wwdc-apple-ai-news-features-ios-18-macos-15-iphone-ipad-mac?utm_source=tldrai) |pple announced “Apple Intelligence” at WWDC 2024, its name for a new suite of AI features for the iPhone, Mac, and more. Starting later this year, Apple is rolling out what it says is a more conversational Siri, custom, AI-generated “Genmoji,” and GPT-4o access that lets Siri turn to OpenAI’s chatbot when it can’t handle what you ask it for. |
|[Asana says its new AI teammates are ready to manage your projects.](https://www.fastcompany.com/91134681/asana-ai-teammates-dustin-moskovitz-interview) |With the goal of enhancing productivity and output quality, Asana has introduced "AI teammates" to take care of duties like proactive project detail organization and request triaging. This innovative feature is integrated into the workflow and functions like a human team member while yet being supervised by humans. It was showcased at Asana's Work Innovation Summit. |
|[Apple stock reaches record high after announcement of new AI features.](https://www.theguardian.com/technology/article/2024/jun/11/apple-stock-reaches-record-high) |Tech giant’s shares climb 7% a day after reveal of artificial intelligence features meant to increase appeal of the iPhone |
|[Elon Musk abruptly withdraws lawsuit against Sam Altman and OpenAI.](https://www.theguardian.com/technology/article/2024/jun/11/elon-musk-withdraws-lawsuit-against-sam-altman-openai) |Tesla CEO had accused the company of abandoning mission of creating artificial intelligence for greater good of humanity |
|[Mistral raises €600m series B.](https://threadreaderapp.com/thread/1800558395872731379.html) | Mistral announced €600M in Series B funding for thier first anniversary|
|[Mozilla Builders.](https://future.mozilla.org/builders/blog/announcing-mozilla-builders/) | Local AI, which enhances accessibility and privacy by bringing AI models and applications directly onto personal devices, is being embraced by the first Mozilla Builders Accelerator. Tools for developer productivity, locally based AI agents, dynamic user interfaces, fine-tuning adaption, retrieval-augmented creation, and enhanced function calling are some of the key areas of advancement. The initiative's goal is for participants to create an open-source, decentralized AI ecosystem with a focus on user empowerment.|
|[CaseMark Raises $1.7M to Empower Attorneys with AI.](https://www.casemark.ai/post/fueling-the-future-casemark-raises-1-7m-to-empower-attorneys-with-ai) |In order to increase the scope of its AI solutions for the legal sector, Gradient Ventures led pre-seed investment in CaseMark, an AI firm that is transforming legal operations. |
|[OpenAI ex-employees worry about company’s control over their millions of dollars in shares.](https://www.cnbc.com/2024/06/11/openai-insider-stock-sales-are-raising-concern-among-ex-employees-.html) | With OpenAI’s valuation soaring and an IPO nowhere in sight, the company is giving employees the chance to sell some equity in secondary transactions. Ex-employees sitting on millions of dollars worth of stock worry about OpenAI’s ability to force them to give up their shares, according to sources and internal messages. OpenAI recently circulated a document indicating that ex-employees who work at competitors are not included in the tender offers.|
|[Announcing the Open Release of Stable Diffusion 3 Medium.](https://stability.ai/news/stable-diffusion-3-medium) |Stable Diffusion 3 Medium is Stability AI’s most advanced text-to-image open model yet. The small size of this model makes it perfect for running on consumer PCs and laptops as well as enterprise-tier GPUs.|
|[Shutterstock ImageAI, Powered by Databricks.](https://www.databricks.com/company/newsroom/press-releases/introducing-shutterstock-imageai-powered-databricks-image) |  Databricks and Shutterstock announced a text-to-image Generative AI model optimized for enterprise use|
|[OpenAI Annualized Revenue Doubles.](https://seekingalpha.com/news/4115380-openai-annualized-revenue-doubles-to-hit-34b-report) | OpenAI has more than doubled its annualized revenue to hit $3.4B.|
|[Perplexity was planning revenue-sharing deals with publishers when it came under media fire.](https://www.semafor.com/article/06/12/2024/perplexity-was-planning-revenue-sharing-deals-with-publishers) | Perplexity, the AI search startup that recently came under fire from Forbes for allegedly misusing its content, was already working on revenue-sharing deals with high-quality publishers.|
|[Microsoft’s Nadella Is Building an AI Empire. OpenAI Was Just the First Step.](https://www.wsj.com/tech/ai/microsoft-nadella-openai-inflection-9727e77a?st=8kan7s8rxto660t) |After landing the deal that launched his company to the front of the artificial-intelligence race, the tech chief is spreading his bets. Will it be enough? |
|[OpenAI adds former NSA chief to its board.](https://www.axios.com/2024/06/13/open-ai-security-nakasone-nsa) |OpenAI said on Thursday that it is adding former NSA head and retired Gen. Paul Nakasone to its board of directors as well as its newly formed Safety and Security Committee. Why it matters: OpenAI is looking to convince skeptics that it is taking sufficient steps to ensure its models are safe as it works toward its goal of super intelligence. |
|[Apple Made Once-Unlikely Deal With Sam Altman to Catch Up in AI.](https://www.bloomberg.com/news/articles/2024-06-05/why-is-apple-aapl-teaming-up-with-openai-both-companies-need-each-other) |An OpenAI agreement is due to be announced at the Apple’s developer conference next week. |
|[LLM-Squared .](https://sakana.ai/llm-squared/) | Sakana AI has found a preference optimization scheme that works better than DPO by using an evolutionary approach. It trained models based on code that was suggested by a language model. It has a few suggested variations with very high performance after about 100 generations.|
|[Gemini 1.5 Pro and 1.5 Flash GA, 1.5 Flash tuning support, higher rate limits, and more API updates.](https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/) |Updates to the Gemini API and Google AI Studio have been released by Google AI. These include support for model tuning, the stable release of Gemini 1.5, increased API rate limitations, additional JSON schema features, and mobile compatibility. The changes boost the alternatives available to developers more efficiently and more customized large-scale building. |
|[AI generated sound effects are here.](https://elevenlabs.io/blog/sound-effects-are-here/) | A new AI audio model from ElevenLabs can generate a variety of voices, tunes, and sound effects based on text cues. By utilizing Shutterstock's audio library, our partnership helps media professionals create better content by facilitating the quick and scalable production of high-quality audio. ElevenLabs' platform makes it simple for users to create sounds, which streamlines the audio design process.|
|[OpenAI welcomes Sarah Friar (CFO) and Kevin Weil (CPO).](https://openai.com/index/openai-welcomes-cfo-cpo/) |With the appointment of Kevin Weil as CPO and Sarah Friar as CFO, OpenAI has strengthened its leadership team to further its goal of developing AI products and doing research that are useful to developers, businesses, and consumers. |
|[Why the pope has the ears of G7 leaders on the ethics of AI.](https://www.theguardian.com/world/article/2024/jun/14/why-the-pope-has-the-ears-of-g7-leaders-on-the-ethics-of-ai) |Pope Francis is leaning on thinking of Paolo Benanti, a friar adept at explaining how technology can change world |
|[AI used to predict potential new antibiotics in groundbreaking study.](https://www.theguardian.com/society/article/2024/jun/05/ai-antibiotic-resistance) |Scientists used an algorithm to mine ‘the entirety of the microbial diversity’ on Earth, speeding up antibiotic resistance research |


## Resources
|Link|description|
|---|---|
|[Spreadsheet Is All You Need.](https://github.com/dabochen/spreadsheet-is-all-you-need) |Complete GPT-2 style transformer model with all weights, parameters, and connections included in a spreadsheet. It is a tiny model that runs entirely within the rows and columns of a spreadsheet and is based on NanoGPT. |
|[Inspectus.](https://github.com/labmlai/inspectus) | Inspectus is a versatile visualization tool for large language models. It runs smoothly in Jupyter notebooks via an easy-to-use Python API. Inspectus provides multiple views, offering diverse insights into language model behaviors.|
|[SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model.](https://www.anjiecheng.me/SpatialRGPT) | SpatialRGPT is a powerful vision-language model adept at understanding both 2D and 3D spatial arrangements. It can process any region proposal, such as boxes or masks, and provide answers to complex spatial reasoning questions.|
|[Thread.](https://github.com/squaredtechnologies/thread) | Thread is a Jupyter Notebook that combines the experience of OpenAI's code interpreter with the familiar development environment of a Python notebook. With Thread, you can use natural language to generate cells, edit code, ask questions or fix errors all while being able to edit or re-run code as you would in a regular Jupyter Notebook.|
|[How AI Image Models Work.](https://every.to/p/how-ai-image-models-work) | Since 2022, AI image production has advanced beyond producing images with text explanations. This article illustrates the quick progress and promise of AI in visual creation by explaining how these models hone chaotic inputs to create precise and detailed visuals using a kid's game comparison.|
|[Active Stereo Without Pattern Projector.](https://vppstereo.github.io/) | Without the need for a hardware pattern projector, researchers have presented a new framework that incorporates active stereo concepts into passive cameras that are commonly used.|
|[GLM-4-9B-Chat.](https://huggingface.co/THUDM/glm-4-9b-chat) |Excellent model with support for 26 languages, trained on 10T tokens by the Tsinghua KEM group. |
|[DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data.](https://direct-3d.github.io/) | DIRECT-3D is a new text-to-3D generative model that directly generates 3D contents in a single forward pass without optimization.|
|[Together MoA.](https://www.together.ai/blog/together-moa) |Together has presented Mixture of Agents (MoA), a cutting-edge technique that mixes many LLMs for optimal performance, outperforming GPT-4o with an AlpacaEval 2.0 score of 65.1%. MoA employs a tiered architecture in which aggregators in later levels improve the initial answers from different models, improving output quality through cooperation. Even with improved precision, MoA still struggles with latency. Reducing latency and improving model design are two potential future possibilities. |
|[Mistral.rs.](https://github.com/EricLBuehler/mistral.rs) |Mistral.rs is a fast LLM inference (Rust-based inference framework) platform supporting inference on a variety of devices, quantization, and easy-to-use application with an Open-AI API compatible HTTP server and Python bindings. |
|[Generalizable Human Gaussians from Single-View Image.](https://jinnan-chen.github.io/projects/HGM/) |A diffusion-guided framework for building 3D human models from a single image is the Human Gaussian Model (HGM). |
|[Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis.](https://srameo.github.io/projects/le3d/) |Real-time HDR view synthesis from RAW pictures can be achieved with the LE3D approach. It works especially well for situations set at night. |
|[TORAX.](https://github.com/google-deepmind/torax) | The Python-Jax differentiable fusion tokamak simulator developed by DeepMind at Google is now publicly available. The simulator supports several very powerful PDEs and has good auto-diff capabilities.|
|[AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising.](https://czg1225.github.io/asyncdiff_page/) | A novel acceleration approach called AsyncDiff makes it possible to perform parallel processing in diffusion models. By splitting the noise prediction model into several parts and executing them on different devices, it drastically cuts latency without sacrificing quality.|
|[PowerInfer-2: Fast Large Language Model Inference on a Smartphone.](https://powerinfer.ai/v2/) |Fast inference on the phone for the special Mistral 47B MoE model. |
|[The AXLearn Library for Deep Learning.](https://github.com/apple/axlearn) |AXLearn is a library built on top of JAX and XLA to support the development of large-scale deep learning models. |
|[Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling.](https://github.com/microsoft/Samba) |Samba is a simple yet powerful hybrid model with an unlimited context length. Its architecture is frustratingly simple: Samba = Mamba + MLP + Sliding Window Attention + MLP stacking at the layer level. |
|[DiffusionKit.](https://github.com/argmaxinc/DiffusionKit) | Framework and tooling for running diffusion models on Apple's MLX framework.|
|[Splash Attention.](https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py) |new DeepMind kernel in Jax for Sparse Flash Attention |
|[Hugging Face acquires Agrilla.](https://huggingface.co/posts/dvilasuero/203008804842390) |Argilla a company specialized on data for preference optimization has been acquired. |


## Perspectives
|Link|description|
|---|---|
|[Building AI products.](https://www.ben-evans.com/benedictevans/2024/6/8/building-ai-products) |Though they can't give exact answers to questions, large language models (LLMs) like ChatGPT are excellent at producing responses that seem correct. In order to improve user experience and enhance functionality while reducing errors, AI in the future will integrate LLMs into specialized tools or embed them into already-existing applications. This will contextualize AI outputs within controllable, specified areas. |
|[Why passwords still matter in the age of AI.](https://www.theguardian.com/technology/article/2024/jun/11/apple-password-app-tech-age-of-ai) |As Apple’s new Passwords app tries to solve our identity crisis, why are we still proving who we are via strings of random characters? |
|[Examining LLM performance on public benchmarks.](https://threadreaderapp.com/thread/1785888203943161970.html) | Popular LLMs on public benchmarks: how overfit are they? Mistral and Phi are overfitting benchmarks, but GPT, Claude, Gemini, and Llama are not, according to new research from Scale AI SEAL. The scientists assessed public LLMs for overfitting on GSM8k and created a new eval GSM1k.|
|[How to track the economic impact of public investments in AI.](https://www.nature.com/articles/d41586-024-01721-1) | National statistics systems should recognize the researchers whose ideas drive artificial-intelligence applications, not just machines and factory outputs.|
|[Maintaining Large-Scale AI Capacity At Meta.](https://engineering.fb.com/2024/06/12/production-engineering/maintaining-large-scale-ai-capacity-meta/) |To meet AI demands, Meta is modernizing its data centers throughout the world. For AI training tasks, it intends to scale to 600,000 GPUs. In order to assure minimal disruptions and constant performance while enabling quick infrastructure scalability, this calls for creative maintenance tactics and tools like OpsPlanner. |

# ML news: Week 3 - 9 June

## Research
|Link|description|
|---|---|
|[Contextual Position Encoding: Learning to Count What's Important.](https://arxiv.org/abs/2405.18719) |The general position encoding method can attend to the i-th particular word, noun, or sentence; it improves perplexity on language modeling and coding tasks; it is context-dependent and can represent different levels of position abstraction; it suggests a new position encoding method, CoPE, to enable the position to be conditioned on context by incrementing position only on certain tokens. |
|[Faithful Logical Reasoning via Symbolic Chain-of-Thought.](https://arxiv.org/abs/2405.18357) |suggests a way to enhance LLMs' capacity for logical thinking by combining logical rules and symbolic expressions with chain-of-thought (CoT) prompting; this prompting method is known as Symbolic Chain-of-Thought and it is a fully LLM-based framework that consists of the following important steps: converts the context of natural language to symbolic format, 2) creates a step-by-step solution plan based on symbolic logical rules, and 3) employs a verifier to validate the translation and reasoning chain. |
|[Transformers Can Do Arithmetic with the Right Embeddings.](https://arxiv.org/abs/2405.17399) | The main problem this work addresses is the inability of transformers to track the exact position of digits; they do this by adding an embedding to each digit that encodes its position relative to the start of the number; these gains also transfer to multi-step reasoning tasks that include sorting and multiplication. achieves 99% accuracy on 100-digit addition problems by training on only 20-digit numbers with a single GPU.|
|[GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning.](https://arxiv.org/abs/2405.20139) | blends the reasoning powers of GNNs with the language understanding skills of LLMs in a RAG fashion; the GNN extracts relevant and useful graph information, and the LLM uses the information to answer questions over knowledge graphs (KGQA); GNN-RAG outperforms or matches GPT-4 performance with a 7B tuned LLM, and improves vanilla LLMs on KGQA.|
|[Attention as an RNN.](https://arxiv.org/abs/2405.13956) |is based on the parallel prefix scan algorithm, which enables efficient computation of attention's many-to-many RNN output. It achieves comparable performance to Transformers on 38 datasets while being more time and memory-efficient. presents a new attention mechanism that can be trained in parallel (like Transformers) and updated with new tokens requiring constant memory usage for inferences (like RNNs). |
|[Are Long-LLMs A Necessity For Long-Context Tasks? ](https://arxiv.org/abs/2405.15318) |suggests a reasoning framework to allow short-LLMs to handle long-context tasks by adaptively accessing and utilizing the context based on the tasks presented; it breaks down the long context into short contexts and processes them using a decision-making process. The argument makes the claim that long-LLMs are not necessary to solve long-context tasks. |
|[Sparse maximal update parameterization: A holistic approach to sparse training dynamics.](https://arxiv.org/abs/2405.15743) |All frontier model labs use muP, a potent tool, to transfer hyper parameters fine-tuned on tiny models to bigger, more costly training runs. This study investigates how to achieve that for sparse models, resulting in significantly better training results and lower computation expenses. |
|[Exploring Color Invariance through Image-Level Ensemble Learning.](https://arxiv.org/abs/2401.10512v1) | To address color bias in computer vision, researchers have created a novel learning technique called Random Color Erasing. By selectively excluding color information from training data, this technique strikes a balance between the significance of color and other parameters, producing models that perform better in challenging situations like industrial and wide-area surveillance.|
|[Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models.](https://github.com/coniferlm/conifer) |Conifer enhances LLMs' comprehension of intricate instructions by utilizing a progressive learning methodology and a customized dataset. |
|[LLM Merging Competition: Building LLMs Efficiently through Merging.](https://llm-merging.github.io/) |Sakana AI is sponsoring the LLM Merging challenge at NeurIPS this year. |
|[Tribeca to Screen AI-Generated Short Films Created by OpenAI’s Sora.](https://www.indiewire.com/news/festivals/tribeca-ai-generated-short-films-sora-shorts-1235010911/) |Short films generated by artificial intelligence are popping up at more and more film festivals, and the largest event yet is dedicating an entire section to AI-generated movies. |
|[Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning.](https://arxiv.org/abs/2405.12217v1) |A technique called InvariantSelectPR is intended to make Large Multimodal Models (LMMs) more adaptive in domain-specific fields such as healthcare. |
|[TAIA: Large Language Models are Out-of-Distribution Data Learners.](https://arxiv.org/abs/2405.20192v1) |A technique called TrainAllInfAttn improves the performance of big language models in niche markets with little data. |
|[MegActor: Harness the Power of Raw Video for Vivid Portrait Animation](https://megvii-research.github.io/MegFaceAnimate/) |A new model called MegActor uses unprocessed driving videos to create more lifelike portrait animation. It addresses identity leaking and background interference and produces remarkable results with unique data creation framework and background encoding approaches. |
|[MeshXL: Neural Coordinate Field for Generative 3D Foundation Models.](https://arxiv.org/abs/2405.20853) | MeshXL is a new model that generates high-quality 3D meshes.|
|[Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays.](https://github.com/sunzc-sunny/ppad) |Position-guided Prompt learning method for Anomaly Detection in chest X-rays (PPAD). PPAD leverages learnable text prompt and image prompt to minimize the gap between pre-training data and task-specific data. Through the position-guided prompts, the model can focus on various regions, simulating the diagnostic process of experts. |
|[Tree Diffusion: Diffusion Models For Code.](https://tree-diffusion.github.io/) |Wonderful diffusion paper that diffuses picture code. As part of the diffusion process, it has the ability to directly edit. Although it is sluggish, it can be simply used with search to significantly increase one's capacity for reasoning. |
|[Improved Techniques for Optimization-Based Jailbreaking on Large Language Models.](https://arxiv.org/abs/2405.21018v1) | Expanding upon the Greedy Coordinate Gradient (GCG) approach, researchers have enhanced methods for optimization-based jailbreaking of huge language models.|
|[ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation.](https://ssyang2020.github.io/zerosmooth.github.io/) |A training-free video interpolation technique for generative video diffusion models has been developed by researchers. This novel method improves frame rates without requiring a lot of training or big datasets and works with different models. |
|[A whole-slide foundation model for digital pathology from real-world data.](https://www.nature.com/articles/s41586-024-07441-w) | Prov-GigaPath, a whole-slide pathology foundation model pretrained on 1.3 billion 256 × 256 pathology image tiles in 171,189 whole slides. To pretrain Prov-GigaPath, we propose GigaPath, a novel vision transformer architecture for pretraining gigapixel pathology slides. We further demonstrate the potential of Prov-GigaPath on vision–language pretraining for pathology by incorporating the pathology reports. In sum, Prov-GigaPath is an open-weight foundation model that achieves state-of-the-art performance on various digital pathology tasks, demonstrating the importance of real-world data and whole-slide modelling.|
|[DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models.](https://zzzyuqing.github.io/dreammat.github.io/) |Using Dream Mat to enhance 3D object texture production is a brilliant idea. Given a 3D model, it employs several traditional graphic methods including Metallic, Roughness, and Albedo to generate a very appealing result. |
|[LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing.](https://arxiv.org/abs/2406.02350v1) | To solve classification problems in large language models (LLMs), researchers have developed LlamaCare, a refined LLM for medical information, in conjunction with Extended Classification Integration (ECI).|
|[XRec: Large Language Models for Explainable Recommendation.](https://github.com/hkuds/xrec) | XRec is a framework independent of models that improves explainable recommender systems by utilizing the language capabilities of huge language models.|
|[MetaMixer Is All You Need.](https://arxiv.org/abs/2406.02021v1) |Using simply convolutions, researchers have created a novel method called FFNification that preserves the query-key-value structure while converting self-attention processes into more effective token mixers. |
|[GrootVL: Tree Topology is All You Need in State Space Model.](https://github.com/easonxiao-888/grootvl) | By dynamically constructing a tree topology based on spatial correlations and input information, GrootVL is a network that enhances state space models.|
|[ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization.](https://arxiv.org/abs/2406.01906v1) | In order to increase Visual Geo-localization (VG) and boost its performance in applications such as SLAM, augmented reality, and autonomous driving, researchers have created a new two-stage training process.|
|[ReLUs Are Sufficient for Learning Implicit Neural Representations.](https://arxiv.org/abs/2406.02529v1) | A review of the application of ReLU activation functions to implicit neural representations (INRs) learning has been conducted. They countered spectrum bias by introducing basic limitations to ReLU neurons, which were inspired by second-order B-spline wavelets.|

## News
|Link|description|
|---|---|
|[OpenAI Is Restarting Its Robotics Research Group.](https://www.therobotreport.com/openai-is-restarting-its-robotics-research-group/) |The San Francisco-based company has been a pioneer in generative artificial intelligence and is returning to robotics after a three-year break. |
|[AI Overviews: About last week.](https://blog.google/products/search/ai-overviews-update-may-2024/) |In order to improve search results and give users more precise and pertinent information, particularly for complex inquiries, Google created AI Overviews. While there were certain problems, such incorrect results and misread content, Google has fixed these difficulties with over a dozen technical updates, like improving the identification of absurd questions and reducing the amount of user-generated content in AI Overviews. |
|[Nvidia said to be prepping AI PC chip with Arm and Blackwell cores.](https://www.theregister.com/2024/05/28/nvidia_ai_pc_arm_blackwell_core) | Competition could be heating up in the Windows on Arm space amid talk in the industry that Nvidia is readying a chip pairing next-gen Arm cores with its Blackwell GPU architecture.|
|[Ex-OpenAI board member reveals what led to Sam Altman's brief ousting.](https://www.msn.com/en-ae/money/companies/ex-openai-board-member-reveals-what-led-to-sam-altman-s-brief-ousting/ar-BB1ndnZE) |In a recent interview, former OpenAI board member Helen Toner provided fresh information into the circumstances surrounding CEO Sam Altman's November dismissal. It appears that the board was informed via Twitter about the release of ChatGPT. According to Toner, Altman had repeatedly lied to the board. It has been alleged that Altman had been lying about events within the organization for years and hiding facts. The board found it difficult to make decisions as a result of his lies, and they concluded that he wasn't the best person to take the firm to AGI. |
|[AI hardware firm Nvidia unveils next-gen products at Taiwan tech expo.](https://www.theguardian.com/technology/article/2024/jun/02/ai-hardware-firm-nvidia-unveils-next-gen-products-at-taiwan-tech-expo) |CEO Jensen Huang tells packed stadium in Taipei ‘next Industrial Revolution has begun’ |
|[AMD unveils new AI chips to compete with Nvidia.](https://www.fastcompany.com/91134766/amd-unveils-new-ai-chips-to-compete-with-nvidia) | AMD has been vying to compete against Nvidia, which currently dominates the lucrative market for AI semiconductors and commands about 80% of its share.|
|[Anthropic’s Claude 3 Opus and tool use are generally available on Vertex AI.](https://cloud.google.com/blog/products/ai-machine-learning/anthropics-claude-3-opus-and-tool-use-are-generally-available-on-vertex-ai) | Google Cloud now offers Claude 3 Opus with tool use along with the smaller models as part of its Vertex AI offering.|
|[State Space Duality (Mamba-2).](https://goombalab.github.io/blog/2024/mamba2-part1-model/) |Mambda is an effective model of state space. A lengthy and comprehensive explanation of the model and its enhancements is included in the second version that its team has issued. |
|[No physics? No problem. AI weather forecasting is already making huge strides.](https://arstechnica.com/ai/2024/06/as-a-potentially-historic-hurricane-season-looms-can-ai-forecast-models-help/) | With AI models like WindBorne's WeatherMesh, which leverages the extensive ERA5 dataset to outperform conventional models while using much less processing power, the weather forecasting industry is undergoing a transformation.|
|[Amazon’s Project PI AI looks for product defects before they ship.](https://www.theverge.com/2024/6/3/24170567/amazons-project-pi-product-defect-return-ai-computer-vision) |  Project PI combines computer vision and generative AI to catch damaged items and prevent returns.|
|[The Opaque Investment Empire Making OpenAI’s Sam Altman Rich.](https://wallstreetsights.com/business/openais-sam-altman-get-rich/4975/) |One of Silicon Valley's most active and successful individual investors is Sam Altman. At the beginning of this year, his stakes in his investment empire were valued at least $2.8 billion. A large portion of the portfolio is unknown. Readers are guided through Altman's investment knowledge in this article. |
|[Even the Raspberry Pi is getting in on AI.](https://www.theverge.com/2024/6/4/24170818/raspberry-pi-ai-chip-hailo-devices) |Raspberry Pi partnered with Hailo to provide an optional AI add-on to its microcomputers. |
|[Using AI to decode dog vocalizations.](https://news.umich.edu/using-ai-to-decode-dog-vocalizations) |Leveraging a human speech model to identify different types of barks.  University of Michigan researchers are exploring the possibilities of AI, developing tools that can identify whether a dog’s bark conveys playfulness or aggression. |
|[The future is … sending AI avatars to meetings for us, says Zoom boss.](https://www.theguardian.com/technology/article/2024/jun/05/the-future-is-sending-ai-avatars-to-meetings-for-us-says-zoom-boss) | Eric Yuan suggests technology is five or six years away and will free up time to spend with family|
|[AI researchers build ‘future self’ chatbot to inspire wise life choices.](https://www.theguardian.com/technology/article/2024/jun/05/ai-researchers-build-future-self-chatbot-to-inspire-wise-life-choices) |Scientists at MIT hope talking to 60-year-old self will shift thinking on health, money and work |
|[Cartwheel generates 3D animations from scratch to power up creators.](https://techcrunch.com/2024/06/05/cartwheel-generates-3d-animations-from-scratch-to-power-up-creators/) |Animating a 3D character from scratch is generally both laborious and expensive, requiring the use of complex software and motion capture tools. |
|[Mistral launches fine-tuning API.](https://mistral.ai/news/customization/) |Mistral has launched customization for its models via its platform and API. |
|[If you aren't seeing AI Overviews in your search results, it's probably thanks to Google.](https://www.androidpolice.com/google-ai-overviews-scaled-back-rocky-launch/) |After receiving heavy criticism since their mid-May public launch, AI Overviews in Google Search have dropped in visibility across search results. Since I/O, the average percentage of queries where AI Overviews appear has dropped from 27 percent to just 11 percent. Despite the reduction, healthcare-related queries a large percentage of AI results, raising concerns about both accuracy and reliability across Google.|
|[Google optimizes shipping routes.](https://research.google/blog/heuristics-on-the-high-seas-mathematical-optimization-for-cargo-ships/) |The mathematical optimization for cargo shipping routes was enhanced by Google's operations research group. They discovered a 13% drop in gasoline expenses and consumption. |
|[BrightEdge Releases Post Google I/O Data on The Impact of AI Overviews.](https://www.globenewswire.com/news-release/2024/06/04/2893289/0/en/BrightEdge-Releases-Post-Google-I-O-Data-on-The-Impact-of-AI-Overviews.html) | The main businesses affected by AI Overviews, what generates results, and where Google automatically anticipates and responds to search inquiries are all revealed by new research from BrightEdge Generative Parser.|
|[Nvidia emails: Elon Musk diverting Tesla GPUs to his other companies.](https://arstechnica.com/cars/2024/06/elon-musk-is-diverting-teslas-gpus-to-x-xai-nvidia-emails-say/) | The Tesla CEO is accused of diverting resources from the company again. Elon Musk is yet again being accused of diverting Tesla resources to his other companies. This time, it's high-end H100 GPU clusters from Nvidia. |
|[Securing Research Infrastructure for Advanced AI.](https://openai.com/index/securing-research-infrastructure-for-advanced-ai) |In its description of the security architecture of its AI training supercomputers, OpenAI highlights the use of Azure-based infrastructure and Kubernetes for orchestration to safeguard critical model weights and other assets. |
|[Extracting Concepts from GPT-4.](https://openai.com/index/extracting-concepts-from-gpt-4) |The team at OpenAI has discovered 16 million interpretable features in GPT-4 including price increases, algebraic rings, and who/what correspondence. This is a great step forward for SAE interpretability at scale. They shared the code in a companion GitHub repository. |
|[Mesop: Gradio Competition.](https://google.github.io/mesop/) |A rival to the well-liked AI prototyping framework Gradio has been made available by Google. Gradio is more mature than Mesop, which is pure Python and slightly more composable. |
|[Nvidia is now more valuable than Apple at $3.01 trillion.](https://www.theverge.com/2024/6/5/24172363/nvidia-apple-market-cap-valuation-trillion-ai) | The AI boom has pushed Nvidia’s market cap high enough to make it the second most valuable company in the world.|

## Resources
|Link|description|
|---|---|
|[An Introduction to Vision-Language Modeling.](https://arxiv.org/abs/2405.17247) |  we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them.|
|[Aya 23: Open Weight Releases to Further Multilingual Progress.](https://arxiv.org/abs/2405.15032) |a family of multilingual language models with up to 23 languages supported; it demonstrates that it can perform better on those particular languages than other large-scale multimodal models by purposefully concentrating on fewer languages and allocating greater capacity to them. |
|[Financial Statement Analysis with Large Language Models](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311) |claims that by analyzing trends and financial ratios, LLMs can produce insightful insights; demonstrates that GPT-4 outperforms more specialized models; and develops a profitable trading strategy based on GPT's predictions. |
|[SimPO: Simple Preference Optimization with a Reference-Free Reward.](https://arxiv.org/abs/2405.14734) | SimPO demonstrates how it outperforms other methods like DPO and claims to generate the strongest 8B open-source model. It is a more straightforward and efficient method for preference optimization with a reference-free reward; it uses the average log probability of a sequence as an implicit reward (i.e., no reference model required), which makes it more compute and memory efficient.|
|[Experimenting with local alt text generation.](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/) |A model that runs in the browser and can provide alt text for web photos automatically has been trained by Mozilla. |
|[Mora: More like Sora for Generalist Video Generation.](https://github.com/lichao-sun/mora) |Mora is a multi-agent framework designed to facilitate generalist video generation tasks, leveraging a collaborative approach with multiple visual agents. It aims to replicate and extend the capabilities of OpenAI's Sora.|
|[FABRIC: Personalizing Diffusion Models with Iterative Feedback.](https://github.com/sd-fabric/fabric) |FABRIC (Feedback via Attention-Based Reference Image Conditioning) is a technique to incorporate iterative feedback into the generative process of diffusion models based on StableDiffusion. |
|[KL is All You Need.](https://blog.alexalemi.com/kl-is-all-you-need.html) |KL divergence is a quick, affordable, and effective method of measuring a certain type of distance between objects. In both conventional and contemporary AI, it is widely employed. This piece examines the potent idea both mathematically and graphically. |
|[7 Ways AI-Native Companies Can Improve User Retention.](https://a16z.com/7-ways-ai-native-companies-can-improve-retention/) |a manual with examples of how businesses like Perplexity, Civit, Lapse, Omnivore, and others are using them to increase retention for founders and product executives. |
|[FineWeb: decanting the web for the finest text data at scale.](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) | The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. Recently, we released 🍷 FineWeb, a new, large-scale (15-trillion tokens, 44TB disk space) dataset for LLM pretraining. FineWeb is derived from 96 CommonCrawl snapshots and produces better-performing LLMs than other open pretraining datasets.|
|[An entirely open-source AI code assistant inside your editor.](https://ollama.com/blog/continue-code-assistant) | Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.|
|[MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark.](https://arxiv.org/abs/2406.01574) | A popular benchmark for reasoning tasks is MMLU. It is frequently seen as the gold standard and as something that models overfit. A new, more rigorous, and refined benchmark called MMLU Pro is used to gauge language model reasoning.|
|[Omost.](https://github.com/lllyasviel/Omost) |Omost gives you control over how your images are generated. It comes from the same designer as ControlNet. First, it rewrites the prompts into a collection of illustrative code. After that, it renders the finished image using that. Crucially, you can modify the code either prior to or following generation in order to subtly alter the model's output. |
|[Control-GIC.](https://github.com/lianqi1008/Control-GIC) |A novel generative image compression framework called Control-GIC enables fine-grained bitrate modification while preserving high-quality output. |
|[LLM inference speed of light.](https://zeux.io/2024/03/15/llm-inference-sol/) |Using theoretical speed of light modeling as grounding is extremely significant for problems where the amount of computation and memory access is known a priori as it helps assess the quality of implementations and predict the impact of architectural modifications. |
|[Neural Surface Reconstruction.](https://github.com/prstrive/gens) | Without the need for 3D supervision, GenS is an end-to-end generalizable neural surface reconstruction model that performs exceptionally well at reconstructing surfaces from multi-view images.|
|[MatMul-Free LM.](https://github.com/ridgerchu/matmulfreellm) |Even at the billion-parameter scale, researchers have managed to remove matrix multiplication (MatMul) from huge language models without sacrificing speed. |
|[stable-audio-open-1.0 .](https://huggingface.co/stabilityai/stable-audio-open-1.0) | The weights for Stable Audio, which was trained to produce sound effects on audio samples with permissive licenses, have been released by Stability AI.|
|[CV-VAE: A Compatible Video VAE for Latent Generative Video Models.](https://ailab-cvc.github.io/cvvae/index.html) |With its spatio-temporally compressed latent spaces, CV-VAE is a video VAE that works with current image and video models to efficiently train new ones utilizing pre-trained ones. |
|[Qwen2.](https://qwenlm.github.io/blog/qwen2/) | Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B.Having been trained on data in 27 additional languages besides English and Chinese. Having been trained on data in 27 additional languages besides English and Chinese. State-of-the-art performance in a large number of benchmark evaluations|
|[Dragonfly: A large vision-language model with multi-resolution zoom.](https://www.together.ai/blog/dragonfly-v1) | We are also launching two new open-source models  Llama-3-8b-Dragonfly-v1 a general-domain model trained on 5.5 million image-instruction pairs and Llama-3-8b-Dragonfly-Med-v1 finetuned on additional 1.4 biomedical image-instruction data. Dragonfly demonstrates promising performance on vision-language benchmarks like commonsense visual QA and image captioning. Dragonfly-Med outperforms prior models, including Med-Gemini on multiple medical imaging tasks, showcasing its capabilities for high-resolution medical data.|
|[MMLU Pro.](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro) |The industry standard for assessing knowledge and reasoning in language models is MMLU. |


## Perspectives
|Link|description|
|---|---|
|[Beyond the Cloud: Distributed AI and On-Device Intelligence.](https://sidstage.substack.com/p/beyond-the-cloud-distributed-ai-and) | Transition of AI workflows from cloud to the edge with specialized chip infrastructure & models, multi-modality and ambience across devices|
|[Sure, Google’s AI overviews could be useful – if you like eating rocks.](https://www.theguardian.com/commentisfree/article/2024/jun/01/sure-googles-ai-overviews-could-be-useful-if-you-like-eating-rocks) |The company that shaped the development of search engines is banking on chatbot-style summaries. But so far, its suggestions are pretty wild |
|[AI's Communication Revolution: We're All Talking to Computers Now.](https://www.digitalnative.tech/p/ais-communication-revolution-were) | With its real-time integration of text, vision, and audio, OpenAI's GPT-4o is driving a revolution in communication through AI. As a result, human-to-AI communication becomes a fundamental form of digital connection and has the potential to bring about substantial societal changes as well as the emergence of new companies focused on AI-centric communication. This transition makes it possible for more natural interactions with AI.|
|[A Right to Warn about Advanced Artificial Intelligence.](https://righttowarn.ai/) | A group of AI workers, both present and past, is pleading with advanced AI companies to adopt values that guarantee openness and safeguard workers who voice concerns about risks. They emphasize how important it is for businesses to refrain from enforcing non-disparagement agreements, to make anonymous reporting procedures easier, to encourage candid criticism, and to shield whistleblowers from reprisals.|
|[Will Scaling Solve Robotics?](https://spectrum.ieee.org/solve-robotics) |The Conference on Robot Learning, which included 11 workshops and nearly 200 submitted papers, drew over 900 attendees last year. Whether it was possible to tackle robotics problems by training a huge neural network on a large data set was one of the main points of contention throughout the event. To help readers better comprehend the topic, this piece offers the opposing viewpoints. Scaling has been successful in several related domains. It is not feasible, though, because there is a lack of readily available robotics data and no obvious method for obtaining it. Scaling, even if it performs as well as it does in other domains, is probably not going to solve robotics. |
|[Plentiful, high-paying jobs in the age of AI.](https://www.noahpinion.blog/p/plentiful-high-paying-jobs-in-the) |Due to comparative advantage, it's feasible that a large number of professions that humans currently perform will be performed by humans eternally, regardless of how much better AIs become at those tasks. |
|[What I learned from looking at 900 most popular open source AI tools.](https://huyenchip.com/2024/03/14/ai-oss.html) |The goal of this study of open source AI repositories is to provide readers with a broad overview of the intimidating AI ecosystem. |
|[Meta AI system is a boost to endangered languages — as long as humans aren’t forgotten.](https://www.nature.com/articles/d41586-024-01619-y) |Automated approaches to translation could provide a lifeline to under-resourced languages, but only if companies engage with the people who speak them. |
|[Misinformation poses a bigger threat to democracy than you might think.](https://www.nature.com/articles/d41586-024-01587-3) |In today’s polarized political climate, researchers who combat mistruths have come under attack and been labelled as unelected arbiters of truth. But the fight against misinformation is valid, warranted and urgently required. |
|[Is AI misinformation influencing elections in India?](https://www.nature.com/articles/d41586-024-01588-2) |A sample of roughly two million WhatsApp messages highlights urgent concerns about the spread and prevalence of AI-generated political content. |
|[I'm Bearish OpenAI.](https://stovetop.substack.com/p/im-bearish-openai) |A shift toward products and a research brain drain should ring your alarm bells |
|[The future of foundation models is closed-source.](https://blog.johnluttig.com/p/the-future-of-foundation-models-is) |if the centralizing forces of data and compute hold, open and closed-source AI cannot both dominate long-term |
|[A Grand Unified Theory of the AI Hype Cycle.](https://blog.glyph.im/2024/05/grand-unified-ai-hype.html) | Over the years, the AI sector has experienced multiple hype cycles, each of which produced really useful technology and outlasted the previous one. Instead of following an exponential process, every cycle adheres to a sigmoid one. There is an inevitable limit to any technology development strategy, and it is not too difficult to find. Although this AI hype cycle is unlike any other that has come before it, it will probably go in the same direction.|
|[Hi, AI: Our Thesis on AI Voice Agents.](https://a16z.com/ai-voice-agents/) |The current state of AI speech agents is described in a blog post and deck created by Andreessen Horowitz, along with potential areas for advancement and investment. It outlines the present state of the B2B and B2C application layer landscape and covers the top infrastructure stack. |






















