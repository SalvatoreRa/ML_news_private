# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


#############################################
# On working

# ML news: 

## Research
|Link|description|
|---|---|
|[Agents Are Not Enough.](https://www.arxiv.org/abs/2412.16241) |This work argues that AI agents, while promising, cannot fully solve the challenges of autonomous task execution. It proposes an ecosystem comprising three components: Agents (focused modules for specific tasks), Sims (digital representations of user preferences and behaviors), and Assistants (coordinators between users, Sims, and Agents). |
|[2 OLMo 2 Furious.](https://arxiv.org/abs/2501.00656) |This work introduces an improved architecture, advanced training methods, and a specialized data mixture called Dolmino Mix 1124. Released in 7B and 13B parameter scales with fully transparent training data and code, the model matches or exceeds the performance of open-weight models like Llama 3.1 and Qwen 2.5 while requiring fewer computational resources. Its instruction-tuned version, OLMo 2-Instruct, remains competitive with comparable models. |
|[Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs.](https://arxiv.org/abs/2412.21187) | This work proposes a self-training strategy to address overthinking in o1-like LLMs, reducing token output by 48.6% while maintaining accuracy on the MATH500 test set, as demonstrated with QwQ-32B-Preview.|
|[MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes.](https://arxiv.org/abs/2412.19260) | MEDEC is a publicly available benchmark for medical error detection and correction in clinical notes, focusing on five error types: Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism. It includes 3,848 clinical texts, with 488 clinical notes from three U.S. hospital systems. Experiments show that Claude 3.5 Sonnet excels in error detection, while o1-preview outperforms in error correction.|
|[Aviary: training language agents on challenging scientific tasks.](https://arxiv.org/abs/2412.21154) |An extensible open-source gymnasium designed to develop language agents that outperform zero-shot frontier LLMs and even humans on various challenging scientific tasks. |
|[Memory Layers at Scale.](https://arxiv.org/abs/2412.09764) |This work demonstrates the scalability and effectiveness of memory layers, showing that models equipped with these layers outperform traditional dense models using half the computation, especially on factual tasks. It includes a parallelizable memory layer implementation that scales to 128B memory parameters and 1 trillion training tokens, validated against base models up to 8B parameters. |
|[HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs.](https://arxiv.org/abs/2412.18925) |This work introduces a novel approach to enhance medical reasoning in language models through a medical verifier that validates outputs and guides the development of complex reasoning skills. The system combines fine-tuning and reinforcement learning with verifier-based rewards in a two-stage process, achieving superior performance over existing models using just 40,000 verifiable medical problems. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[Amazon aims to branch into UK internet market with satellite broadband plan.](https://www.theguardian.com/business/2025/jan/05/amazon-aims-to-branch-into-uk-internet-market-with-satellite-broadband-plan) | Proposed space launches within next two years could ultimately deliver mobile phone signal even to most remote areas|
|[Memo to Trump: US telecoms is vulnerable to hackers. Please hang up and try again.](https://www.theguardian.com/commentisfree/2025/jan/04/memo-to-trump-us-telecoms-is-vulnerable-to-hackers-please-hang-up-and-try-again) | State-backed cyberspies are exploiting ageing infrastructure to penetrate every corner of the US government, it seems – even its phone-tapping systems|
|[How Elon Musk’s X became the global right’s supercharged front page.](https://www.theguardian.com/technology/2025/jan/04/elon-musk-x-trump-far-right) |Musk has now used X as a platform to make aggressive interventions in US politics – and in those of other countries |
|[Meta is killing off its own AI-powered Instagram and Facebook profiles.](https://www.theguardian.com/technology/2025/jan/03/meta-ai-powered-instagram-facebook-profiles) | Instagram profile of ‘proud Black queer momma’, created by Meta, said her development team included no Black people|
|[Football coaches could soon be calling on AI to scout the next superstar.](https://www.theguardian.com/technology/2025/jan/04/football-coaches-could-soon-be-calling-on-ai-to-scout-the-next-superstar) |Technologists claim managers could wish for specific player attributes and AI would suggest perfect youth prospect |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning.](https://openreview.net/forum?id=YXnwlZe0yf&noteId=yrsGpHd0Sf) |Putnam-AXIOM, a new math reasoning benchmark, includes 236 Putnam Competition problems and 52 variations. The best-performing model, OpenAI's o1-preview, achieves only 41.95% accuracy on the original problems and fares significantly worse on the variations. |
|[1.58-bit FLUX.](https://arxiv.org/abs/2412.18653) |This work introduces the first successful quantization of the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (values in {-1, 0, +1}). The approach leverages self-supervision from the FLUX.1-dev model and preserves comparable performance in generating 1024 x 1024 images to the original model. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[Machine-Assisted Proof.](https://www.ams.org//notices/202501/rnoti-p6.pdf) | This work explores how mathematicians have historically used machines to aid research and highlights recent AI tools revolutionizing mathematical proof assistance.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |































































































