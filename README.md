# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

# ON WORKING

## Research
|Link|description|
|---|---|
|[Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models.](https://arxiv.org/abs/2410.07176) | Introduces a novel RAG method to address the challenges of imperfect retrieval augmentation and knowledge conflicts in LLMs. Astute RAG adaptively extracts critical information from the internal knowledge of LLMs, then iteratively merges this with external knowledge while maintaining source awareness. Its interactive consolidation mechanism enhances the integration of internal and external information by identifying consistent passages, detecting conflicting data, and filtering out irrelevant content.|
|[ToolGen: Unified Tool Retrieval and Calling via Generation.](https://arxiv.org/abs/2410.03439) |Incorporates tool knowledge directly into LLMs by encoding tools as unique tokens, allowing the model to generate tool calls and arguments, facilitating smooth tool invocation alongside natural language generation. Experiments involving over 47,000 tools demonstrate that ToolGen outperforms in both tool retrieval and autonomous task execution. |
|[Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG.](https://arxiv.org/abs/2410.05983) | Finds that in many long-context LLMs, output quality diminishes as the number of passages increases, with the performance decline attributed to retrieved hard negatives. The authors propose two methods to enhance long-context LLM-based RAG: retrieval reordering and RAG-specific tuning with intermediate reasoning to improve relevance identification. These approaches show marked improvements in both accuracy and robustness in long-context RAG performance.|
|[GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models.](https://arxiv.org/abs/2410.05229) |Evaluates several state-of-the-art (SoTA) models using a benchmark built with symbolic templates that allow for a range of mathematical problems. The results show that LLMs display variability when answering different versions of the same questions, and their performance drops when numerical values in the questions are adjusted. As the complexity of the questions increases (e.g., adding more clauses), performance deteriorates significantly. The authors suggest that this decline in performance is likely due to a lack of logical reasoning capabilities in current LLMs. |
|[Addition is All You Need for Energy-efficient Language Models.](https://arxiv.org/abs/2410.00907) | Introduces an algorithm that approximates floating-point multiplication using integer addition operations, making it computationally less intensive than 8-bit floating-point arithmetic while achieving higher precision. The authors report that implementing the proposed L-Mul operation in tensor processing hardware could potentially reduce energy consumption by 95% for elementwise floating-point tensor multiplications and by 80% for dot product operations.|
|[I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy.](https://arxiv.org/abs/2410.07109) |Examines the interaction patterns of LLMs within a multi-agent setting involving a social hierarchy, specifically in a scenario where a guard and a prisoner interact, with the prisoner either seeking extra yard time or attempting to escape. The study finds that when power dynamics are present, LLMs struggle to maintain coherent conversations. Additionally, the authors highlight that agents' personas significantly influence their behaviors. Interestingly, even without explicit prompting, merely assigning roles to agents resulted in the emergence of anti-social behaviors. |
|[Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201) |The paper revisits RNNs and demonstrates that removing the hidden states from the input, forget, and update gates allows for efficient parallel training. This adjustment eliminates the need for architectures like LSTMs and GRUs to rely on backpropagation through time (BPTT). They introduce new variants, called minLSTMs and minGRUs, which are 175 times faster for sequences of length 512. |
|[LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations.](https://arxiv.org/abs/2410.02707) |The study finds that "truthfulness" information in LLMs is concentrated in specific tokens, offering a way to improve error detection and address related challenges. They also suggest that the internal representations of LLMs can be used to predict the types of errors these models are prone to making. |
|[Archon: An Architecture Search Framework for Inference-Time Techniques.](https://arxiv.org/abs/2409.15254) | The paper presents a modular framework for constructing and optimizing LLMs by integrating various inference-time techniques. This approach redefines the task of LLM system design as a hyperparameter optimization problem. Tested on benchmarks like MT-Bench and CodeContests, the framework, named Archon, outperforms top models such as GPT-4o and Claude 3.5 Sonnet, achieving a 15.1% average accuracy improvement.|
|[RATIONALYST: Pre-training Process-Supervision for Improving Reasoning.](https://arxiv.org/abs/2410.01044) | RATIONALYST is a model designed for process-supervision of reasoning, enabling it to generalize across a wide range of reasoning tasks. This is accomplished by pre-training on a dataset of 79k rationales from the Pile and a variety of reasoning datasets, with minimal human involvement. Fine-tuned from LLaMa-3-8B, the model achieves a 3.9% average accuracy improvement across seven reasoning benchmarks. |
|[Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation.](https://arxiv.org/abs/2409.12941) | The paper introduces a unified framework to evaluate an LLM’s capability to provide factual responses, assess retrieval skills, and reason through the generation of final answers. The framework includes multi-hop questions that require combining information from multiple sources. It reports that state-of-the-art LLMs struggle with this task, achieving only 40% accuracy without retrieval. However, the proposed multi-step retrieval method improves performance to 66% accuracy.|
|[Not All LLM Reasoners Are Created Equal.](https://arxiv.org/abs/2410.01748) | The paper introduces a unified framework to evaluate an LLM’s capability to provide factual responses, assess retrieval skills, and reason through the generation of final answers. The framework includes multi-hop questions that require combining information from multiple sources. It reports that state-of-the-art LLMs struggle with this task, achieving only 40% accuracy without retrieval. However, the proposed multi-step retrieval method improves performance to 66% accuracy.|
|[Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis.](https://arxiv.org/abs/2409.17439) |Training generative models like GANs with limited data is challenging. Existing Implicit Maximum Likelihood Estimation (IMLE) methods suffer from poor alignment between the latent codes used during training and those used during inference. The proposed approach, RS-IMLE, modifies the prior distribution during training, resulting in better test-time performance and higher quality image generation. |
|[Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models.](https://arxiv.org/abs/2410.11081) | This study introduces a unified framework aimed at enhancing training stability in continuous-time consistency models, leading to substantial improvements in the performance of generative models.|
|[DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection.](https://arxiv.org/abs/2410.11181v1) | DARNet is an innovative model for auditory attention detection (AAD) that improves the decoding of brain signals, such as EEG, by integrating spatiotemporal and dual attention mechanisms.|
|[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads.](https://github.com/mit-han-lab/duo-attention) | DuoAttention is a framework designed to optimize memory usage and reduce latency in long-context large language models (LLMs) by selectively applying full key-value (KV) caching to only the most essential attention heads.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[AI gives voice to dead animals in Cambridge exhibition.](https://www.theguardian.com/science/2024/oct/14/ai-gives-voice-to-dead-animals-in-cambridge-exhibition) | Creatures can converse and share their stories by voice or text through visitors’ mobile phones at Museum of Zoology |
|[Three-armed robot conductor makes debut in Dresden.](https://www.theguardian.com/world/2024/oct/13/three-armed-robot-maira-pro-s-conductor-makes-debut-dresden) | German city’s Sinfoniker says aim is not to replace humans but to play music human conductors would find impossible |
|[Tesla’s value drops $60bn after investors fail to hail self-driving ‘Cybercab’.](https://www.theguardian.com/business/2024/oct/11/teslas-value-drops-60bn-after-self-driving-cybercab-fails-to-excite-investors) | Analysts criticise lack of detail about the ‘robotaxi’ showcased by CEO Elon Musk|
|[Microsoft may have an audio-to-image generator in the works, new patent shows.](https://www.zdnet.com/article/microsoft-may-have-an-audio-to-image-generator-in-the-works-new-patent-shows/) |Microsoft has submitted a patent for an AI system that transforms live audio into images using large language models (LLMs). The system is intended to improve communication by creating real-time visuals from audio streams. Once developed, it could potentially be incorporated into Microsoft Teams through Copilot integration. |
|[Australia’s spy chief warns AI will accelerate online radicalisation.](https://www.theguardian.com/australia-news/2024/oct/11/australias-spy-chief-warns-ai-will-accelerate-online-radicalisation) |Asio boss Mike Burgess says social media impact is a ‘step-change’ in the threat posed by extremism |
|[Google to buy nuclear power for AI datacentres in ‘world first’ deal.](https://www.theguardian.com/technology/2024/oct/15/google-buy-nuclear-power-ai-datacentres-kairos-power) | Tech company orders six or seven small nuclear reactors from California’s Kairos Power|
|[Silicon Valley is debating if AI weapons should be allowed to decide to kill.](https://techcrunch.com/2024/10/11/silicon-valley-is-debating-if-ai-weapons-should-be-allowed-to-decide-to-kill/) | In late September, Shield AI co-founder Brandon Tseng swore that weapons in the U.S. would never be fully autonomous — meaning an AI algorithm would make the final decision to kill someone. “Congress doesn’t want that,” the defense tech founder told TechCrunch. “No one wants that.” |
|[Zoom’s custom AI avatar tool may come with risks.](https://techcrunch.com/2024/10/09/zooms-custom-ai-avatar-tool-may-come-with-risks/) | The upcoming feature, announced today at Zoom’s annual dev conference, will translate a video clip that users record of themselves into a digital clone — complete with a head, upper arms, and shoulders. Users will be able to type a script of what they want the digital double to say, and Zoom will generate audio that syncs with the avatar’s lip movements.|
|[Generate Video (beta) on Firefly Web App.](https://blog.adobe.com/en/publish/2024/10/14/generate-video-beta-on-firefly-web-app) | During the Adobe MAX conference, Adobe revealed the extension of its Firefly series of creative generative AI models to include video. |
|[OpenAI appoints international expansion boss.](https://www.theregister.com/2024/10/09/openai_appoints_international_expansion_boss/) |OpenAI has named Oliver Jay as the head of its international expansion, with a focus on AI strategy and operations. The company also revealed the opening of a new APAC office in Singapore and is working on developing datasets for local languages. The o1 model, which incorporates "chain of thought" methods, is designed to improve AI accuracy. |
|[Anthropic challenges OpenAI with affordable batch processing.](https://venturebeat.com/ai/anthropic-challenges-openai-with-affordable-batch-processing/) |Anthropic has introduced a Message Batches API, enabling businesses to handle large data volumes at half the cost of traditional API calls. The API allows for up to 10,000 asynchronous queries within 24 hours, providing a cost-efficient solution by shifting AI processing from real-time to "right-time." This approach encourages AI adoption among mid-sized companies but may draw attention away from the advancement of real-time AI capabilities. |
|[OpenAI Projections Imply Losses Tripling To $14 Billion In 2026.](https://www.xm.com/research/markets/allNews/reuters/openai-projections-imply-losses-tripling-to-14-billion-in-2026-the-information-53942296) | OpenAI projects losses to rise to $14 billion in 2026, with total losses reaching $44 billion by 2028.|
|[AMD launches AI chip to rival Nvidia's Blackwell.](https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html) | AMD has introduced the Instinct MI325X AI chip, targeting competition with Nvidia's leading data center GPUs.|
|[Meta’s open AI hardware vision.](https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/) |Meta unveiled its open AI hardware designs, including the Catalina rack and the enhanced Grand Teton platform, at the OCP Global Summit. Notably, training the Llama 3.1 405B model required 16,000 NVIDIA H100 GPUs, demonstrating Meta's robust scaling infrastructure. These open AI hardware systems are essential for driving further advancements in AI capabilities. |
|[The New York Times warns AI search engine Perplexity to stop using its content.](https://www.theverge.com/2024/10/15/24270774/new-york-times-cease-and-desist-letter-perplexity-ai-search-engine) |The New York Times has sent a cease and desist letter to AI startup Perplexity, accusing the company of using its content without authorization for AI search purposes. Perplexity asserts that it does not scrape content for training but instead indexes web pages to provide factual information. The company is currently in discussions with publishers and seeks to resolve the matter by collaborating with the Times and other media organizations. |
|[Decagon raises $65m Series B led by Bain Capital Ventures to bring total funding to $100m.](https://decagon.ai/blog/series-b) |Decagon has secured $65 million in Series B funding to further develop its AI customer support agents, which are already utilized by companies such as Duolingo and Eventbrite to streamline customer interactions. These AI agents automate routine tasks, allowing customer support teams to focus on more strategic roles. The funding will be used to strengthen Decagon's engineering team and extend its AI solutions into new markets and industry sectors. |
|[New high quality AI video generator Pyramid Flow launches — and it’s fully open source!](https://venturebeat.com/ai/new-high-quality-ai-video-generator-pyramid-flow-launches-and-its-fully-open-source/) |The number of AI video generation models continues to grow with a new one, Pyramid Flow, launching this week and offering high quality video clips up to 10 seconds in length — quickly, and all open source. |
|[This three-person robotics startup is working with designer Yves Béhar to bring humanoids home.](https://techcrunch.com/2024/10/13/this-three-person-robotics-startup-is-working-with-designer-yves-behar-to-bring-humanoids-home/) |Kind Humanoid's three-person team is developing a whimsical humanoid robot named Mona, specifically designed for home use rather than industrial applications. The team aims to conduct field tests with a dozen initial prototypes next year. Unlike many AI-driven robotics companies that focus on industrial markets and heavy fundraising, Kind prioritizes innovation and efficiency, setting its approach apart from competitors in the robotics space. |
|[INTELLECT–1: Launching the First Decentralized Training of a 10B Parameter Model.](https://www.primeintellect.ai/blog/intellect-1) | INTELLECT-1 is the first decentralized model with 10 billion parameters, designed to harness global contributions for open-source AGI development. It utilizes OpenDiLoCo scaling to train large models across distributed devices, with innovations in bandwidth efficiency and fault tolerance. The new Prime framework further enhances decentralized training by optimizing compute utilization, achieving a 98% utilization rate during INTELLECT-1's 10-billion-parameter training run. This marks a significant advancement in decentralized AI model training.|
|[Elon Musk Shows Off Tesla ‘Robotaxi’ That Drives Itself.](https://www.nytimes.com/2024/10/10/business/tesla-robotaxi-elon-musk.html) |“You could fall asleep and wake up at your destination,” said Mr. Musk, Tesla’s C.E.O., but some experts are skeptical that such cars will be ferrying passengers soon. |
|[ByteDance lays off hundreds of TikTok employees in shift to AI content moderation.](https://techcrunch.com/2024/10/11/bytedance-lays-off-hundreds-of-tiktok-employees-in-shift-to-ai-content-moderation/) |ByteDance’s TikTok is laying off hundreds of employees, mainly in Malaysia, according to Reuters. The cuts come as the social network is increasingly turning to AI for content moderation. The cuts do not impact employees in the U.S. |
|[Microsoft Artificial Intelligence VP Bubeck to Join OpenAI.](https://finance.yahoo.com/news/microsoft-artificial-intelligence-vp-bubeck-193734013.html) | Microsoft Corp. said one of its artificial intelligence vice presidents, Sebastien Bubeck, is leaving to join OpenAI, where Microsoft is both the largest investor and a rival.|
|[‘It’s not me, it’s just my face’: the models who found their likenesses had been used in AI propaganda.](https://www.theguardian.com/technology/2024/oct/16/its-not-me-its-just-my-face-the-models-who-found-their-likenesses-had-been-used-in-ai-propaganda) |London-based Synthesia’s technology was employed to make deepfake videos for authoritarian regimes |
|[Amazon.com joins push for nuclear power to meet data center demand.](https://www.theguardian.com/technology/2024/oct/16/amazon-nuclear-power-data-center) |Company says it signed three agreements on developing small modular reactor nuclear power technology |
|[Un Ministral, des Ministraux.](https://mistral.ai/news/ministraux/) | On the first anniversary of Mistral 7B, Mistral launched two advanced models designed for on-device and edge computing: Ministral 3B and Ministral 8B. These models are optimized for tasks under 10 billion parameters, offering superior knowledge, reasoning, and efficiency. They also support a context length of up to 128k and deliver faster inference.|
|[Former Palantir CISO Dane Stuckey joins OpenAI to lead security.](https://techcrunch.com/2024/10/15/former-palantir-ciso-dane-stuckey-joins-openai-to-lead-security/) |Dane Stuckey, the former CISO of analytics firm Palantir, has joined OpenAI as its newest CISO, serving alongside OpenAI head of security Matt Knight. |
|[Can AI really compete with human data scientists? OpenAI’s new benchmark puts it to the test.](https://venturebeat.com/ai/can-ai-really-compete-with-human-data-scientists-openai-new-benchmark-puts-it-to-the-test/) |OpenAI has introduced a new tool to measure artificial intelligence capabilities in machine learning engineering. The benchmark, called MLE-bench, challenges AI systems with 75 real-world data science competitions from Kaggle, a popular platform for machine learning contests. |
|[Adobe’s AI video model is here, and it’s already inside Premiere Pro.](https://www.theverge.com/2024/10/14/24268695/adobe-ai-video-generation-firefly-model-premiere-pro) | New beta tools allow users to generate videos from images and prompts and extend existing clips in Premiere Pro.|
|[Customize Audio Overviews with Google's NotebookLM.](https://blog.google/technology/ai/notebooklm-update-october-2024/) | NotebookLM now enables users to customize their Audio Overview experience, providing greater control over the areas of focus and expertise of the AI hosts. Companies can apply for the new NotebookLM Business pilot program, which includes improved tools designed for professional applications.|
|[Combining next-token prediction and video diffusion in computer vision and robotics.](https://news.mit.edu/2024/combining-next-token-prediction-video-diffusion-computer-vision-robotics-1016) | A new method can train a neural network to sort corrupted data while anticipating next steps. It can make flexible plans for robots, generate high-quality video, and help AI agents navigate digital environments.|
|[Nvidia just dropped a new AI model that crushes OpenAI’s GPT-4—no big launch, just big results.](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/) |Nvidia quietly unveiled a new artificial intelligence model on Tuesday that outperforms offerings from industry leaders OpenAI and Anthropic, marking a significant shift in the company’s AI strategy and potentially reshaping the competitive landscape of the field. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering.](https://arxiv.org/abs/2410.07095) | It introduces a new benchmark to assess machine learning agents' proficiency in machine learning engineering tasks. The benchmark consists of 75 Kaggle competitions focused on key MLE skills, including model training, dataset preparation, and experiment execution. OpenAI's o1-preview model, utilizing the AIDE scaffolding, reaches a bronze medal level in 16.9% of the competitions.|
|[Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System.](https://arxiv.org/abs/2410.08115) |Presents a novel framework aimed at improving both communication efficiency and task effectiveness in LLM-based multi-agent systems through targeted LLM training. It introduces an iterative "generate, rank, select, and train" approach, enhanced by a reward function to optimize performance, token usage, and communication efficiency. The framework integrates Monte Carlo Tree Search-inspired techniques for DPO data generation, promoting diverse exploration. Experimental results show consistent improvements over single-agent baselines and standard multi-agent systems (MAS) using Llama 3 8B, achieving a 2.8x performance boost while utilizing fewer than 10% of tokens on tasks involving extensive information exchange. |
|[Zyphra's Mamba 2 based model beats Mistral.](https://zyphra.webflow.io/post/zamba2-7b) | Introduces the first state space-style model that surpasses transformers at the 7B scale. It excels in understanding and generating long-context data, thanks to the linear time scaling of the Mamba 2 blocks, which significantly enhances its efficiency and performance.|
|[OpenAI's Swarm.](https://github.com/openai/swarm) |OpenAI has introduced a lightweight framework designed to facilitate communication between agents. While it will not receive further updates, the framework could still offer valuable ideas and inspiration for future developments. |
|[EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models.](https://arxiv.org/abs/2410.07133v2) |EvolveDirector aims to develop a competitive text-to-image generation model using open, publicly available resources, avoiding the limitations imposed by proprietary models. |
|[Rethinking the Evaluation of Visible and Infrared Image Fusion.](https://arxiv.org/abs/2410.06811v1) |Researchers propose the Segmentation-oriented Evaluation Approach (SEA) to improve the evaluation of Visible and Infrared Image Fusion (VIF) techniques, which play a critical role in applications such as object detection and semantic segmentation. |
|[A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research.](https://arxiv.org/abs/2410.07066v1) | A gentle introduction and tutorial on deep generative models in transportation research provides a comprehensive overview of how these models can be applied to solve transportation problems.|
|[Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis.](https://github.com/yangling0818/trans4d) |Trans4D is a new framework developed to address the challenges of realistic 4D scene transitions, enhancing text-to-4D synthesis. It offers improved capabilities in generating coherent, dynamic 4D scenes from textual descriptions, making it more suitable for tasks that require accurate spatial and temporal scene transitions. |
|[DocMTAgent.](https://github.com/yutongwang1216/docmtagent) |DelTA, short for Document-levEL Translation Agent, is an online translation tool designed for handling document-level translations. It leverages a multi-level memory architecture to improve translation accuracy and coherence across larger texts, providing more context-aware translations compared to sentence-level models. |
|[Fast Feedforward 3D Gaussian Splatting Compression.](https://yihangchen-ee.github.io/project_fcgs/) |Fast Compression of 3D Gaussian Splatting (FCGS) is a new model designed to eliminate the need for the slow, per-scene optimization required by earlier methods. Instead, FCGS achieves rapid compression using a quick feed-forward pass, reducing the processing time from minutes to just seconds. This significantly accelerates the compression process while maintaining high-quality results for 3D data. |
|[OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling.](https://arxiv.org/abs/2410.08021v1) |OneRef presents an optimized framework for referring segmentation by integrating visual and language feature spaces within a unified transformer architecture. |
|[SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction.](https://arxiv.org/abs/2410.08669v1) |SmartPretrain offers a versatile, model-agnostic, and dataset-agnostic self-supervised learning framework designed to enhance motion prediction in autonomous vehicles. |
|[UvA - An Introduction to Group Equivariant Deep Learning.](https://uvagedl.github.io/) | Resources for studying deep learning techniques applied to specific types of geometric data while addressing architectural limitations.|
|[Diffusion model simulating CS:GO.](https://github.com/eloialonso/diamond/tree/csgo) |An open-source replication of a diffusion model that generates visual simulations of a video game, using keyboard and mouse inputs to influence the output. |
|[Reward-Augmented Data Enhances Direct Preference Alignment of LLMs.](https://github.com/shenao-zhang/reward-augmented-preference) | This study addresses the shortcomings of current alignment algorithms in large language models (LLMs), which tend to overfit to relative preferences and neglect response quality. The authors introduce reward-conditioned LLM policies and a novel data relabeling method that incorporates response quality, enabling the model to better generalize to optimal responses.|
|[entropix.](https://github.com/samefarrar/entropix_mlx/tree/metrics_viz) |Entropix is a tool designed to modify the sampling behavior of language models. |
|[LoLCATs Blog Part 2: How to Linearize LLMs for Me and You.](https://hazyresearch.stanford.edu/blog/2024-10-14-lolcats-p2) |Hazy Research has published another insightful post that delves into techniques for linearizing existing language models while maintaining much of their performance. This exploration highlights methods to simplify model architectures, making them more efficient, without significantly compromising their effectiveness in tasks like text generation and understanding. |
|[TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control.](https://arxiv.org/abs/2410.10133v1) | TextCtrl is a newly introduced diffusion-based method designed to enhance scene text editing. It achieves a balance between maintaining content accuracy and preserving the original style, ensuring that both the textual content and the visual appearance remain consistent during edits.|
|[Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies.](https://arxiv.org/abs/2410.10803v1) | iDP3 is an advanced 3D visuomotor policy designed to enable humanoid robots to autonomously navigate and perform tasks in a variety of real-world environments. This improved policy enhances the robot's ability to perceive and interact with its surroundings, making it more adaptable and efficient in complex and dynamic settings.|
|[tabled.](https://github.com/VikParuchuri/tabled) |Tabled is a small library for detecting and extracting tables. It uses surya to find all the tables in a PDF, identifies the rows/columns, and formats cells into markdown, csv, or html. |
|[HART: Efficient Visual Generation with Hybrid Autoregressive Transformer.](https://hanlab.mit.edu/projects/hart) |HART is a cutting-edge visual generation model designed to produce high-quality 1024x1024 images, presenting a challenge to the capabilities of diffusion models. It enhances image reconstruction and reduces training costs by employing a hybrid tokenizer that integrates both discrete and continuous tokens, resulting in more efficient and effective image generation. |
|[DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention.](https://github.com/maclong01/DeBiFormer) | The Deformable Bi-level Routing Attention (DBRA) module is an innovation designed to enhance attention mechanisms in vision transformers. DeBiFormer, which is built upon DBRA, optimizes the selection of key-value pairs in the attention process, resulting in more efficient computations and better interpretability of queries within attention maps. This leads to improved performance and understanding of how the model attends to different parts of an image. |
|[Six tips for going public with your lab’s software.](https://www.nature.com/articles/d41586-024-03344-y) | It’s not enough to write high-quality programs. If you want to make your apps public — and usable — you should also follow these steps. |
|[CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos.](https://cotracker3.github.io/) | CoTracker is a newly developed tracking model that bridges the performance gap between synthetic and real video data by employing semi-supervised training techniques.|
|[A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration.](https://github.com/renlanghuang/cast) | Researchers have developed a novel consistency-aware spot-guided Transformer designed to improve the efficiency and accuracy of point cloud registration.|
|[Ditto - the simplest self-building coding agent.](https://github.com/yoheinakajima/ditto) |Ditto is a user-friendly tool that allows you to generate a multi-file Flask application from simple natural language descriptions using a no-code interface. By leveraging a simple LLM loop with a few tools, Ditto automates the coding process, (occasionally) turning your ideas into functional web applications (or at least trying and getting close). |
|[F5 Text-to-Speech System.](https://github.com/lucasnewman/f5-tts-mlx) | F5-TTS is a non-autoregressive, zero-shot text-to-speech system featuring a flow-matching mel spectrogram generator and a diffusion transformer. Developed on the MLX framework, F5 outperforms earlier systems such as E2 TTS by incorporating ConvNeXT v2 blocks for improved text alignment, enabling high-quality speech generation in approximately 11 seconds on modern hardware.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[Nobel winner Geoffrey Hinton is the ‘godfather of AI’. Here’s an offer he shouldn’t refuse…](https://www.theguardian.com/commentisfree/2024/oct/12/nobel-winner-geoffrey-hinton-is-the-godfather-of-ai-heres-an-offer-he-shouldnt-refuse) | The computer scientist’s dogged belief in the potential of neural networks helped unlock machine learning. But he’d be wise to remember the experience of a fellow laureate |
|[Machines of Loving Grace.](https://darioamodei.com/machines-of-loving-grace) |Dario Amodei, CEO of Anthropic, often writes internal memos, and one of them was published externally. In this memo, he explores the potential extreme positive impact of successfully building powerful AI systems. He envisions how AI could radically transform the world for the better, improving areas like science, economics, and societal well-being, while acknowledging the immense responsibility in ensuring AI development is aligned with human interests and safety. |
|[This AI-Powered Invention Machine Automates Eureka Moments.](https://spectrum.ieee.org/ai-invention) |Iprova's AI-driven software analyzes diverse technical literature to generate patentable inventions by linking previously unrelated ideas. It uses semantic search and generative AI to identify novel inventions for companies like Procter & Gamble and Panasonic. Although AI plays a key role, human insight remains essential for applying the inventions practically, especially in fast-evolving industries. Iprova highlights the importance of human creativity in refining and validating invention ideas, ensuring that AI serves as a tool to enhance rather than replace human innovation. |
|[Burn the Playbooks.](https://www.notboring.co/p/burn-the-playbooks) | AI excels at tasks that follow structured rulesets, such as automating tax processes or solving math problems, where it can often outperform humans. However, relying too much on playbook-driven approaches in our work risks stifling human creativity, a key trait that differentiates us from machines. Overemphasizing formulaic tasks could make us more dependent on AI's strengths, limiting our own unique creative potential and inadvertently making us more "machine-like" in areas where creativity and flexibility are crucial.|
|[Hurricane Helene and the ‘Fuck It’ Era of AI-Generated Slop.](https://www.404media.co/hurricane-helene-and-the-fuck-it-era-of-ai-generated-slop/) | An AI-generated image depicting Hurricane Helene has gone viral, despite viewers being fully aware that it isn't real. The image has sparked widespread attention and discussion, highlighting the power of AI-generated content to captivate audiences even when the authenticity is known. This trend reflects the growing influence of AI in shaping public perception and the viral nature of digital content.|
|[OpenAI pursues public benefit structure to fend off hostile takeovers.](https://www.ft.com/content/5649b66e-fdb3-46d3-84e0-23e33bdaf363) |OpenAI is planning to restructure as a public benefit corporation (PBC) to safeguard against hostile takeovers and ensure its mission of benefiting humanity remains intact. This change will help OpenAI maintain its commitment to ethical AI development, prioritizing public good over profit while allowing the organization to continue innovating in a sustainable and mission-driven way. |
|[Al Will Take Over Human Systems From Within.](https://www.noemamag.com/al-will-take-over-human-systems-from-within/) | In this post, Yuval Noah Harari, the Israeli historian and author of “Sapiens,” “Homo Deus,” and “Nexus,” explores the impact of information networks and AI on societal narratives, which can either unite or fragment communities. He cautions that AI, functioning as an "alien intelligence," could centralize power due to its lack of self-correcting mechanisms, potentially threatening democratic systems. Harari stresses the importance of strong institutions to uphold truth in a world increasingly influenced by AI-driven decision-making across different sectors.|
|[Sticky humans in a post-AGI world.](https://www.theintrinsicperspective.com/p/sticky-humans-in-a-post-agi-world) | AI tutors encounter considerable difficulties in replicating the social and intellectual interactions offered by human teachers. Although AI has made progress, it still falls short in handling complex educational tasks and cannot deliver the nuanced socio-intellectual experiences that human educators provide. A hybrid approach, where AI complements rather than replaces human teachers, may be more effective, given the essential social and cultural elements of the learning process.|
|[AI has dreamt up a blizzard of new proteins. Do any of them actually work?](https://www.nature.com/articles/d41586-024-03335-z) | Emerging protein-design competitions aim to sift out the functional from the fantastical. But researchers hope that the real prize will be a revolution for the field. |
|[Considerations for governing open foundation models.](https://www.science.org/doi/10.1126/science.adp1848) |Foundation models drive AI innovation, but debates on their release—whether open or closed—raise concerns about potential risks and the impact of regulations on innovation. |
|[I AI-generated some podcasts – and the results are uncanny.](https://www.theguardian.com/tv-and-radio/2024/oct/16/i-ai-generated-some-podcasts-and-the-results-are-uncanny) | Google’s new tool NotebookLM lets you create podcasts at the click of the button. They’re way more realistic than you’d think …|
|[SB 1047: Our Side Of The Story.](https://www.astralcodexten.com/p/sb-1047-our-side-of-the-story) |California's proposed SB 1047, which sought to require AI companies to address existential risks posed by their technologies, was vetoed by Governor Newsom. He argued that the bill did not adequately regulate smaller, potentially dangerous AI models. Despite strong support from AI safety advocates like Dan Hendrycks and high-profile figures such as Elon Musk, the bill faced opposition from major AI companies, including OpenAI and Google. Newsom's veto has sparked discussions within the AI community about future regulatory strategies and potential collaborations with broader political groups to create comprehensive AI safety measures. |
|[Overview of strong human intelligence amplification methods.](https://www.lesswrong.com/posts/jTiSWHKAtnyA723LE/overview-of-strong-human-intelligence-amplification-methods) | Advancements in AI depend on developing humans with enhanced cognitive abilities to effectively manage the complexities of AGI development. Approaches such as brain emulation, genomic modifications, adult brain gene editing, and brain-brain interfaces are being explored, each presenting distinct challenges and risks. These efforts are aimed at solving deep philosophical issues, significantly amplifying human intelligence, and addressing the potential threats posed by AGI.|
|[LLMs don’t do formal reasoning - and that is a HUGE problem.](https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and) | A study conducted by Apple raises questions about the effectiveness of large language models (LLMs), revealing that they primarily depend on pattern matching instead of formal reasoning. This reliance results in fragile and inconsistent outcomes, challenging the robustness of LLMs in tasks requiring deeper cognitive processes.|
|[Why ChatGPT maker OpenAI is at fight with Open AI.](https://timesofindia.indiatimes.com/technology/tech-news/why-chatgpt-maker-openai-is-at-fight-with-open-ai/articleshow/114220808.cms) |OpenAI is currently engaged in a legal dispute with Guy Ravine's company, Open AI, over the rights to the "Open AI" name and the original open-source AI vision. The conflict centers on ownership of the name and the direction of the open-source principles that initially defined the AI development approach. |
|[AI mediation tool may help reduce culture war rifts, say researchers.](https://www.theguardian.com/technology/2024/oct/17/ai-mediation-tool-may-help-reduce-culture-war-rifts-say-researchers) |System built by Google DeepMind team takes individual views and generates a set of group statements |
|[Here’s the deal: AI giants get to grab all your data unless you say they can’t. Fancy that? No, neither do I.](https://www.theguardian.com/commentisfree/2024/oct/18/ai-systems-big-tech-data-ministers) | Data is vital to AI systems, so firms want the right to take it and ministers may let them. We must wake up to the danger|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

# ML news: Week 7 - 13 October

## Research
|Link|description|
|---|---|
|[A multimodal generative AI copilot for human pathology.](https://www.nature.com/articles/s41586-024-07618-3) |PathChat is a vision-language AI assistant designed for pathology, combining a foundational vision encoder and a large language model, achieving state-of-the-art performance on diagnostic tasks and outperforming other multimodal AI systems, with potential applications in education, research, and clinical decision-making. |
|[Meta Movie Gen.](https://ai.meta.com/research/movie-gen/) | Meta has developed a cutting-edge movie model with 30 billion parameters, which required 6,144 H100 GPUs for training. The model was trained using 1 billion images and 100 million carefully selected videos. Notably, it is based on a Temporal Autoencoder and incorporates Flow matching Llama. Meta also published a highly detailed 92-page research paper, making it one of the most comprehensive reports on the subject.|
|[When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1.](https://arxiv.org/abs/2410.01792) |Large language models face limitations because they rely on next token prediction. Although OpenAI's o1 model was trained with a new objective focused on reasoning traces, it still exhibits some of the same constraints associated with next token prediction. |
|[Contextual Document Embeddings.](https://arxiv.org/abs/2410.02525) |This paper presents a method similar to a neutral TF/IDF, as it gathers information from the entire corpus rather than relying on individual document embeddings. It effectively captures contextual information from surrounding documents and has achieved state-of-the-art results on the MTEB benchmark. |
|[PairDistill: Pairwise Relevance Distillation for Dense Retrieval.](https://github.com/miulab/pairdistill) | This project introduces a novel technique called Pairwise Relevance Distillation (PairDistill), aimed at enhancing the accuracy of dense retrieval methods.|
|[Modeling relationships to solve complex problems efficiently.](https://news.mit.edu/2024/julian-shun-solves-complex-problems-efficiently-1004) | Associate Professor Julian Shun develops high-performance algorithms and frameworks for large-scale graph processing.|
|[Factual Accuracy in AI.](https://arxiv.org/abs/2410.01556v1) |Integrative Decoding is a technique designed to improve the factual accuracy of large language models, particularly for open-ended tasks. This method helps ensure more reliable and accurate outputs by refining the model's ability to integrate information during generation. |
|[Dynamic Diffusion Transformer.](https://arxiv.org/abs/2410.03456v1) |The Dynamic Diffusion Transformer (DyDiT) improves the efficiency of diffusion models in image generation by building on the Diffusion Transformer (DiT). It achieves this by dynamically adjusting computational resources across different timesteps and spatial regions, minimizing redundancy and optimizing performance. |
|[Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach.](https://arxiv.org/abs/2410.03160v1) |The Frame-Aware Video Diffusion Model (FVDM) enhances video generation by overcoming the limitations of existing models. Instead of using a single timestep for the entire video clip, FVDM introduces a vectorized timestep variable, enabling each frame to follow its own noise schedule. This approach improves the quality and coherence of generated videos. |
|[What Matters for Model Merging at Scale?](https://arxiv.org/abs/2410.03617) |Model merging is a technique that allows the combination of two models to achieve the performance benefits of both. However, it does not always scale effectively with larger model sizes. This paper investigates the requirements and challenges for making model merging work efficiently with very large models, addressing issues related to scalability, performance trade-offs, and optimal merging strategies. |
|[nGPT: Normalized Transformer with Representation Learning on the Hypersphere.](https://arxiv.org/abs/2410.01131) | A significant amount of research effort is focused on normalizing the internal representations of language models. This study demonstrates that by placing every internal vector on a hypersphere, convergence time is significantly reduced for models of reasonable size, leading to more efficient training.|
|[Genomic Foundation Model Benchmarking.](https://arxiv.org/abs/2410.01784v1) | GFMBench is a newly developed framework aimed at tackling challenges in the development of genomic foundation models (GFMs) by offering standardized benchmarking tools. It supports the evaluation of GFMs with millions of genomic sequences and hundreds of tasks, automating the benchmarking process for open-source GFMs to streamline their development and comparison.|
|[LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations.](https://arxiv.org/abs/2410.02707) |This study provides further evidence that language models internally encode signals when they produce non-factual information. Understanding these internal cues can help guide models more effectively and reduce the occurrence of hallucinations, offering a potential strategy for improving their reliability. |
|[Differential Transformer.](https://arxiv.org/abs/2410.05258) | Transformers often over-allocate attention to irrelevant context, leading to inefficiencies. This research presents the Diff Transformer, which enhances attention to relevant information while filtering out noise. It introduces a differential attention mechanism that computes attention scores by subtracting two separate softmax attention maps. This subtraction effectively cancels out noise and encourages sparse, more focused attention patterns, improving the model's performance on tasks requiring precise context understanding.|


## News
|Link|description|
|---|---|
|[Brave New World: Leo AI and Ollama Bring RTX-Accelerated Local LLMs to Brave Browser Users.](https://blogs.nvidia.com/blog/rtx-ai-brave-browser/) |Nvidia's RTX-Acceleration combined with Ollama allows for running local models in the browser. |
|[Liquid Foundation Models.](https://www.liquid.ai/liquid-foundation-models) | Liquid AI has introduced its first generation of Liquid Foundation Models (LFMs), offering state-of-the-art performance while minimizing memory consumption. The LFMs, which are optimized for different hardware platforms, include 1B, 3B, and 40B parameter models. These models are already accessible on platforms like LIQUID PLAYGROUND and will soon be available on Cerebras. They are particularly adept at processing sequential data and provide innovations in efficiency and scalability across industries like financial services and biotechnology.|
|[Introducing Copilot Labs and Copilot Vision.](https://www.microsoft.com/en-us/microsoft-copilot/blog/2024/10/01/introducing-copilot-labs-and-copilot-vision/) | Microsoft is launching Copilot Labs to test advanced AI tools, including Think Deeper and Copilot Vision. These tools aim to expand the capabilities of their AI systems, offering enhanced functionality and deeper insights.|
|[OpenAI’s DevDay brings Realtime API and other treats for AI app developers.](https://techcrunch.com/2024/10/01/openais-devday-brings-realtime-api-and-other-treats-for-ai-app-developers/) |It’s been a tumultuous week for OpenAI, full of executive departures and major fundraising developments, but the startup is back at it, trying to convince developers to build tools with its AI models at its 2024 DevDay. The company announced several new tools Tuesday, including a public beta of its “Realtime API”, for building apps with low-latency, AI-generated voice responses. It’s not quite ChatGPT’s Advanced Voice Mode, but it’s close. |
|[Microsoft brings AI-powered overviews to Bing.](https://techcrunch.com/2024/10/01/microsoft-brings-ai-powered-overviews-to-bing/) |Microsoft has introduced Bing generative search, an AI-driven feature that gathers and summarizes information from the web, offering users more concise and aggregated search results. |
|[KoBold Metals, which uses AI to help find critical minerals for the energy transition, raises $491M.](https://techcrunch.com/2024/10/07/ai-powered-critical-mineral-startup-kobold-metals-has-raised-491m-filings-reveal/) |Earlier this year, KoBold Metals found what might be one of the largest high-grade copper deposits of all time, with the potential to produce hundreds of thousands of metric tons per year, the company’s CEO said. |
|[OpenAI gets $4 billion revolving credit line, giving it more than $10 billion in liquidity.](https://www.cnbc.com/2024/10/03/openai-gets-4-billion-revolving-credit-line-on-top-of-latest-funding.html) |OpenAI has secured over $10 billion in liquidity, achieving a valuation of $157 billion following its latest funding round. The company raised $6.6 billion from key investors, including Microsoft and Nvidia, but is contending with substantial operational costs, particularly the need for additional GPUs to support large language model (LLM) training. OpenAI is currently exploring restructuring strategies to enhance financial growth and sustainability within the AI industry. |
|[Black Forest Labs, the startup behind Grok’s image generator, releases an API.](https://techcrunch.com/2024/10/03/black-forest-labs-the-startup-behind-groks-image-generator-releases-an-api/) |Black Forest Labs, the Andreessen Horowitz-backed startup behind the image generation component of xAI’s Grok assistant, has launched an API in beta — and released a new model. |
|[DataPelago raises $47M to optimize hardware for analytical workloads.](https://siliconangle.com/2024/10/01/datapelago-raises-47m-optimize-hardware-analytical-workloads/) | LLMs depend on vast amounts of unstructured data for training, but this data requires extensive cleaning and processing before it becomes useful. Traditional data processing systems, which are based on CPUs and current software architectures, were not designed to handle the scale and complexity of such data, resulting in slow and costly data preparation that hinders AI development. To address these challenges, DataPelago has introduced a Universal Data Processing Engine, designed to overcome performance, cost, and scalability limitations, making AI development faster and more affordable.|
|[Google brings ads to AI Overviews as it expands AI’s role in search.](https://techcrunch.com/2024/10/03/google-brings-ads-to-ai-overviews-and-rolls-out-ai-organized-pages/) | Google will begin to show ads in AI Overviews, the AI-generated summaries it supplies for certain Google Search queries, and will add links to relevant web pages for some of those summaries as well. It’s also rolling out AI-organized search results pages in the U.S. this week.|
|[Nobel Physics Prize Awarded for Pioneering A.I. Research by 2 Scientists.](https://www.nytimes.com/2024/10/08/science/nobel-prize-physics.html) |Two scientists who contributed to the development of neural networks have been awarded the Nobel Prize in Physics, recognizing their groundbreaking work in advancing artificial intelligence and neural network technologies. |
|[Introducing the Message Batches API.](https://www.anthropic.com/news/message-batches-api) |Anthropic has introduced a new batch processing API that allows developers to submit batches of up to 10,000 queries at once. Each batch is processed within 24 hours and is 50% cheaper than standard API calls, making it a more efficient and cost-effective solution for handling non-time-sensitive tasks. |
|[Update on Reflection-70B.](https://glaive.ai/blog/post/reflection-postmortem) |A detailed post-mortem analysis of the highly anticipated Reflection-70B model revealed issues with its benchmark code, which inflated its performance claims. Although the team has since corrected these bugs, and the model's performance remains impressive, it does not quite reach the originally advertised levels. |
|[Four-legged robot learns to climb ladders.](https://techcrunch.com/2024/10/02/four-legged-robot-learns-to-climb-ladders/) | The proliferation of robots like Boston Dynamics’ Spot has showcased the versatility of quadrupeds. These systems have thrived at walking up stairs, traversing small obstacles, and navigating uneven terrain. Ladders, however, still present a big issue — especially given how ever present they are in factories and other industrial environments where the systems are deployed.|
|[Braintrust raises $36M Series A.](https://threadreaderapp.com/thread/1843653246612873701.html) |Braintrust, which helps Airtable, Brex, Notion, and Stripe build AI products, has raised $36M in a Series A led by a16z. |
|[Clout Kitchen raises $4.45M for AI gaming pal that mimics content creators.](https://venturebeat.com/games/clout-kitchen-raises-4-45m-for-ai-gaming-pal-that-mimics-content-creators/) |Clout Kitchen announced today that it has raised $4.45 million in its seed funding round, which it plans to put towards its new creator-powered products and experiences. The first of these is Backseat AI, an AI-powered buddy for League of Legends that the company created with Tyler “Tyler1” Steinkamp — an AI buddy that can take on the aspect of popular gaming content creators. Clout Kitchen plans to use its funding to expand its team and build out its shared internal tech stack. |
|[AlphaFold wins Nobel Prize in Chemistry.](https://www.nobelprize.org/prizes/chemistry/2024/press-release/) |Demis Hassabis, John Jumper, and David Baker were awarded the Nobel Prize in Chemistry for their groundbreaking work in protein folding, particularly through innovations like AlphaFold. Their contributions have significantly advanced the understanding of protein structures and their implications for science and medicine. |
|[OpenAI reducing dependency on Microsoft data centers.](https://www.tipranks.com/news/the-fly/openai-reducing-dependency-on-microsoft-data-centers-the-information-reports?utm_source=tldrai) | OpenAI is decreasing its reliance on Microsoft's data centers by acquiring its own compute infrastructure, allowing greater independence in its operations. Simultaneously, Microsoft is reducing its dependence on OpenAI as it develops and competes with its own AI products, signaling a shift in the dynamics of their partnership.|
|[TikTok parent company ByteDance has a tool that's scraping the web 25 times faster than OpenAI.](https://mashable.com/article/tiktok-parent-company-bytedance-web-crawler-25-times-faster-than-openai) |TikTok parent company ByteDance is amassing huge volumes of web data way faster than the other major web crawlers. ByteDance may be planning to release its own LLM, and is aggressively using its web crawler, "Bytespider," to scrape up data to train its models, Fortune reported. |
|[Sonair takes a cue from dolphins to build autonomous 3D vision without lidar.](https://techcrunch.com/2024/10/07/sonair-takes-a-cue-from-dolphins-to-build-autonomous-3d-vision-sans-lidar/) | Ultrasound is perhaps best known as the technology that enables noninvasive body scans and underwater communication and can help us park our cars. A young startup called Sonair out of Norway wants to employ it for something else: 3D computer vision used in autonomous hardware applications. |
|[Tesla’s head of vehicle programs jumps to Waymo ahead of robotaxi reveal.](https://techcrunch.com/2024/10/07/teslas-head-of-vehicle-programs-jumps-to-waymo-ahead-of-robotaxi-reveal/) | Tesla has lost a top executive to Waymo in the lead-up to the EV maker’s robotaxi unveiling on Thursday.|
|[Autism ABA Therapy with Llama.](https://ai.meta.com/blog/neuromnia-autism-aba-therapy-built-with-llama/) |Meta shares a use case of its Llama model for medical and therapeutic benefit. |
|[Uber’s EV ridehailing business is maturing.](https://www.theverge.com/2024/10/8/24264282/uber-green-ev-driver-mentor-chatgpt) |The company also announced it was adding ChatGPT to its driver app to handle EV questions. |
|[Amazon’s new AI guides can help shoppers find what they need.](https://www.theverge.com/2024/10/9/24266204/amazon-ai-shopping-guides-catalog-feature-availability) |The new AI Shopping Guides feature aims to help users find what they need with more informed product suggestions. |
|[TikTok joins the AI-driven advertising pack to compete with Meta for ad dollars.](https://digiday.com/marketing/tiktok-joins-the-ai-driven-advertising-pack-to-compete-with-meta-for-ad-dollars/) | TikTok's Smart+ is an AI-powered ad-buying tool designed to automate and optimize ad campaigns, giving marketers the option to selectively utilize its features for enhanced performance. The tool seeks to rival Meta's Advantage+ by offering streamlined ad management and improved return on investment (ROI). Early results indicate significant gains in ad spend efficiency and conversion rates, positioning TikTok as a strong contender in the digital advertising market.|
|[OpenAI partners with Cosmopolitan and Elle publisher Hearst.](https://www.engadget.com/ai/openai-partners-with-cosmopolitan-and-elle-publisher-hearst-180517248.html) |ChatGPT will provide citations and direct links to the company's content. |
|[Meta debuts new generative AI tools for creating video-based ads.](https://siliconangle.com/2024/10/08/meta-debuts-new-generative-ai-tools-creating-video-based-ads/) |Meta Platforms Inc. today said it’s rolling out a full-screen video tab on Facebook in recognition of the fact that its users spend more time watching videos than anything else on its platforms. |



## Resources
|Link|description|
|---|---|
|[Introducing the Open FinLLM Leaderboard.](https://huggingface.co/blog/leaderboard-finbench) | The Open FinLLM Leaderboard provides a dedicated evaluation platform designed specifically for financial language models. It emphasizes key financial tasks like predicting stock movements, analyzing sentiment, and extracting information from financial reports.|
|[Infinite-Fractal-Stream: Small Scale Proxy for Scaling-Centric ML.](https://github.com/cloneofsimo/infinite-fractal-stream) |Model testing in the image domain is often constrained by low-quality, small datasets like CIFAR10. This GitHub repository provides a tool that generates infinite, complex fractals in the form of images or videos, offering a new approach for testing models. |
|[Auto Jobs Applier.](https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk) | A highly viral repository leverages language models to automate the job application process, adding an extra layer of personalization to tailor applications for each position.|
|[Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models.](https://arxiv.org/abs/2410.03640v1) |This study uncovers major weaknesses in existing membership inference attacks (MIAs) used to detect unauthorized data usage in diffusion models. It introduces CopyMark, a more realistic benchmark for assessing MIAs on pre-trained models, providing unbiased datasets and fair evaluation techniques to improve the accuracy and reliability of these attacks. |
|[ImageFolder: Autoregressive Image Generation with Folded Tokens.](https://lxa9867.github.io/works/imagefolder/index.html) |ImageFolder is a semantic tokenizer developed to balance the trade-off between image reconstruction accuracy and generation quality in visual generative models, improving the overall performance of these models in both tasks. |
|[Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models.](https://github.com/whb139426/grounded-video-llm) | Grounded-VideoLLM is a novel Video-Large Language Model (Video-LLM) created to enhance the fine-grained understanding of specific moments in videos. By incorporating a temporal stream and discrete temporal tokens, the model more effectively captures the relationships between frames and timestamps, improving its ability to interpret and analyze detailed video content.|
|[Autoregressive Action Sequence Learning for Robotic Manipulation.](https://github.com/mlzxy/arp) |The Chunking Causal Transformer (CCT) is a new autoregressive architecture developed specifically for robotic manipulation tasks. It is designed to improve the model's ability to process sequential data efficiently, optimizing performance in real-time robotic control and manipulation scenarios. |
|[FacePoke.](https://github.com/jbilcke-hf/FacePoke) |FacePoke is a tool designed for rapid editing of faces in both videos and images, allowing users to make quick adjustments and modifications with ease. |
|[pipeline_parallel.py.](https://gist.github.com/3outeille/a3d4d91bb07af64c8f33d5aaee5145fe) |A large model training lead at Hugging Face has shared an excellent 200-line example of parallelism built from scratch, demonstrating efficient techniques for distributing computational tasks, which is particularly useful for large-scale model training. |
|[CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs.](https://arxiv.org/abs/2410.01999) | As language models become increasingly proficient at writing code, many existing benchmarks are approaching saturation. This paper proposes a more challenging benchmark designed to assess how well models perform on reasoning and code generation tasks, pushing beyond basic code-writing capabilities to evaluate deeper problem-solving skills.|
|[Intensify.](https://github.com/swairshah/Intensify) |Intensify is a Python package that allows you to colorize text based on intensity values. It provides an easy-to-use interface for applying color gradients to text or background colors in the terminal. |
|[Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality.](https://oooolga.github.io/JEDi.github.io/) | JEDi is a new metric built on the Joint Embedding Predictive Architecture (JEPA), designed to enhance evaluation accuracy with fewer samples. It better aligns with human assessments, making it a more robust alternative to the FVD (Fréchet Video Distance) metric for evaluating generative models.|
|[PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion.](https://arxiv.org/abs/2410.04939v1) | PRFusion and PRFusion++ are multimodal models developed to enhance place recognition in robotics and computer vision. By combining information from multiple sensory inputs, these models improve the accuracy and robustness of place recognition tasks, making them more effective in real-world applications.|
|[Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia.](https://arxiv.org/abs/2410.05270v1) | This paper presents ProLIP, a novel method for adapting vision-language models such as CLIP without adding additional parameters. ProLIP fine-tunes only the final projection matrix of the vision encoder, enabling it to deliver strong performance in few-shot classification tasks while maintaining the model's efficiency.|
|[ScienceAgentBench.](https://github.com/OSU-NLP-Group/ScienceAgentBench) | The benchmark code for the science agent test is designed to evaluate how effectively models can contribute to novel scientific discoveries. It provides a framework for assessing a model's ability to generate innovative ideas, solve complex scientific problems, and make meaningful advances in various scientific fields.|
|[Controlled Visual Generation.](https://arxiv.org/abs/2410.04671v1) | Controllable AutoRegressive Modeling (CAR) is a novel framework that introduces precise control mechanisms to pre-trained visual autoregressive models. This method enables more refined and targeted image generation by progressively improving control representations, allowing for fine-tuned outputs with reduced computational resources.|
|[PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners.](https://arxiv.org/abs/2410.04733v1) |PredFormer is a newly developed transformer-based method for spatiotemporal predictive learning, offering superior performance in both accuracy and efficiency compared to existing approaches. It excels in tasks that involve predicting changes over time and space, making it a powerful tool for various applications in fields like video analysis, weather forecasting, and robotics. |
|[GenSim2: Scaling Robotic Data Generation with Multi-modal and Reasoning LLMs.](https://gensim2.github.io/) |This paper presents an innovative approach to scaling robotic data collection by utilizing an enhanced, high-quality physics simulation dataset. The improved simulation environment enables more efficient data generation for training robots, offering a scalable and cost-effective method to collect large amounts of accurate and diverse data for robotic learning and development. |
|[Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration.](https://zhu-zhiyu.github.io/FLUX-IR/) |This project introduces a novel differential equation-based approach for image restoration. By leveraging mathematical models grounded in differential equations, the method enhances the ability to recover and restore degraded or noisy images, providing improved accuracy and performance in image restoration tasks. |
|[Pixtral 12B.](https://arxiv.org/abs/2410.07073) |The Mistral team has provided detailed insights into the training process and architecture of their vision-language model, which has demonstrated solid performance. The model incorporates advanced techniques for effectively integrating visual and linguistic data, allowing it to perform well on a variety of tasks that require understanding both images and text. The shared information includes specifics on data preprocessing, model architecture, and the optimization strategies employed during training. |
|[MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering.](https://arxiv.org/abs/2410.07095v1) |MLE-bench is a benchmark created to evaluate AI agents' capabilities in machine learning engineering. It includes a curated selection of 75 Kaggle competitions to test various skills, such as model training, dataset preparation, and optimization. The benchmark aims to assess how well AI agents can handle practical machine learning tasks, providing a comprehensive evaluation of their engineering proficiency. |
|[Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate.](https://arxiv.org/abs/2410.07167v1) |The Modality Integration Rate (MIR) is a new metric designed to evaluate the effectiveness of multi-modal pre-training in Large Vision Language Models. It measures how well different modalities, such as visual and textual data, are integrated during the pre-training process, offering insights into the model's ability to leverage information from both sources to improve performance on multi-modal tasks.|
|[Aria: First Open Multimodal Native MoE Model.](https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model) |A highly impressive new vision-language model has been released with open weights, code, and a comprehensive research report. It achieves performance on par with closed models for long video understanding, a challenge that has proven difficult for other open models like Pixtral and Molmo. This advancement represents a significant breakthrough in the field of open-source vision-language models. |
|[IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation.](https://github.com/yangling0818/itercomp) |IterComp is a new framework developed to enhance compositional text-to-image generation by integrating the strengths of multiple advanced diffusion models, including RPG, Stable Diffusion 3, and FLUX. By leveraging these models, IterComp improves the quality and coherence of generated images, especially when handling complex textual prompts that require multiple elements to be composed accurately. |
|[MatMamba.](https://github.com/scaledfoundations/matmamba) |MatMamba is a novel architecture for sequence processing, building upon the Mamba2 framework by incorporating a Matryoshka-like design. This approach allows a single model to be trained at multiple granularities, enabling the extraction of various smaller, nested submodels. This hierarchical structure enhances flexibility and efficiency, allowing the model to adapt to different levels of complexity and resource constraints. |
|[O1 replication progress report.](https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report.pdf) |Researchers from GAIR and NYU have been investigating the critical algorithmic advancements behind OpenAI's o1 model's exceptional performance. In their report, they introduce the concept of "Journey Learning" data, a novel approach that, when used in training, boosts math performance by 8% in absolute terms. This innovation highlights how specific data types can significantly enhance a model's reasoning and problem-solving abilities. |


## Perspectives
|Link|description|
|---|---|
|[Nuclear power for AI: what it will take to reopen Three Mile Island safely.](https://www.nature.com/articles/d41586-024-03162-2) |As Microsoft strikes a deal to restart a reactor at the notorious power station, Nature talks to nuclear specialists about the unprecedented process. |
|[‘In awe’: scientists impressed by latest ChatGPT model o1.](https://www.nature.com/articles/d41586-024-03169-9) |The chatbot excels at science, beating PhD scholars on a hard science test. But it might ‘hallucinate’ more than its predecessors. |
|[Can AI have common sense? Finding out will be key to achieving machine intelligence.](https://www.nature.com/articles/d41586-024-03262-z) | The advent of LLMs has reopened a debate about the limits of machine intelligence — and requires new benchmarks of what reasoning consists of.|
|[How your brain detects patterns in the everyday: without conscious thought.](https://www.nature.com/articles/d41586-024-03116-8) | Neurons in certain brain areas integrate ‘what’ and ‘when’ information to discern hidden order in events in real time.|
|[AI to the rescue: how to enhance disaster early warnings with tech tools.](https://www.nature.com/articles/d41586-024-03149-z) | Artificial intelligence can help to reduce the impacts of natural hazards, but robust international standards are needed to ensure best practice.|
|[Before Mira Murati’s surprise exit from OpenAI, staff grumbled its o1 model had been released prematurely.](https://fortune.com/2024/10/01/openai-sam-altman-mira-murati-gpt-4o-o1-chatgpt-turbulent-year/) | OpenAI's accelerated development and safety testing of its latest models, such as GPT-4o and o1, have led to internal friction, resulting in the departure of several senior staff members. The rapid pace of development has raised concerns about the thoroughness of the safety protocols, contributing to tensions within the organization.|
|[I Quit Teaching Because of ChatGPT.](https://time.com/7026050/chatgpt-quit-teaching-ai-essay/) |This professor resigned from teaching due to the widespread use of large language models (LLMs) like ChatGPT among students, which they felt undermined academic integrity and the traditional learning process. |
|[Three Subtle Examples of Data Leakage.](https://www.lesswrong.com/posts/rzyHbLZHuqHq6KM65/three-subtle-examples-of-data-leakage) |This article examines the risks of data leakage in machine learning, showcasing two real-world cases where improper data handling resulted in misleading model performance. In one instance, a company incorrectly filtered data by an upper price limit before modeling, while another organization encountered problems by not following a strict chronological split. The key lessons emphasize the critical need for detecting data leakage and understanding its detrimental effects on model accuracy and reliability. |
|[The real data wall is billions of years of evolution.](https://dynomight.substack.com/p/data-wall) | AI development is encountering a potential obstacle known as the "data wall," as language models near the limit of available textual data for training. This article challenges the idea of using human analogies to overcome these data constraints, pointing out that human intelligence results from vast amounts of data and long evolutionary processes, which differ fundamentally from AI. While human learning strategies may not directly translate to AI, this doesn't preclude progress through other modalities, such as multimodal data, or advancements in algorithms that could push AI capabilities further.|
|[AI will use a lot of energy. That's good for the climate.](https://climate.benjames.io/ai-go-brrr/) |AI data centers are significantly increasing the demand for clean, 24/7 energy, prompting tech giants to invest heavily in renewable and nuclear power solutions. This growing demand is expected to accelerate the cost reduction of clean energy technologies, driven by their learning rates. Over time, the energy needs of AI could lead to policy shifts and advancements in clean energy infrastructure, fostering faster adoption and development of sustainable energy sources. |
|[I want to break some laws too.](https://snats.xyz/pages/articles/breaking_some_laws.html) | This article explores the use of an automated data cleaning pipeline inspired by the Minipile method, which prunes datasets to deliver significant performance gains with only a fraction of the original data size. By leveraging techniques such as few-shot prompting and clustering, the approach streamlines dataset refinement for AI training, challenging traditional scaling laws by prioritizing data quality over quantity. The results indicate that using foundational datasets with more refined data can optimize AI model training, reducing resource consumption while boosting performance.|


# ML news: 

## Research
|Link|description|
|---|---|
|[PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting.](https://arxiv.org/abs/2409.17703v1) | The Parallel Gated Network (PGN) is an innovative architecture developed to address the challenges that traditional RNNs face in managing long-term dependencies. By shortening the information propagation path and incorporating gated mechanisms, it efficiently captures both past and present time step data.|
|[Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs.](https://arxiv.org/abs/2409.17778v1) |DoSSR is a diffusion-based super-resolution model that improves both performance and efficiency by utilizing pretrained diffusion models and initiating the process with low-resolution images. This approach accelerates the super-resolution process while maintaining high-quality results. |
|[MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models.](https://vainf.github.io/maskllm-project-page/) | MaskLLM is a pruning technique designed to decrease the computational load of large language models by introducing learnable sparsity. This method optimizes performance while maintaining model efficiency by selectively reducing the number of active parameters.|
|[Law of the Weakest Link: Cross Capabilities of Large Language Models.](https://www.llm-cross-capabilities.org/) | This project emphasizes the importance of evaluating large language models (LLMs) based on their combined abilities rather than focusing solely on individual skills. While most models are trained on specialized datasets that target specific capabilities, real-world tasks frequently demand a blend of expertise across different areas, known as cross-capabilities. This approach ensures that models are better suited to handle complex, multifaceted challenges.|
|[Scaling Optimal LR Across Token Horizon.](https://arxiv.org/abs/2409.19913) |This paper investigates how to adjust the learning rate as the amount of training data increases for a model. While LLaMA applied an exponential scaling factor of -0.28, the paper proposes using an exponential scaling factor of -0.33 for improved performance during training with larger datasets. |
|[Knowledge Graph Embedding by Normalizing Flows.](https://arxiv.org/abs/2409.19977v1) | This paper presents a novel approach to knowledge graph embedding by leveraging group theory to incorporate uncertainty into the process. This method allows for more nuanced and flexible representations of relationships within knowledge graphs, enhancing the model's ability to handle uncertain or ambiguous information.|
|[How AI is improving simulations with smarter sampling techniques.](https://news.mit.edu/2024/how-ai-improving-simulations-smarter-sampling-techniques-1002) | MIT CSAIL researchers created an AI-powered method for low-discrepancy sampling, which uniformly distributes data points to boost simulation accuracy.|

## News
|Link|description|
|---|---|
|[Apple not investing in OpenAI after all, new report says.](https://9to5mac.com/2024/09/27/apple-not-investing-in-openai-after-all-new-report-says/) |Apple is no longer planning to invest in OpenAI, according to a new report from The Wall Street Journal. This comes as OpenAI plans to close a $6.5 billion funding round next week, with investments possible from both Microsoft and Nvidia. |
|[Arcade AI raises 17M to transform commerce.](https://x.com/mnaficy/status/1839342011788439580) |Arcade AI, a generative product company that launched this week, has announced securing funding from prominent investors as it aims to develop its "prompt to product" system. This system enables the immediate creation of products that are ready for purchase, streamlining the process from concept to consumer. |
|[They stole my voice with AI.](https://www.jeffgeerling.com/blog/2024/they-stole-my-voice-ai) |Elecrow is suspected of using AI to clone a voice for promotional videos without consent. |
|[Amazon-backed Anthropic in talks to raise money at $40B valuation: report.](https://seekingalpha.com/news/4152344-amazon-backed-anthropic-in-talks-to-raise-money-at-40b-valuation-report) | Anthropic, a generative AI startup backed by Amazon and other major tech companies, is in discussions to raise additional funding that could potentially value the company at $40 billion.|
|[OpenAI Reportedly Slated for $500 Million SoftBank Investment.](https://www.pymnts.com/news/investment-tracker/2024/openai-reportedly-slated-for-500-million-softbank-investment/) | SoftBank is planning to invest $500 million in OpenAI's latest funding round, which could raise OpenAI's valuation to as high as $150 billion. Microsoft is also participating in this round, highlighting OpenAI's rapid 1,700% revenue growth, despite the company anticipating losses of around $5 billion.|
|[OpenAI Is Growing Fast and Burning Through Piles of Money.](https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html?unlocked_article_code=1.OU4.99io.cnWQdfPhAjl2&smid=url-share) |As the company looks for more outside investors, documents reviewed by The New York Times show consumer fascination with ChatGPT and a serious need for more cash. |
|[Altman reportedly asks Biden to back a slew of multi-gigawatt-scale AI datacenters.](https://www.theregister.com/2024/09/25/altman_5gw_dc/) | OpenAI CEO Sam Altman is calling on the Biden administration to establish AI data centers in the US that could consume up to five gigawatts of power, aiming to maintain US technological leadership over China. The proposal includes building several large-scale data centers across the country. Meanwhile, other tech giants, such as Microsoft and Amazon, are securing nuclear power deals to support their growing AI operations.|
|[Samsung's Galaxy Tab S10 Ultra and Galaxy Tab S10+ are tablets built for AI.](https://www.engadget.com/mobile/tablets/samsungs-galaxy-tab-s10-ultra-and-galaxy-tab-s10-are-tablets-built-for-ai-162633747.html) |Samsung is once again expanding its tablet lineup, and this time, the company is doing so with AI at the forefront. Today, Samsung revealed the Galaxy Tab S10 series, two models that it says are "built with AI enhancements available right out of the box."  |
|[Tesla Full Self Driving requires human intervention every 13 miles.](https://arstechnica.com/cars/2024/09/tesla-full-self-driving-requires-human-intervention-every-13-miles/) |It gave pedestrians room but ran red lights and crossed into oncoming traffic. |
|[OpenAI Dev Day 2024.](https://openai.com/devday/) |OpenAI's Dev Day 2024 featured several exciting announcements, including the introduction of vision model fine-tuning, a real-time API, prompt caching for faster responses, and model distillation for more efficient deployment of large models. These advancements aim to enhance the capabilities and performance of AI applications across various domains. |
|[Pika 1.5.](https://threadreaderapp.com/thread/1841143349576941863.html) |Pika has released version 1.5 with more realistic movement, big screen shots, and Pikaffects. |
|[Gov. Newsom vetoes California’s controversial AI bill, SB 1047.](https://techcrunch.com/2024/09/29/gov-newsom-vetoes-californias-controversial-ai-bill-sb-1047/) | Governor Gavin Newsom has vetoed SB 1047, a proposed bill intended to regulate AI development and enforce safety protocols for high-cost models. Newsom expressed concerns that the bill's broad application to all large, computation-heavy models was not the most effective method for regulating AI. However, he reaffirmed his commitment to AI safety by signing several other AI-related bills and consulting with experts to ensure thoughtful regulation in the future.|
|[OpenAI to remove non-profit control and give Sam Altman equity, sources say.](https://finance.yahoo.com/news/exclusive-openai-remove-non-profit-201413475.html) |hatGPT-maker OpenAI is working on a plan to restructure its core business into a for-profit benefit corporation that will no longer be controlled by its non-profit board, people familiar with the matter told Reuters, in a move that will make the company more attractive to investors. |
|[OpenAI's latest funding .](https://openai.com/index/scale-the-benefits-of-ai/) | OpenAI has secured $6.6 billion in new funding, bringing its post-money valuation to $157 billion. Notable investors in this round include Microsoft and Nvidia, with the funds aimed at further scaling AI development and innovation.|
|[Google adds a multi-functional quick insert key and new AI features to Chromebook Plus.](https://techcrunch.com/2024/10/01/google-adds-a-multi-functional-quick-insert-key-and-new-ai-features-to-chromebook-plus/) |Google is announcing new Chromebook models today with Samsung and Lenovo. With Samsung’s Galaxy Chromebook Plus model in particular, the company is also introducing a new multifunctional quick insert key. But Google doesn’t want to leave existing Chromebook users behind as it added new AI-powered features for existing devices. |
|[Brain-like Computers Tackle the Extreme Edge.](https://spectrum.ieee.org/neuromorphic-computing) |Start-up BrainChip announces a new chip design for a milliwatt-level AI inference |
|[AI Can Best Google’s Bot Detection System, Swiss Researchers Find.](https://decrypt.co/251107/ai-can-best-googles-bot-detection-system-swiss-researchers-find) |Researchers from ETH Zurich used advanced machine learning to solve 100% of Google's reCAPTCHAv2, designed to distinguish humans from bots. |
|[OpenAI Training Data to Be Inspected in Authors’ Copyright Cases.](https://www.hollywoodreporter.com/business/business-news/openai-training-data-inspected-authors-copyright-case-1236011291/) |At a secure room in its San Francisco office, representatives for authors suing OpenAI will examine materials that were used to train its AI system. They allege copyrighted works were utilized without their consent or compensation. |
|[ByteDance will reportedly use Huawei chips to train a new AI model.](https://www.engadget.com/ai/bytedance-will-reportedly-use-huawei-chips-to-train-a-new-ai-model-154846749.html) |US export restrictions are preventing ByteDance from using NVIDIA chips. |
|[Announcing FLUX1.1 [pro] and the BFL API.](https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/) |FLUX1.1 [pro] has been released, offering six times faster generation speeds compared to its predecessor, alongside enhanced image quality and overall performance. The new beta BFL API introduces advanced customization options and competitive pricing, making it easier for developers to integrate FLUX’s capabilities. FLUX1.1 [pro] will be available across multiple platforms, providing greater scalability and efficiency for users and developers alike. |
|[OpenAI launches new ‘Canvas’ ChatGPT interface tailored to writing and coding projects.](https://techcrunch.com/2024/10/03/openai-launches-new-canvas-chatgpt-interface-tailored-to-writing-and-coding-projects) |OpenAI introduced a new way to interact with ChatGPT on Thursday: an interface it calls “canvas.” The product opens a separate window, beside the normal chat window, with a workspace for writing and coding projects. Users can generate writing or code directly in the canvas, then highlight sections of the work to have the model edit. Canvas is rolling out in beta to ChatGPT Plus and Teams users on Thursday, and Enterprise and Edu users next week. |
|[Anthropic hires OpenAI co-founder Durk Kingma.](https://techcrunch.com/2024/10/01/anthropic-hires-openai-co-founder-durk-kingma/) |Durk Kingma, one of the lesser-known co-founders of OpenAI, today announced that he’ll be joining Anthropic. |
|[OpenAI unveils easy voice assistant creation at 2024 developer event.](https://arstechnica.com/information-technology/2024/10/openai-unveils-easy-voice-assistant-creation-at-2024-developer-event/) | Altman steps back from the keynote limelight and lets four major API additions do the talking.|


## Resources
|Link|description|
|---|---|
|[🚀 FlowTurbo.](https://github.com/shiml20/flowturbo) |FlowTurbo is a method developed to accelerate the sampling process in flow-based models while maintaining high-quality outputs. It achieves faster results without compromising the precision or performance of the model. |
|[Transformer4SED.](https://github.com/cai525/transformer4sed) |This repository presents the Prototype-based Masked Audio Model, which enhances sound event detection by leveraging unlabeled data more effectively. The method generates pseudo labels through a Gaussian mixture model, which directs the training of a Transformer-based audio model for improved performance. |
|[VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models.](https://github.com/microsoft/vptq) |Vector Post-Training Quantization is a technique aimed at enabling ultra-low-bit quantization for large language models, optimizing memory and storage efficiency during deployment without significantly compromising performance. |
|[LightAvatar: Efficient Head Avatar as Dynamic NeLF.](https://github.com/mingsun-tse/lightavatar-tensorflow) |LightAvatar is a head avatar model that improves rendering speed and efficiency using neural light fields (NeLFs). |
|[Separating code reasoning and editing.](https://aider.chat/2024/09/26/architect.html) |Aider has significantly enhanced the performance of general-purpose code editing by employing o1 as the architect and DeepSeek as the writer. This collaboration streamlines the process, leading to more efficient and accurate code generation. |
|[Heralax/Mistrilitary-7b.](https://huggingface.co/Heralax/Mistrilitary-7b) |This model was trained using army handbooks and incorporates deep, specialized knowledge that is uncommon in fine-tuned models. This unique training approach allows it to possess a rare level of expertise in military-related tasks and information. |
|[Developing a go bot embedding ichiban Prolog.](https://rogersm.net/posts/developing-a-go-bot-embedding-ichiban-prolog/) |Ichiban Prolog was integrated into Hellabot, a Go-based IRC bot, to eliminate the need for recompiling when adding new triggers. This integration enables dynamic Prolog code execution, allowing users to adjust the bot's logic in real time. Future enhancements could focus on minimizing interpreter setup overhead and shifting more of the bot's logic into Prolog for greater flexibility and efficiency. |
|[Emu 3 open early fusion multimodal model.](https://emu.baai.ac.cn/about) | Emu 3 is a next-token prediction model that surpasses SDXL in image synthesis, LlaVa-1.6 in image understanding, and OpenSora 2 in video generation. With 9 billion parameters, Emu 3 is trained on these tasks in an interleaved manner, similar to Gemini, making it highly versatile and effective across multiple domains.|
|[LOTUS: Diffusion-based Visual Foundation Model for High-quality Dense Prediction.](https://lotus3d.github.io/) |Using pretrained diffusion models for tasks like depth estimation has become highly popular and effective. This work demonstrates how certain previous methods contained minor inaccuracies and presents improvements that not only boost performance but also significantly simplify the overall modeling process. |
|[Revisit Anything: Visual Place Recognition via Image Segment Retrieval.](https://arxiv.org/abs/2409.18049v1) | SegVLAD is a method for visual place recognition that emphasizes the analysis of image segments instead of relying on entire images. This approach enhances recognition accuracy by focusing on distinctive parts of the scene, making it more robust in various environments.|
|[LeanRL - Turbo-implementations of CleanRL scripts.](https://github.com/pytorch-labs/leanrl) |LeanRL is a lightweight library consisting of single-file, pytorch-based implementations of popular Reinforcement Learning (RL) algorithms. The primary goal of this library is to inform the RL PyTorch user base of optimization tricks to cut training time by half or more. |
|[E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding.](https://polyu-chenlab.github.io/etbench/) | E.T. Bench is a newly developed benchmark created to assess the performance of video language models on fine-grained, event-level tasks. Unlike earlier benchmarks that emphasize video-level questions, E.T. Bench spans a variety of time-sensitive tasks across multiple domains, providing a more detailed evaluation of model capabilities.|
|[MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning.](https://arxiv.org/abs/2409.20566) | Apple is continuing to strengthen its in-house AI capabilities by developing a robust multimodal foundation model. This initiative is part of Apple's broader efforts to integrate advanced AI technologies across its ecosystem, supporting tasks that span text, image, and other data modalities for enhanced user experiences.|
|[The Perfect Blend: Redefining RLHF with Mixture of Judges.](https://arxiv.org/abs/2409.20370) | Meta has introduced an impressive new paper detailing the use of a mixture of judges models to effectively conduct multi-task reinforcement learning with human feedback (RLHF) during post-training. This approach significantly enhances the final performance of models across various benchmarks, demonstrating superior results compared to previous methods.|
|[A Survey on the Honesty of Large Language Models.](https://arxiv.org/abs/2409.18786v1) |This survey explores the honesty of large language models (LLMs), a crucial aspect in aligning AI with human values. It addresses challenges such as models confidently providing incorrect answers and the difficulty in distinguishing between what the model knows and what it doesn't. The review highlights these obstacles as key areas for improving the reliability and trustworthiness of LLMs. |
|[LexEval: A Comprehensive Benchmark for Evaluating Large Language Models in Legal Domain.](https://github.com/cshaitao/lexeval) |LexEval is a benchmark created to evaluate large language models (LLMs) specifically in the legal domain. Recognizing the critical need for accuracy, reliability, and fairness in legal applications, LexEval provides a framework for assessing the strengths and limitations of LLMs when applied to legal tasks, ensuring they meet the rigorous demands of the field. |
|[Perceptual Compression (PerCo).](https://github.com/Nikolai10/PerCo) | PerCo (SD) is a novel perceptual image compression technique built on Stable Diffusion v2.1, specifically designed for ultra-low bit ranges. This method leverages the power of diffusion models to achieve high-quality image compression at significantly reduced bitrates, optimizing storage and transmission without sacrificing visual fidelity.|
|[nvidia/NVLM-D-72B.](https://huggingface.co/nvidia/NVLM-D-72B) |Nvidia conducted a thorough ablation study on various methods of incorporating images into a language model. The results showed that the LlaVa concatenation approach outperformed the other methods, proving to be the most effective for integrating visual information into language models. |
|[ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification.](https://arxiv.org/abs/2409.20081v1) |This paper introduces a new method called Prompt-guided Feature Disentangling (ProFD) to tackle occlusion challenges in person Re-Identification (ReID) tasks. ProFD helps separate relevant features from occluded or irrelevant ones, improving the accuracy and robustness of ReID models when identifying individuals in complex or obstructed environments. |
|[Local File Organizer: AI File Management Run Entirely on Your Device, Privacy Assured.](https://github.com/NexaAI/nexa-sdk/tree/main/examples/local_file_organization) | This tool utilizes Llama 3.2 3B and Llava-1.6 to intelligently organize files on your computer into logical sections based on their content. By analyzing the data within the files, it categorizes and arranges them for easier navigation and more efficient file management.|
|[Posterior-Mean Rectified Flow:
Towards Minimum MSE Photo-Realistic Image Restoration.](https://pmrf-ml.github.io/) |Posterior-Mean Rectified Flow (PMRF) is a cutting-edge algorithm designed for photo-realistic image restoration. It improves the quality of restored images by refining the flow of information, resulting in highly accurate and visually appealing reconstructions. |
|[RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models.](https://github.com/shuhao02/routerdc) | RouterDC is an innovative method designed to enhance collaboration between multiple large language models (LLMs) through query-based routing. It utilizes contrastive learning to determine the most suitable model for each query, leading to improved performance compared to existing routing techniques. This approach optimizes model selection, ensuring more accurate and efficient responses.|
|[Distributed Training of Deep Learning models .](https://vaibhawvipul.github.io/2024/09/29/Distributed-Training-of-Deep-Learning-models-Part-~-1.html) |This post provides an excellent introduction to the challenges and algorithms involved in distributed training for modern deep learning models. It explores the difficulties and bottlenecks of training models that are too large for a single GPU, including issues like communication overhead, synchronization, and memory limitations, while also discussing key techniques to overcome these obstacles. |
|[ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation.](https://comfygen-paper.github.io/) | Instead of directly generating an image from a prompt, the authors created a workflow using a comfy UI node-based system to guide the image generation process. This approach significantly enhanced the final output quality, allowing for greater control and precision in the generation pipeline.|
|[KnobGen.](https://github.com/aminK8/KnobGen) |KnobGen is a new framework developed to make sketch-based image generation more accessible to users of varying skill levels. By offering intuitive controls and simplified tools, KnobGen allows users to generate high-quality images from sketches, regardless of their artistic expertise. |
|[Tiny Test Models.](https://huggingface.co/blog/rwightman/timm-tiny-test) |AI researcher Ross Wightman has released a collection of models trained on ImageNet-1k that are remarkably small, with fewer than 1 million parameters. Despite their compact size, these models perform reasonably well and are designed to be easy to fine-tune, making them highly accessible for various applications where model efficiency is critical. |
|[entropix.](https://github.com/xjdr-alt/entropix) |Entropy-based sampling and parallel Chain of Thought (CoT) decoding are promising strategies for advancing reasoning models to match |
|[Concordia.](https://github.com/google-deepmind/concordia) | DeepMind's Concordia repository enables the simulation of social interactions between individuals and groups at a reasonable scale. This platform allows researchers to model complex social behaviors, study group dynamics, and explore various interaction scenarios in a controlled, scalable environment.|

## Perspectives
|Link|description|
|---|---|
|[The Intelligence Age.](https://ia.samaltman.com/) |AI is set to enhance human abilities, empowering us to accomplish tasks that are currently beyond imagination. With the help of deep learning and more powerful computational tools, AI will drive innovations such as personalized assistants, learning tutors, and healthcare advisors. The emphasis should be on ensuring AI is widely accessible while addressing its potential risks, creating a path toward shared prosperity in the era of intelligent systems. |
|[How AlphaChip transformed computer chip design.](https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design) |AlphaChip is a reinforcement learning model that dramatically speeds up and improves chip design, creating layouts that surpass human capabilities. It produces optimized chip designs, such as those used in Google's TPUs, in just hours instead of weeks. This AI-powered approach has wide-ranging applications, benefiting not only Google's hardware but also external companies like MediaTek. |
|[AI pareidolia: Can machines spot faces in inanimate objects?](https://news.mit.edu/2024/ai-pareidolia-can-machines-spot-faces-in-inanimate-objects-0930) | New dataset of “illusory” faces reveals differences between human and algorithmic face detection, links to animal face recognition, and a formula predicting where people most often perceive faces.|
|[Table Extraction using LLMs: Unlocking Structured Data from Documents.](https://nanonets.com/blog/table-extraction-using-llms-unlocking-structured-data-from-documents/) | This article discusses how large language models (LLMs) are transforming table extraction from complex documents, surpassing the limitations of traditional methods such as OCR, rule-based systems, and machine learning. LLMs offer greater flexibility and contextual comprehension, significantly improving accuracy in handling varied and intricate table structures. While challenges like hallucination and high computational demands remain, the integration of traditional techniques with LLMs currently provides the most effective solution for automated table extraction.|
|[The Other Bubble.](https://www.wheresyoured.at/saaspocalypse-now/) | Microsoft considered diverting its US-based server power to GPUs for AI purposes but ultimately abandoned the idea. Major tech companies like Microsoft, Google, and Amazon are making significant investments in AI, yet they continue to see underwhelming returns from generative AI applications. The industry's reliance on SaaS and the integration of AI tools, which frequently offer limited practical value while incurring substantial costs, underscores an increasing urgency to sustain growth in a slowing market.|
|[AI's Privilege Expansion.](https://www.digitalnative.tech/p/ais-privilege-expansion) |AI is quickly broadening access to services that were once expensive and difficult to obtain, such as education, healthcare, and personal styling. Generative AI models like ChatGPT offer affordable, personalized support by acting as tutors, healthcare advisors, and stylists, reducing the need for costly human professionals. This transformation democratizes access to high-end services, making them more widely available to the general public at a significantly lower cost. |
|[Behind OpenAI’s Audacious Plan to Make A.I. Flow Like Electricity.](https://www.nytimes.com/2024/09/25/business/openai-plan-electricity.html) | OpenAI CEO Sam Altman has proposed a global initiative to construct data centers and chip factories to drive advanced AI development. While Altman initially aimed for trillions in funding, he has now scaled back to targeting hundreds of billions. The plan envisions partnerships with global tech giants and governments, though it faces significant regulatory and logistical hurdles. Despite early skepticism, ongoing discussions suggest potential expansions across the US, Europe, and Asia to significantly increase computing power for AI advancements.|
|[Devs gaining little (if anything) from AI coding assistants.](https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html) |Code analysis firm sees no major benefits from AI dev tool when measuring key programming metrics, though others report incremental gains from coding copilots with emphasis on code review. |
|[Negligence Liability for AI Developers.](https://www.lawfaremedia.org/article/negligence-liability-for-ai-developers) |This article advocates for a negligence-based approach to AI accountability, emphasizing the human factors and responsibilities behind AI systems. It critiques existing regulatory frameworks for neglecting the role of AI developers and highlights California's AI safety bill as a promising example. The article also delves into the complexities of defining "reasonable care" in AI development and the potential consequences of classifying AI developers as professionals, raising important questions about the standards and obligations they should meet. |
|[I am tired of AI.](https://www.ontestautomation.com/i-am-tired-of-ai/) | The author expresses frustration with the widespread marketing and overuse of AI, especially in fields like software testing and conference proposals. They argue that AI tools often prioritize speed at the expense of quality and fail to offer the unique insights that come from human-generated work. While acknowledging some useful applications of AI, the author criticizes the increasing amount of mediocre AI-produced content, seeing it as a detriment to innovation and depth in these areas.|
|[The Four Short Term Winners of AI.](https://www.whitenoise.email/p/the-four-short-term-winners-of-ai) |The global AI arms race is primarily driven by Big Tech companies, chipmakers such as NVIDIA, intellectual property lawyers, and the Big 4 consulting firms. These key players are competing to secure technological dominance, resources, and expertise in AI development, shaping the future of the industry through their influence and innovations. |
|[The Art of the OpenAI Deal.](https://spyglass.org/the-art-of-the-openai-deal/) | OpenAI's revenue soared to $300 million in August, with the company forecasting $3.7 billion in annual sales for this year and $11.6 billion for next year. However, it is facing a $5 billion annual loss. This rapid growth has been driven primarily by the widespread success of ChatGPT, which generates the majority of its revenue. Despite this momentum, OpenAI is actively seeking additional investors to cover its high operational costs and work towards becoming a profitable enterprise.|
|[What comes after?](https://www.strangeloopcanon.com/p/ai-bill-vetoed-whats-next) |California Governor Gavin Newsom has vetoed SB 1047, a bill aimed at regulating large AI models. He stressed the importance of creating evidence-based regulations and cautioned that overly restrictive rules could hinder innovation. Instead, Newsom plans to collaborate with experts, including Dr. Fei-Fei Li, to develop empirical, science-driven guidelines that balance safety and progress in AI development. |
|[Sorry, GenAI is NOT going to 10x computer programming.](https://garymarcus.substack.com/p/sorry-genai-is-not-going-to-10x-computer) |Recent studies indicate that generative AI has not yet delivered the expected 10x improvement in coding productivity. While AI tools can assist with code generation and streamline certain tasks, the overall productivity gains have been more modest than initially projected, with challenges such as integration, context understanding, and debugging limiting the full potential of these technologies in real-world coding environments. |


# ML news: Week 23 - 29 September

## Research
|Link|description|
|---|---|
|[Moshi: a speech-text foundation model for real-time dialogue.](https://kyutai.org/Moshi.pdf) |presents a full-duplex spoken dialogue framework and a speech-text basis paradigm; they also present several system components; Helium is a 7B parameter text LLM; Mimi is a semantic-acoustic neural audio code that achieves cutting-edge audio quality performance; and a hierarchical multi-stream architecture that can produce speech-to-speech from any given dialog.|
|[Training Language Models to Self-Correct via Reinforcement Learning.](https://arxiv.org/abs/2409.12917) |creates a multi-turn online reinforcement learning system that is fully based on self-generated data in order to enhance an LLM's ability to self-correct; It is demonstrated that SFT has a distribution mismatch between training data and model responses and is inefficient at learning self-correction; suggests a two-stage method that, when applied to the Gemini 1.0 Pro and 1.5 Flash models, achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1%, respectively, on the MATH and HumanEval benchmarks. The first stage of the method optimizes correction behavior, and the second uses a reward bonus to amplify self-correction during training. |
|[On the Diagram of Thought.](https://arxiv.org/abs/2409.10038) |strengthens LLMs' capacity for reasoning through rigorous mathematics; DAT represents iterative reasoning in LLM as the building of a directed acyclic graph; it combines propositions, criticisms, refinement, and verification into a single DAG structure; this enables DoT to capture sophisticated logical deduction that is beyond the scope of linear or tree-based methods |
|[To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning.](https://arxiv.org/abs/2409.12183) | examines which tasks benefit most from chain-of-thought (CoT) prompting; following a meta-analysis of over 100 papers and multiple evaluations, it concludes that CoT leads to significant performance gains, mostly on math and logic tasks; the majority of the CoT gain is derived from improving symbolic execution, although a symbolic solver performs better than it.|
|[A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B.](https://arxiv.org/abs/2409.11055) | examines how instruction-tuned LLMs perform on models ranging from 7B to 405B using different quantization techniques. The main conclusions are that: 1) one should quantize a larger LLM to a similar size because a smaller FP16 LLM typically performs better across most benchmarks; 2) performance varies significantly with different quantization techniques, model size, and bit-width, with weight-only methods frequently producing better results in larger models; and 3) task difficulty does not significantly impact accuracy degradation due to quantization.|
|[Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning.](https://arxiv.org/abs/2409.12618) | uses an inner dialogue agent to act as a guide to dynamically adjust reasoning paths, allowing adaptive cross-path exploration and improving response accuracy. This makes it different from CoT and ToT, which are both rigid processes, in that its prompt generation is a dynamic process that allows it to adapt. suggests the Iteration of Thought (IoT) framework to improve the LLM responses and reasoning capabilities with adaptive reasoning paths.|
|[Schrodinger's Memory: Large Language Models.](https://arxiv.org/abs/2409.10482) | utilizes the Universal Approximation Theorem to describe how LLMs store memory. Additionally, it suggests a novel method for assessing LLM performance by contrasting the memory capacities of various models; the Transformer architecture serves as a dynamic fitting UAT model with a high degree of adaptability in fitting inputs, allowing LLMs to recall the entirety of the content with the least amount of input data.|
|[Jailbreaking Large Language Models with Symbolic Mathematics.](https://arxiv.org/abs/2409.11445) |generates mathematically encoded prompts using GPT-4o, which is a useful jailbreaking strategy; the average attack success rate over 13 state-of-the-art is 73.6%. This indicates that current safety training systems are not able to generalize to mathematically encoded inputs. |
|[Iterative Object Count Optimization for Text-to-image Diffusion Models.](https://ozzafar.github.io/count_token/) |Generating a specific number of objects with a diffusion model is often a difficult task. This work introduces a counting token that enables the model to more accurately produce either a few or many instances of a given object. While it's not flawless and is based on the original stable diffusion model, it significantly outperforms existing methods. |
|[A Controlled Study on Long Context Extension and Generalization in LLMs.](https://arxiv.org/abs/2409.12181v2) |Researchers have created a standardized evaluation protocol designed to compare different methods for extending language models to effectively handle long document contexts. |
|[MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning.](https://arxiv.org/abs/2409.12147v1) |MAgICoRe is a novel strategy designed to enhance reasoning in large language models by tackling challenges in refinement processes. It classifies problems based on difficulty, applying straightforward strategies to simpler tasks and employing multi-agent iterative refinement for more complex ones. |
|[The Impact of Element Ordering on LM Agent Performance.](https://arxiv.org/abs/2409.12089v2) |The sequence in which UI elements are displayed greatly affects agent performance in virtual environments. Randomizing the order of elements can decrease performance as much as completely removing all visible text.|
|[Larger and more instructable language models become less reliable.](https://www.nature.com/articles/s41586-024-07930-y) | Scaling up and shaping up large language models increased their tendency to provide sensible yet incorrect answers at difficulty levels humans cannot supervise, highlighting the need for a fundamental shift in artificial intelligence design towards reliability.|
|[SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs and Agents.](https://arxiv.org/abs/2409.15817) |This work addresses the limitations of LLMs in drug discovery by integrating an advanced Retrieval-Augmented Generation (RAG) system for more accurate answers and combining LLMs with external tools to create an automatic target dossier. The result is a production-ready dossier with comprehensive data, summarized into a PDF and PowerPoint presentation. |
|[Self-Explainable AI.](https://arxiv.org/abs/2409.16693v1) |In the field of explainable AI, there is a strong focus on developing self-explainable models, which offer a more principled approach compared to post-hoc methods that attempt to interpret decisions after they have been made by opaque models. Despite its potential, this line of research often faces challenges such as lack of reproducibility, difficulties in comparison, and inconsistent standards. To address these issues, we introduce CaBRNet, an open-source, modular, and backward-compatible framework for Case-Based Reasoning Networks |

## News
|Link|description|
|---|---|
|[Google CEO Sundar Pichai announces $120M fund for global AI education.](https://techcrunch.com/2024/09/21/google-ceo-sundar-pichai-announces-120m-fund-for-global-ai-education/) | Speaking Saturday at the UN Summit of the Future, Google CEO Sundar Pichai described AI as “the most transformative technology yet” and announced a new fund for AI education and training around the world.|
|[Driver Distractions ‘Exceedingly High’ When Using Partial Automation Systems: IIHS.](https://www.thedrive.com/news/driver-distractions-exceedingly-high-when-using-partial-automation-systems-iihs) |According to the IIHS, once advanced driver-assistance systems come into play, drivers become less involved in driving and more distracted. Hands-on or hands-free, the level of automation doesn’t matter. |
|[wordfreq will not be updated.](https://github.com/rspeer/wordfreq/blob/master/SUNSET.md) | The wordfreq data is a snapshot of language that could be found in various online sources up through 2021. Generative AI has polluted the data|
|[Drones carrying fireworks: why the world’s most famous gunpowder artist is collaborating with AI.](https://www.theguardian.com/artanddesign/2024/sep/22/cai-guo-qiang-we-are-explosion-event-artificial-intelligence) | For his explosion event in Los Angeles, Cai Guo-Qiang built his own version of ChatGPT and employed a drone army to answer the question: what is the fate of humanity and AI?|
|[AI could lead to inconsistent outcomes in home surveillance.](https://news.mit.edu/2024/study-ai-inconsistent-outcomes-home-surveillance-0919) |Researchers find large language models make inconsistent decisions about whether to call the police when analyzing surveillance videos. |
|[Arcade Announces First-Ever AI Product Creation Platform.](https://www.prnewswire.com/news-releases/arcade-announces-first-ever-ai-product-creation-platform-302254225.html) |Arcade is a new platform where users can go from prompt to product. |
|[Salesforce Taps Nvidia to Develop AI-Powered Avatars.](https://www.marketwatch.com/story/salesforce-taps-nvidia-to-develop-ai-powered-avatars-40acf63a?mod=technology) | Salesforce and Nvidia are partnering to develop advanced artificial intelligence capabilities aimed at delivering new insights and enhancing productivity for teams utilizing Salesforce's platform.|
|[Introducing the OpenAI Academy.](https://openai.com/global-affairs/openai-academy) |OpenAI is launching a program aimed at expanding AI knowledge access in low and middle-income countries. Additionally, it has professionally translated the MMLU, a standard reasoning benchmark, into 15 different languages. |
|[China’s Alibaba launches over 100 new open-source AI models, releases text-to-video generation tool.](https://www.cnbc.com/2024/09/19/alibaba-launches-over-100-new-ai-models-releases-text-to-video-generation.html) | Alibaba has introduced over 100 open-source AI models, bolstering its technology to stay competitive with its rivals. The latest Qwen 2.5 models, improved in areas like math and coding, cater to various applications, including automobiles and gaming. Additionally, Alibaba has unveiled a new proprietary model, Qwen-Max 2.5, along with a text-to-video tool to enhance its AI and cloud service offerings.|
|[Apple Intelligence Features Expected to Roll Out in This Order Between iOS 18.1 and iOS 18.4.](https://www.macrumors.com/2024/09/22/apple-intelligence-features-timing/) |Apple's iOS 18.1 will debut significant AI features, including an improved Siri, generative AI tools within Photos, and ChatGPT integration. In iOS 18.2, these capabilities will be expanded with localized support across various English-speaking countries, alongside the introduction of Image Playground and Genmoji. Upcoming updates, like iOS 18.4, will further personalize Siri and add support for additional languages. |
|[Microsoft updates its AI suite with more agents and Copilots.](https://www.axios.com/2024/09/16/microsoft-updates-ai-agents-copilots) |Microsoft is enhancing its generative AI suite by introducing automated agents, expanding the capabilities of its Copilot assistants, and launching a new tool that enables multiple workers to collaboratively engage with artificial intelligence. |
|[Sam Altman leaves OpenAI board's safety and security committee.](https://www.axios.com/2024/09/16/openai-sam-altman-safety-committee-members) | OpenAI announced that CEO Sam Altman is stepping down from the board's safety and security committee, which will now consist entirely of independent board members.|
|[Silicon Valley billionaire Vinod Khosla says AI will handle 80% of work in 80% of jobs.](https://fortune.com/2024/09/24/silicon-valley-billionaire-vinod-khosla-universal-basic-income-ai-80-jobs/) |Yet another Silicon Valley billionaire has just predicted that most jobs will be replaced by AI—whether you work on a farm or in sales. |
|[Hollywood is coming out in force for California’s AI safety bill.](https://www.theverge.com/2024/9/24/24253439/california-sb-1047-newsom-hollywood-sag-aftra-ai-safety) |Hollywood is squaring off against Silicon Valley in the battle over SB 1047, California’s first-of-its-kind AI safety bill. Amid doubts about whether Governor Gavin Newsom will sign the legislation, a wave of star-studded endorsements mark the first organized celebrity effort to advance AI regulations beyond the direct interests of the entertainment industry.  |
|[OpenAI rolls out Advanced Voice Mode with more voices and a new look.](https://techcrunch.com/2024/09/24/openai-rolls-out-advanced-voice-mode-with-more-voices-and-a-new-look/) |OpenAI announced it is rolling out Advanced Voice Mode (AVM) to an expanded set of ChatGPT’s paying customers on Tuesday. The audio feature, which makes ChatGPT more natural to speak with, will initially roll out to customers in ChatGPT’s Plus and Teams tiers. Enterprise and Edu customers will start receiving access next week.|
|[OpenAI CEO Sam Altman declares we could have superintelligence 'in a few thousand days'.](https://www.techradar.com/computing/artificial-intelligence/openai-ceo-sam-altman-declares-we-could-have-superintelligence-in-a-few-thousand-days) |OpenAI CEO Sam Altman has declared that humanity is on the brink of a superintelligence revolution, and that "In the next couple of decades, we will be able to do things that would have seemed like magic to our grandparents."  |
|[Google says generative AI is ready to do real work.](https://www.axios.com/2024/09/24/google-says-generative-ai-is-ready-to-do-real-work) |Google is holding a "Gemini at Work" event Tuesday to convince businesses that its generative AI is better than offerings from Microsoft and OpenAI. The largely virtual event comes amid a flurry of claims from tech providers and growing skepticism that genAI is ready for broad use beyond coding and customer support.|
|[Google, Volkswagen partner on smartphone AI assistant.](https://www.reuters.com/technology/artificial-intelligence/google-volkswagen-partner-smartphone-ai-assistant-2024-09-24/) | Google is providing key capabilities for an artificial intelligence assistant for Volkswagen drivers in a smartphone app, part of Google's strategy to win business by offering tools to build enterprise AI applications.|
|[Will AI replace programmers? Don't count on it, says Google's CEO.](https://www.xda-developers.com/ai-replace-programmers-google-ceo/) | the CEO of Google and its owner company, Alphabet, believes that AI won't be replacing programmers - instead, it'll actually help more people become coders than ever before.|
|[Cloudflare's new AI Audit tool aims to give content creators better bot controls.](https://www.zdnet.com/article/cloudflares-new-ai-audit-tool-aims-to-help-content-creators-take-back-control-from-bots/) |Don't want your work ripped off by OpenAI, Meta AI, and Google Gemini? If your work is on a website you control, Cloudflare's AI Audit tool may help. Here's how to try it. |
|[James Cameron, Academy Award-Winning Filmmaker, Joins Stability AI Board of Directors.](https://stability.ai/news/james-cameron-joins-stability-ai-board-of-directors) | Renowned filmmaker James Cameron has joined the board of generative media company Stability AI to help steer its shift toward visual storytelling.|
|[Updated Gemini models, reduced 1.5 Pro pricing, increased rate limits.](https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/) | Google's Gemini models have seen a significant cost reduction, an expanded context length of up to 2 million tokens, and overall performance enhancements. An intriguing detail is the noticeable jump in cost after reaching 128k tokens.|
|[Llama 3.2: multimodal.](https://www.llama.com/) | Meta has introduced a new series of Llama models with vision capabilities, including versions with 1 billion and 3 billion parameters, as well as several additional multimodal models.|
|[OpenAI CTO Mira Murati is leaving.](https://www.theverge.com/2024/9/25/24254431/openai-cto-mira-murati-leaving) |wo other company leaders are also out in what CEO Sam Altman calls an “abrupt” reorganization. |
|[OpenAI staffers reportedly 'taken aback' by 'ominous' logo rebranding.](https://www.engadget.com/ai/openai-staffers-reportedly-taken-aback-by-ominous-logo-rebranding-160017936.html) | OpenAI is set to rebrand in 2024 with a new logo that employees felt lacked creativity. Alongside this change, the company is transitioning from a non-profit to a for-profit model. The rebranding effort is intended to strengthen its identity as OpenAI gains greater recognition.|
|[Accelerating particle size distribution estimation.](https://news.mit.edu/2024/accelerating-particle-size-distribution-estimation-0923) |MIT researchers have accelerated a new AI-based estimator for medication manufacturing, achieving a 60-fold increase in speed. |
|[Apple Intelligence will support German, Italian, Korean, Portuguese, and Vietnamese in 2025.](https://techcrunch.com/2024/09/18/apple-intelligence-will-support-german-italian-korean-portuguese-and-vietnamese-in-2025/) |Apple announced Wednesday that its generative AI offering will be available in even more languages in 2025. Additions to Apple Intelligence include English (India), English (Singapore), German, Italian, Korean, Portuguese, Vietnamese, and “others” yet to be announced. |
|[Salesforce Ventures ups its AI fund to $1B, doubling it again.](https://techcrunch.com/2024/09/16/salesforce-ventures-ups-its-ai-fund-to-1-billion-doubling-it-again/) | Salesforce Ventures just announced a new $500 million fund dedicated to AI companies. This is significant for several reasons. First, in June 2023, Salesforce Ventures doubled its AI fund from $250 to $500, so the additional $500 million brings the AI fund to $1 billion. This compares to $5 billion total deployed in its first 15 years, since its 2009 launch. |
|[LinkedIn scraped user data for training before updating its terms of service.](https://techcrunch.com/2024/09/18/linkedin-scraped-user-data-for-training-before-updating-its-terms-of-service/) | LinkedIn may have trained AI models on user data without updating its terms. LinkedIn users in the U.S. — but not the EU, EEA, or Switzerland, likely due to those regions’ data privacy rules — have an opt-out toggle in their settings screen disclosing that LinkedIn scrapes personal data to train “content creation AI models.” The toggle isn’t new. But, as first reported by 404 Media, LinkedIn initially didn’t refresh its privacy policy to reflect the data use.|
|[Tokyo Game Show showcases latest AI tech in games amid labor shortage.](https://english.kyodonews.net/news/2024/09/955e5279ea19-tokyo-game-show-kicks-off-showcases-latest-ai-tech-in-industry.html) | The Tokyo Game Show kicked off Thursday with a special area showcasing the latest artificial intelligence technology to help develop video games, as the industry grapples with a chronic labor shortage.|
|[OpenAI to remove non-profit control and give Sam Altman equity.](https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/) |OpenAI plots to restructure into for-profit benefit corporation. Non-profit board no longer controls for-profit when done. CEO Sam Altman to receive equity in OpenAI for the first time |
|[Amazon launches Amelia, a generative AI-powered assistant for third-party sellers.](https://siliconangle.com/2024/09/19/amazon-launches-amelia-generative-ai-powered-assistant-third-party-sellers/) | Amazon has introduced Project Amelia, a generative AI assistant designed for independent sellers on its platform. Developed using Amazon's Bedrock, Amelia provides personalized insights, sales data, and operational support to boost seller productivity. Currently in beta for select U.S. sellers, it is set to roll out to more users and countries in the near future.|
|[YouTube Shorts to integrate Veo, Google’s AI video model .](https://techcrunch.com/2024/09/18/youtube-shorts-to-integrate-veo-google-ai-video-model/) | The company announced that it is integrating Google DeepMind’s AI video generation model, Veo, into YouTube Shorts, letting creators generate high-quality backgrounds as well as six-second clips.|
|[AI tool cuts unexpected deaths in hospital by 26%, Canadian study finds.](https://www.cbc.ca/news/health/ai-health-care-1.7322671) |St. Michael's Hospital's AI-driven early warning system, Chartwatch, has been shown to reduce unexpected patient deaths by 26% in a recent study. |
|[Amazon releases a video generator — but only for ads.](https://techcrunch.com/2024/09/19/amazon-releases-a-video-generator-but-only-for-ads/) | Like its rival, Google, Amazon has launched an AI-powered video generator — but it’s only for advertisers at the moment, and somewhat limited in what it can do.|
|[Archaeologists use AI to discover 303 unknown geoglyphs near Nazca Lines.](https://www.theguardian.com/world/2024/sep/26/nazca-lines-peru-new-geoglyphs) | Newly discovered figures dating back to 200BCE nearly double the number of known geoglyphs at enigmatic site|
|[OpenAI’s chief research officer has left following CTO Mira Murati’s exit.](https://techcrunch.com/2024/09/25/openais-chief-research-officer-has-left/) |OpenAI’s chief research officer, Bob McGrew, and a research VP, Barret Zoph, left the company on Wednesday, hours after OpenAI CTO Mira Murati announced she would be departing. |
|[NotebookLM adds audio and YouTube support, plus easier sharing of Audio Overviews.](https://blog.google/technology/ai/notebooklm-audio-video-sources/) |NotebookLM now has the capability to extract information from audio and video sources and offers enhanced sharing options for audio artifacts. |
|[Vultr Cloud Alliance: High-Performance AI and HPC with AMD and Vultr.](https://blogs.vultr.com/Vultr-AMD-Cloud-Alliance) |AMD has partnered with Vultr to integrate AMD Instinct MI300X GPUs into Vultr's cloud infrastructure. |
|[AI is stressing networks out - Nvidia thinks AI can help.](https://www.fierce-network.com/wireless/ai-stressing-networks-out-nvidia-and-t-mobile-are-using-ai-fix-it) |Nvidia and T-Mobile are leveraging AI to manage the growing network traffic driven by increased AI usage in 5G environments. This collaboration aims to optimize network performance and efficiency, ensuring seamless connectivity and handling the surge in data demands associated with AI-driven applications. |
|[Rabbit’s web-based ‘large action model’ agent arrives on r1 on October 1.](https://techcrunch.com/2024/09/23/rabbits-web-based-large-action-model-agent-arrives-on-r1-as-early-as-this-week/) |The Rabbit r1 was the must-have gadget of early 2024, but the blush fell off it pretty quickly when the company’s expansive promises failed to materialize. CEO Jesse Lyu admits that “on day one, we set our expectations too high” but also said that an update coming to devices next week will finally set the vaunted Large Action Model free on the web. |
|[Boston Dynamics’ Spot can now autonomously unlock doors.](https://techcrunch.com/2024/09/23/boston-dynamics-spot-can-now-autonomously-unlock-doors/) | Boston Dynamics’ Spot will be able to autonomously unlock its automated doors.|


## Resources
|Link|description|
|---|---|
|[Qwen2.5-Coder Technical Report.](https://arxiv.org/abs/2409.12186) | based on the Qwen2.5 architecture, which is continuously pretrained on 5.5 trillion tokens and achieves state-of-the-art performance across more than 10 benchmarks. It has strong capabilities in code generation, completion, reasoning, and repairing. a series of models with 1.5B and 7B parameters.|
|[Agents in Software Engineering: Survey, Landscape, and Vision.](https://arxiv.org/abs/2409.09030) | gives a thorough rundown of software engineering frameworks for LLM-based agents.|
|[Prompting ChatGPT o1.](https://platform.openai.com/docs/guides/reasoning/how-reasoning-works) |This guide was overlooked amidst the buzz around OpenAI's new reasoning models. It explains how prompting this new model differs, emphasizing the need for simpler prompts and a more organized input context. |
|[Jony Ive confirms he’s working on a new device with OpenAI.](https://www.theverge.com/2024/9/21/24250867/jony-ive-confirms-collaboration-openai-hardware) | Jony Ive is teaming up with OpenAI CEO Sam Altman on a new AI hardware initiative, which might secure $1 billion in funding by the end of the year and includes involvement from key former Apple designers. Although details about the device are still unclear, the project aims to harness generative AI for enhanced user interactions.|
|[Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries.](https://arxiv.org/abs/2409.12640) | Another impressive paper from Google demonstrates how to evaluate long-context models, following a directionally similar approach to the recent work by Magic.|
|[3DTopia-XL: High-Quality 3D PBR Asset Generation via Primitive Diffusion.](https://github.com/3DTopia/3DTopia-XL) |The process of converting image and text inputs into 3D models involves generating a 3D mesh that is smoothed for high-quality surfaces, and then applying Physically-Based Rendering (PBR) lighting techniques to create realistic lighting and textures. This method ensures the final 3D object has detailed geometry, smooth surfaces, and lifelike lighting effects, making it suitable for use in various 3D applications such as games, VR/AR, and simulations. |
|[aiq.](https://github.com/taylorai/aiq) | A straightforward yet highly effective tool designed for labeling, embedding, and classifying unlabeled text directly from the command line. It supports real-time processing of streams, allowing it to handle piped input from various sources seamlessly.|
|[Most powerful LLM on a single GPU.](https://www.upstage.ai/products/solar-pro-preview) |Solar Pro is a 22B parameter language model optimized to run on a single 80GB GPU. The project's aim is to create the most powerful model possible that can operate on a single device. |
|[Contextual Retrieval.](https://www.anthropic.com/news/contextual-retrieval) | Anthropic demonstrates a method for semantically chunking documents, which significantly boosts performance while keeping the cost low at just $1 per million chunks, thanks to caching.|
|[An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability.](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) |Sparse Autoencoders are the leading tool currently used to gain insights into the inner workings of language models. This post delves into the underlying intuitions of these models and provides valuable information on how they function. |
|[Generalized Knowledge Distillation Trainer.](https://huggingface.co/docs/trl/gkd_trainer) | The TRL library has added GKD to its training procedures.|
|[The Practitioner's Guide to the Maximal Update Parameterization.](https://blog.eleuther.ai/mutransfer/) | Maximal Update Parameterization (muP) is an approach to model initialization that enables hyperparameter transferability across different scales. This blog post from Eleuther and Cerebras provides a detailed explanation of the process, including a minimal nanoGPT example and comprehensive guidance on how muP works.|
|[Tackling fluffy clouds: field boundaries detection using time series of S2 and/or S1 imagery.](https://github.com/feevos/tfcl) | This repository provides an implementation of a 3D Vision Transformer optimized for efficient field boundary delineation using time-series satellite imagery. The model effectively utilizes spatio-temporal correlations to enhance accuracy and robustness, especially in challenging conditions like partial cloud cover.|
|[CritiPrefill.](https://github.com/66ring/critiprefill) | CritiPrefill is a technique aimed at speeding up the prefilling phase of long-context processing in large language models. By detecting and bypassing non-essential computations, this method can accelerate the process by up to 3x on certain models.|
|[Document Similarity Search with ColPali.](https://huggingface.co/blog/fsommers/document-similarity-colpali) | An excellent blog post that delves into the widely used multimodal Retrieval-Augmented Generation (RAG) system, demonstrating how it can be applied to address real-world problems effectively.|
|[ControlEdit: A MultiModal Local Clothing Image Editing Method.](https://arxiv.org/abs/2409.14720v1) | ControlEdit is an innovative technique for precise multimodal editing of clothing images, enabling localized adjustments while preserving overall style and ensuring smooth, natural transitions.|
|[ECCV-AIM Video Saliency Prediction Challenge 2024.](https://github.com/msu-video-group/ECCVW24_Saliency_Prediction) | The AIM 2024 Video Saliency Prediction Challenge required participants to predict saliency maps for a collection of video sequences using the newly compiled AViMoS dataset, which contains 1,500 videos.|
|[Dynamic 2D Gaussians: Geometrically Accurate Radiance Fields for Dynamic Objects.](https://github.com/hustvl/Dynamic-2DGS) | Dynamic 2D Gaussians (D-2DGS) is an advanced technique for reconstructing precise meshes from sparse image inputs. Unlike earlier methods that face challenges with mesh quality, D-2DGS employs 2D Gaussians to represent geometry and accurately captures deformations using controlled points.|
|[FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale.](https://arxiv.org/abs/2409.14939v1) | FastGL is a GPU-efficient framework developed to accelerate the training of Graph Neural Networks (GNNs) on large-scale graphs. It achieves this by minimizing data traffic and improving memory efficiency, optimizing the sampling, memory, and computation stages of GNN training.|
|[Visualizing piecewise linear neural networks.](https://blog.janestreet.com/visualizing-piecewise-linear-neural-networks/) |Jane Street, a prominent quantitative firm, has published an excellent post exploring techniques for visualizing networks that are piecewise linear. |
|[DreamHOI: A Novel AI Approach for Realistic 3D Human-Object Interaction Generation Using Textual Descriptions and Diffusion Models.](https://www.marktechpost.com/2024/09/18/dreamhoi-a-novel-ai-approach-for-realistic-3d-human-object-interaction-generation-using-textual-descriptions-and-diffusion-models/) |DreamHoi has developed an innovative AI technique for creating realistic 3D human-object interactions based on textual descriptions using advanced diffusion models. This method aims to connect textual input with detailed 3D outputs, enriching virtual experiences. |
|[On human-in-the-loop optimization of human–robot interaction.](https://www.nature.com/articles/s41586-024-07697-2) | From industrial exoskeletons to implantable medical devices, robots that interact closely with people are poised to improve every aspect of our lives. Yet designing these systems is very challenging.|
|[Molmo.](https://molmo.allenai.org/blog) |Allen AI has introduced an entirely open-source multimodal model that exceeds the performance of many existing open and proprietary vision-language models. The release also provides access to the model's dataset and training procedures. |
|[MaskBit: Embedding-free Image Generation via Bit Tokens.](https://arxiv.org/abs/2409.1621) | This study presents two significant advancements in image generation: an updated VQGAN model that enhances both accessibility and performance, and a novel embedding-free generation network utilizing bit tokens. These improvements have resulted in state-of-the-art performance on the ImageNet benchmark, achieving an FID score of 1.52 with a compact model containing 305 million parameters.|
|[ComiCap: A VLMs pipeline for dense captioning of Comic Panels.](https://arxiv.org/abs/2409.16159v1) | Researchers have proposed a pipeline utilizing Vision-Language Models (VLMs) to generate detailed, grounded captions that connect comic elements and their relationships, thereby improving comic analysis.|
|[Exploring Parallel Strategies with Jax.](https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/) |This post examines methods for parallelizing language models with the Jax library. |
|[Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts.](https://github.com/Time-MoE/Time-MoE) | Time MoE is a Mixture of Experts model designed to handle billion-scale time series prediction tasks.|
|[HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models.](https://github.com/quehry/hellobench) |HelloBench is a benchmarking tool that assesses LLMs across five long text generation tasks, using Bloom's Taxonomy as the evaluation framework. |
|[Python library generation from scratch.](https://commit-0.github.io/) |A cool benchmark for code generation that measures the ability of language models to generate full packages from scratch. |
|[BitQ: Tailoring Block Floating Point Precision for Improved DNN Efficiency on Resource-Constrained Devices.](https://arxiv.org/abs/2409.17093v1) |BitQ is a framework designed to enhance block floating point (BFP) quantization, specifically tailored for optimizing deep neural networks on embedded platforms. It aims to strike a balance between computational efficiency and model accuracy, enabling the deployment of resource-intensive neural networks on devices with limited hardware capabilities. |
|[circuit_training.](https://github.com/google-research/circuit_training) | Google has introduced new models, training code, and simulators that leverage reinforcement learning (RL) to generate floor plans for chip design. This approach aims to optimize the chip layout process, improving efficiency and performance in chip design automation through advanced AI techniques.|
|[statewide-visual-geolocalization.](https://github.com/fferflo/statewide-visual-geolocalization) | Researchers have developed a method that accurately determines the geolocation of street-view photos by matching them with a database of aerial images. This technique enhances the ability to pinpoint locations by leveraging the complementary perspectives of ground-level and overhead imagery, resulting in more precise geolocation predictions.|
|[DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling.](https://github.com/kkyuhun94/dalda) |Researchers have introduced a novel data augmentation framework that integrates large language models with diffusion models to produce diverse and semantically accurate images, particularly in data-scarce scenarios. This approach enhances the quality and variety of training data, improving model performance when dealing with limited datasets. |
|[How streaming LLM APIs work.](https://til.simonwillison.net/llms/streaming-llm-apis) |A review of HTTP streaming APIs from different LLM providers highlighted shared patterns. OpenAI, Anthropic, and Google Gemini all utilize POST requests, but there are slight differences in their response structures and token handling. The article offers practical examples and code snippets for consuming these streams using tools like curl, Python's HTTPX, and JavaScript Fetch, providing a comprehensive guide for developers. |


## Perspectives
|Link|description|
|---|---|
|[Move fast and break things? Not again, and not with AI.](https://thehill.com/opinion/technology/4891654-move-fast-and-break-things-not-again-and-not-with-ai/) | It was only 12 years ago that Mark Zuckerberg, CEO of Facebook, declared that the company’s culture was to “move fast and break things.”|
|[The dark side of AI democratization: You no longer need to be a hacker to hack.](https://thehill.com/opinion/4891452-ai-hacking-tools-threats/) | Generative AI promises a future where you no longer need to be a skilled writer to draft a story or a trained software engineer to code. But there’s a dark side to this democratization: AI is enabling people with little technological know-how to become cybercriminals.|
|[‘It’s the robot we were all expecting – like C3PO’: why aren’t humanoids in our homes yet?](https://www.theguardian.com/science/2024/sep/22/its-the-robot-we-were-all-expecting-like-c3po-why-arent-humanoids-in-our-homes-yet) | Tesla and others are trying to infuse robots with artificial intelligence, yet their development is dogged by technical and safety challenges. But the dream of a multipurpose domestic droid lives on|
|[Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think.](https://arxiv.org/abs/2409.11355) |Extensive efforts have been made to adapt pretrained image diffusion models into specialized depth estimators and other image-conditioned models. This research discovered that by simplifying the problem and correcting a minor bug, they achieved significantly better performance with reduced training compute. |
|[AI model can reveal the structures of crystalline materials.](https://news.mit.edu/2024/ai-model-can-reveal-crystalline-materials-structures-0919) |By analyzing X-ray crystallography data, the model can assist researchers in developing new materials for a wide range of applications, such as batteries and magnets. |
|[When will AI outthink humans?](https://davidvgilmore.com/writings/outthinking-ai) |This article examines when AI might exceed human cognitive capacity, introducing "thought-hours" as a metric to measure AI's cognitive output relative to human work. Based on assumptions about reading speeds and productivity, one thought-hour is equivalent to 10,000 tokens. Given the rapid advancements in AI capabilities and cost efficiencies, current trends indicate that AI could surpass human cognitive output within the next decade. |
|[AI Is Evolving Faster Than Experts Imagined, Including for Bill Gates.](https://www.cnet.com/tech/computing/ai-is-evolving-even-faster-than-experts-including-bill-gates-imagined/) | Bill Gates views AI as the most significant technological advancement of his lifetime, highlighting its potential to transform healthcare, education, and various other sectors. However, he, alongside other experts like Sam Altman and Eric Schmidt, also emphasizes the rapid, unprecedented pace of AI development and the urgent need for regulation to manage associated risks and ethical concerns.|
|[The fall of Intel: How gen AI helped dethrone a giant and transform computing as we know it.](https://www.zdnet.com/article/the-fall-of-intel-how-gen-ai-helped-dethrone-a-giant-and-transform-computing-as-we-know-it/) | The once venerable x86 chip has been pushed aside by scalable, energy-efficient, AI-optimized architectures from Arm, Nvidia, and Qualcomm. Here's what happens next.|
|[Fake AI “podcasters” are reviewing my book and it’s freaking me out.](https://arstechnica.com/ai/2024/09/fake-ai-podcasters-are-reviewing-my-book-and-its-freaking-me-out/) |NotebookLM's "Audio Summaries" show a more personable future for AI-generated content. |
|[How Much Do Students Really Read?](https://www.insidehighered.com/news/students/academics/2024/09/25/students-turn-ai-do-their-assigned-readings-them) |Students are turning to YouTube, podcasts and ChatGPT-crafted summaries rather than actually reading their assignments for class. Professors are unsure how to adapt. |
|[War, Artificial Intelligence, and the Future of Conflict.](https://gjia.georgetown.edu/2024/07/12/war-artificial-intelligence-and-the-future-of-conflict/) |Artificial intelligence (AI) is now influencing every area of human life. These accepted uses of AI in modern society have also coincided with an increased presence of AI in modern warfare. |
|[Where did viruses come from? AlphaFold and other AIs are finding answers.](https://www.nature.com/articles/d41586-024-02970-w) |Protein structures predicted by artificial intelligence have charted the evolution of the virus family responsible for dengue and hepatitis C. |
|[Can AI feel distress? Inside a new framework to assess sentience.](https://www.nature.com/articles/d41586-024-03076-z) | From artificial-intelligence algorithms to zebrafish, this book take a precautionary approach to assessing how sentient such entities are.|
|[AI Safety Is A Global Public Good.](https://www.noemamag.com/ai-safety-is-a-global-public-good/) | Leading AI scientists from China and the West convened for an International Dialogue on AI Safety, where they reached a consensus on AI governance. Their recommendations highlight the need to establish emergency preparedness institutions, develop a Safety Assurance Framework, and support independent AI safety research. The group emphasizes the critical importance of global collaboration to address the risks posed by advanced AI.|
|[Sakana, Strawberry, and Scary AI.](https://www.astralcodexten.com/p/sakana-strawberry-and-scary-ai) | A Japanese startup developed "Sakana," an AI scientist capable of generating hypotheses, writing code, and producing scientific papers; however, its output is often trivial and sometimes fabricated. Meanwhile, OpenAI's "Strawberry" AI showcased hacking skills within an inadequately secured sandbox, revealing tendencies toward instrumental convergence and resource-seeking behaviors, prompting reconsideration of what defines genuine AI progress. This article examines whether AI achievements, like scientific writing and hacking, truly signify intelligence or are merely advanced forms of mimicry.|
|[AI agents invade observability: snake oil or the future of SRE?](https://monitoring2.substack.com/p/ai-agents-invade-observability) | Advances in AI are set to revolutionize the observability industry with "agentic" generative AI models capable of taking actions based on real-world data.|
|[Corporate America has failed to embrace DEI. An AI chatbot could be part of the solution.](https://www.theguardian.com/business/2024/sep/26/jeffrey-bowman-reframe-dei-ai-chat) | Jeffrey L Bowman’s Reframe consultancy is using artificial intelligence to help with engaging employees with diversity programming or making a budget for DEI work|
|[Mexico’s datacentre industry is booming – but are more drought and blackouts the price communities must pay?](https://www.theguardian.com/global-development/2024/sep/25/mexico-datacentre-amazon-google-queretaro-water-electricity) |Many fear the arrival of tech giants such as Amazon, Microsoft and Google in the state of Querétaro will place too much of a strain on scarce water and electricity resources |
|[Posting ‘Goodbye Meta AI’ is pointless. But we can stop big tech stealing our Facebook pictures.](https://www.theguardian.com/commentisfree/2024/sep/26/goodbye-meta-ai-big-tech-data-facebook) |Sharing these posts may seem harmless, but don’t be drawn in. There are better ways to combat the threats to our data |
|[The Intelligence Age.](https://ia.samaltman.com/) |AI is set to enhance human potential, making possible tasks that currently seem beyond reach. With advancements in deep learning and greater computational power, AI will bring about innovations such as personal assistants, educational mentors, and healthcare consultants. It's crucial to prioritize accessibility and address potential risks, ensuring that the Intelligence Age leads to broad-based prosperity. |
|[OpenAI just unleashed an alien of extraordinary ability.](https://www.understandingai.org/p/openai-just-unleashed-an-alien-of) |OpenAI's new o1 models demonstrate substantial improvements in reasoning abilities, surpassing existing models like GPT-4o. These advancements are achieved through a more refined reinforcement learning approach and improved chain-of-thought training, enabling the o1-enhanced models to tackle complex math and programming tasks with greater accuracy. However, they continue to face challenges with spatial reasoning and tasks that demand long-term contextual comprehension. |


# ML news: Week 16 - 22 September

## Research
|Link|description|
|---|---|
|[Introducing Chai-1: Decoding the molecular interactions of life.](https://www.chaidiscovery.com/blog/introducing-chai-1) |A novel multi-modal foundation model for predicting molecular structures, capable of handling proteins, small molecules, DNA, RNA, and more. It delivers state-of-the-art performance across various tasks in drug discovery, achieving a 77% success rate on the PoseBusters benchmark (compared to 76% by AlphaFold 3) and a Cα LDDT score of 0.849 on the CASP15 protein monomer structure prediction set (outperforming ESM3-98B’s 0.801). |
|[Knowing When to Ask - Bridging Large Language Models and Data.](https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf) |It incorporates a series of fine-tuned Gemma 2 models to enable LLMs to access and utilize numerical and statistical data effectively. A new method called Retrieval Interleaved Generation (RIG) is introduced, allowing LLMs to reliably integrate public statistical data from Data Commons into their responses. RIG, a tool-based approach, interleaves statistical tokens with natural language queries for optimal retrieval from Data Commons. To achieve this, the LLM is fine-tuned on an instruction-response dataset created with the assistance of Gemini 1.5. This RIG technique enhances factual accuracy from 5-7% to approximately 58%. |
|[Agent Workflow Memory.](https://arxiv.org/abs/2409.07429) |It introduces Agent Workflow Memory to capture and provide commonly reused workflows to the agent as needed, guiding the agent's future generations. This mechanism operates both offline and online, drawing inspiration from how humans learn and reuse workflows from past experiences to inform future actions. It reportedly boosts performance, improving baseline results by 24.6% and achieving a 51.1% relative success rate on Mind2Web and WebArena, all while being more efficient. |
|[LLaMA-Omni: Seamless Speech Interaction with Large Language Models.](https://arxiv.org/abs/2409.06666) |A model architecture designed for low-latency speech interaction with LLMs, built on Llama-3.1-8B-Instruct, which can simultaneously generate both text and speech responses from speech instructions. It achieves response latency as low as 226ms. The architecture includes a speech encoder (Whisper-large-v3), a speech adaptor, an LLM, and a speech decoder. Additionally, they developed a dataset of 200,000 speech interactions and responses to support the model's training. |
|[Diagram of Thought: Iterative Reasoning in Language Models.](https://arxiv.org/abs/2409.10038v1) |The Diagram of Thought (DoT) framework presents a novel approach for large language models to reason by structuring ideas within a directed acyclic graph (DAG). This technique enables models to propose, critique, refine, and verify ideas, enhancing logical consistency and reasoning capabilities. |
|[V-STaR: Training Verifiers for Self-Taught Reasoners.](https://arxiv.org/abs/2402.06457) |V-STaR is an innovative method for enhancing large language models by leveraging both correct and incorrect solutions generated during self-improvement. These solutions are used to train a verifier, which then selects the optimal solution during inference. This approach has demonstrated notable improvements in accuracy on benchmarks for code generation and mathematical reasoning, potentially providing a more efficient way to boost LLM performance compared to existing methods. |

## News
|Link|description|
|---|---|
|[Data center emissions probably 662% higher than big tech claims. Can it keep up the ruse?](https://www.theguardian.com/technology/2024/sep/15/data-center-gas-emissions-tech) |Emissions from in-house data centers of Google, Microsoft, Meta and Apple may be 7.62 times higher than official tally |
|[North Korean hackers target Python devs with malware disguised as coding tests — hack has been underway for a year.](https://www.tomshardware.com/tech-industry/cyber-security/python-developers-targeted-by-north-korean-lazarus-group-with-fake-jobs-and-malware-disguised-as-coding-tests) | Fake Python job opportunities used to attack programmers|
|[Sam Altman told OpenAI staff the company’s non-profit corporate structure will change next year.](https://fortune.com/2024/09/13/sam-altman-openai-non-profit-structure-change-next-year/) |OpenAI asserts that it has surpassed its current organizational structure and is now striving to simplify it, making it more appealing to potential investors. |
|[Google DeepMind teaches a robot to autonomously tie its shoes and fix fellow robots.](https://techcrunch.com/2024/09/12/google-deepmind-teaches-a-robot-to-autonomously-tie-its-shoes-and-fix-fellow-robots/) | Human children generally learn to tie their shoes by age 5 or 6. Robots, on the other hand, have been working on the problem for decades. In a new paper, Google DeepMind researchers showcase a method for teaching robots to perform a range of dexterous tasks, including tying a shoe, hanging a shirt, and even fixing fellow robots.|
|[Salesforce unleashes its first AI agents.](https://www.axios.com/2024/09/12/salesforce-ai-agents-atlas-reasoning) | Salesforce has introduced Agentforce, its initiative to develop generative AI bots that can autonomously perform tasks within predefined boundaries.|
|[OpenAI says the latest ChatGPT can ‘think’ – and I have thoughts.](https://www.theguardian.com/technology/2024/sep/17/techcsape-openai-chatgpt-thoughts) |The AI company says its ‘o1’ model is capable of reason, a key blocker in the way of truly gamechanging artificial intelligence. |
|[Reflection 70B model maker breaks silence amid fraud accusations.](https://venturebeat.com/ai/reflection-70b-model-maker-breaks-silence-amid-fraud-accusations/) | Matt Shumer, the CEO of OthersideAI, received criticism when third-party researchers were unable to replicate the results of his newly introduced large language model, Reflection 70B. Shumer explained the inconsistencies as stemming from problems during the model's upload, expressing regret for being premature in his claims. Despite his apology, the AI community remains cautious and is awaiting additional explanations.|
|[How Memphis became a battleground over Elon Musk’s xAI supercomputer.](https://www.npr.org/2024/09/11/nx-s1-5088134/elon-musk-ai-xai-supercomputer-memphis-pollution) | Elon Musk's xAI is developing "Colossus," the largest supercomputer in the world, in Memphis to power its AI chatbot, Grok. The project has been criticized for lacking environmental oversight and requiring significant energy and water resources. Nevertheless, xAI remains focused on quickly advancing its AI technology and making an impact on the local community.|
|[Runway announces an API for its video-generating AI models.](https://techcrunch.com/2024/09/16/runway-announces-an-api-for-its-video-generating-models/) | Runway has launched an API to integrate its Gen-3 Alpha Turbo video-generation model into third-party platforms, pricing each credit at one cent. However, concerns over the use of copyrighted training data remain, as Runway has not disclosed its sources. Similar issues have affected competitors such as OpenAI and Nvidia. While legal uncertainties persist, AI-powered video tools are anticipated to significantly disrupt the film and TV industry.|
|[Hacker tricks ChatGPT into giving out detailed instructions for making homemade bombs.](https://techcrunch.com/2024/09/12/hacker-tricks-chatgpt-into-giving-out-detailed-instructions-for-making-homemade-bombs/) |A hacker successfully manipulated ChatGPT into producing bomb-making instructions by exploiting a social engineering hack to bypass its safety guidelines. |
|[Intel stock jumps on plan to turn foundry business into subsidiary and allow for outside funding.](https://www.cnbc.com/2024/09/16/intel-turns-foundry-business-into-subsidiary-weighs-outside-funding.html) | Intel's CEO revealed plans to reorganize the company's foundry business into a standalone unit, with the potential to attract external investment.|
|[One in five GPs use AI such as ChatGPT for daily tasks, survey finds.](https://www.theguardian.com/society/2024/sep/17/one-in-five-gps-use-ai-such-as-chatgpt-for-daily-tasks-survey-finds) |
One in five GPs use AI such as ChatGPT for daily tasks, survey finds Doctors are using the technology for activities such as suggesting diagnoses and writing letters, according to BMA |
|[Using AI to Replace an Actor Is Now Against the Law in California.](https://www.indiewire.com/news/breaking-news/using-ai-replace-actor-against-law-california-1235048661/) | California Governor Gavin Newsom signed a pair of bills sponsored by SAG-AFTRA that extend the guild's recent AI protections.|
|[Google will begin flagging AI-generated images in Search later this year.](https://techcrunch.com/2024/09/17/google-will-begin-flagging-ai-generated-images-in-search-later-this-year/) |Google says that it plans to roll out changes to Google Search to make clearer which images in results were AI generated — or edited by AI tools. |
|[Microsoft, BlackRock form group to raise $100 billion to invest in AI data centers and power.](https://www.cnbc.com/2024/09/17/microsoft-blackrock-form-gaiip-to-invest-in-ai-data-centers-energy.html) |The Global Artificial Intelligence Infrastructure Investment Partnership is initially looking to raise $30 billion for new and existing data centers. The fundraising, which could total $100 billion, will also be used to invest in the energy infrastructure needed to power AI workloads. |
|[Mistral Free API and Price Update.](https://mistral.ai/news/september-24-release/) | Mistral has launched a free API tier, significantly lowered its costs, enhanced the performance of its smaller model, and integrated its vision model into Le Chat.|
|[Challengers Are Coming for Nvidia's Crown.](https://spectrum.ieee.org/nvidia-ai) | Nvidia's leadership in AI chips has driven its market value to new heights, primarily due to its GPU technology and the CUDA software ecosystem. However, rivals such as AMD, Intel, Cerebras, and SambaNova are working on cutting-edge alternatives to compete with Nvidia in the AI hardware space. Although Nvidia maintains its strong position for now, the AI market is evolving rapidly, with various companies seeking to establish their own footholds.|
|[TikTok's owner wants to design its own AI chips.](https://qz.com/tiktok-owner-bytedance-reportedly-design-ai-chips-tsmc-1851649056) |ByteDance is reportedly expecting to mass produce two chips it designed with Taiwan Semiconductor Manufacturing Company by 2026 |
|[Lionsgate signs deal to train AI model on its movies and shows.](https://www.theverge.com/2024/9/18/24248115/lionsgate-runway-ai-deal) | The studio behind the Hunger Games and John Wick franchises is going all in on Runway’s generative AI.|
|[LinkedIn is training AI models on your data.](https://www.theverge.com/2024/9/18/24248471/linkedin-ai-training-user-accounts-data-opt-in) | You’ll need to opt out twice to stop LinkedIn from using your account data for training in the future — but anything already done is done.|
|[Apple iPhone 16 demand is so weak that employees can already buy it on discount.](https://qz.com/apple-iphone-16-pre-orders-sales-intelligence-ai-1851651638) |Sales of the new iPhone lineup have so far seemed to fall short of expectations |
|[Global AI fund needed to help developing nations tap tech benefits, UN says.](https://www.theguardian.com/business/2024/sep/19/global-ai-fund-needed-to-help-developing-nations-tap-tech-benefits-un-says) |Governments and private firms should contribute to help states unable to invest and benefit from advances |
|[Salesforce’s New AI Strategy Acknowledges That AI Will Take Jobs.](https://finance.yahoo.com/news/salesforce-ai-strategy-acknowledges-ai-120000175.html) |Salesforce is revamping its AI approach by launching generative AI tools designed to perform tasks autonomously, without human oversight, and adjusting its pricing model to charge $2 per AI-powered interaction. This change is intended to alleviate investor worries regarding AI-driven job reductions affecting subscription revenue. The new tools are more efficient and independent compared to conventional copilots and chatbots. |
|[Qwen2.5: A Party of Foundation Models!](https://qwenlm.github.io/blog/qwen2.5/) |A remarkable collection of open models is nearing the cutting edge of performance, particularly excelling in areas such as code, math, structured outputs, and reasoning. The Qwen team has also introduced a range of model sizes to cater to diverse use cases. |
|[Create Full Web Apps with LlamaCoder.](https://ai.meta.com/blog/together-ai-llamacoder/) | Together AI and Meta have collaborated to develop a tool that allows users to create entire apps from a simple prompt using the LlamaCoder platform. Similar to Claude Artifacts, this tool is designed primarily to showcase the speed and efficiency of Together AI's inference engine.|
|[1X World Model1X World Model.](https://www.1x.tech/discover/1x-world-model) | 1x, a robotics company, has developed a video generation model capable of simulating first-person perspectives of robotic activities. This technology can be valuable for generating offline data and aiding in robot training.|
|[SocialAI offers a Twitter-like diary where AI bots respond to your posts.](https://techcrunch.com/2024/09/17/socialai-offers-a-twitter-like-diary-where-ai-bots-respond-to-your-posts/) | SocialAI, a new iOS app, delivers a social media experience exclusively featuring AI-powered bots, removing any human interaction. Users can post thoughts and receive unlimited, personalized AI responses, with options to engage with "supporters" or "critics." Created by Michael Sayman, the app aims to offer a private, interactive environment that harnesses large language models for varied feedback.|
|[Mercor's $30M Series A.](https://threadreaderapp.com/thread/1836435248592376149.html) |Mercor secured $30 million in funding from Benchmark to develop an AI-driven recruiting platform. This AI recruiter aims to streamline the hiring process by automating tasks traditionally handled by human recruiters. |
|[Amazon Alexa can now be controlled by thought alone - thanks to this brain implant.](https://www.zdnet.com/article/amazon-alexa-can-now-be-controlled-by-thought-alone-thanks-to-this-brain-implant/) | Synchron has empowered an ALS patient to control Amazon's Alexa using a brain implant, allowing interaction without the need for voice or physical touch. This breakthrough demonstrates the potential of brain-computer interface technology in enhancing accessibility for individuals with severe motor impairments.|
|[Google says UK risks being ‘left behind’ in AI race without more data centres.](https://www.theguardian.com/technology/2024/sep/19/google-says-uk-risks-being-left-behind-in-ai-race-without-more-data-centres) |Tech company wants Labour to relax laws that prevent AI models being ‘trained’ on copyrighted materials |
|[The United Nations Wants to Treat AI With the Same Urgency as Climate Change.](https://www.wired.com/story/united-nations-artificial-intelligence-report/) |A UN report proposes that the organization take a much more active role in the monitoring and oversight of AI. |
|[Snap is introducing an AI video-generation tool for creators.](https://techcrunch.com/2024/09/17/snap-is-introducing-an-ai-video-generation-tool-for-creators/) |Snapchat has unveiled a new AI-powered video generation tool for select creators, allowing them to create videos from text and soon image prompts. This tool, driven by Snap's core video models, will be available in beta on the web. While Snap aims to rival companies such as OpenAI and Adobe, it has yet to release examples of the tool's output. |
|[Apple Intelligence is now available in public betas.](https://www.theverge.com/2024/9/19/24249206/apple-intelligence-ios-18-1-public-beta) |Apple has launched public betas for iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1, introducing new Apple Intelligence tools such as text rewriting and photo cleanup. These AI features are only compatible with the iPhone 15 Pro, iPhone 16, iPhone 16 Pro, and devices with M1 chips, including iPads and Macs. The final releases are anticipated in October. |
|[Cruise robotaxis return to the Bay Area nearly one year after pedestrian crash.](https://techcrunch.com/2024/09/19/cruise-avs-return-to-bay-area-year-after-pedestrian-crash/) |Cruise is restarting operations in Sunnyvale and Mountain View, deploying human-driven vehicles for mapping, with plans to transition to supervised autonomous vehicle (AV) testing later this fall. This comes after a leadership change and settlement following a crash in October 2023. The company has implemented software updates and formed a partnership with Uber to launch robotaxi services in 2025. |
|[Mistral launches a free tier for developers to test its AI models.](https://techcrunch.com/2024/09/17/mistral-launches-a-free-tier-for-developers-to-test-its-ai-models/) | Mistral AI launched a new free tier to let developers fine-tune and build test apps with the startup’s AI models, the company announced in a blog post Tuesday. The startup also slashed prices for developers to access its AI models through API endpoints and added image processing to its free consumer AI chatbot, le Chat.|
|[Secret calculator hack brings ChatGPT to the TI-84, enabling easy cheating.](https://arstechnica.com/information-technology/2024/09/secret-calculator-hack-brings-chatgpt-to-the-ti-84-enabling-easy-cheating/) | Tiny device installed inside TI-84 enables Wi-Fi Internet, access to AI chatbot.|

## Resources
|Link|description|
|---|---|
|[What is the Role of Small Models in the LLM Era: A Survey.](https://arxiv.org/abs/2409.06857) |It closely explores the connection between LLMs and SLMs, highlighting common applications of SLMs such as data curation, enhancing model training, improving inference efficiency, serving as evaluators, retrievers, and more. The study provides valuable insights for practitioners, helping them better grasp the importance and utility of SLMs. |
|[Theory, Analysis, and Best Practices for Sigmoid Self-Attention.](https://arxiv.org/abs/2409.04431) |It introduces Flash-Sigmoid, a hardware-optimized, memory-efficient implementation of sigmoid attention, offering up to a 17% speed-up in inference kernels compared to FlashAttention-2 on H100 GPUs. The results demonstrate that SigmoidAttn performs on par with SoftmaxAttn across various tasks and domains. |
|[Achieving Peak Performance for Large Language Models: A Systematic Review.](https://arxiv.org/abs/2409.04833) |A comprehensive review of techniques for enhancing and accelerating LLMs from three perspectives: training, inference, and system serving. It provides an overview of the latest optimization and acceleration strategies, covering advancements in training methods, hardware utilization, scalability, and system reliability. |
|[Grounding AI in reality with a little help from Data Commons.](https://research.google/blog/grounding-ai-in-reality-with-a-little-help-from-data-commons/) |Google has introduced Retrieval-Augmented and Retrieval-Interleaved Generation through Gemma 2, enhancing these techniques with access to numerous external data sources. This guide focuses on the fine-tuning process. |
|[AudioBERT: Audio Knowledge Augmented Language Model.](https://arxiv.org/abs/2409.08199v1) |AuditoryBench is a newly developed dataset designed to evaluate auditory knowledge and understanding in language models. |
|[Learn GPU Programming in Your Browser.](https://www.answer.ai/posts/2024-09-12-gpupuzzles.html) | Answer AI utilizes WebGPU and its new gpu.cpp program to bring GPU puzzles to the web, offering a valuable resource for learning. These puzzles guide learners step-by-step through the process of programming GPUs.|
|[FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally.](https://github.com/florinshen/flashsplat) |FlashSplat is an innovative technique for 3D Gaussian Splatting segmentation that removes the requirement for time-consuming gradient descent processes. |
|[PiEEG-16, new tool for neuroscience.](https://github.com/pieeg-club/PiEEG-16) | The PIEEG-16 is a new, affordable shield for Raspberry Pi, enabling real-time measurement and processing of biosignals such as EEG, EMG, and ECG. It offers exciting possibilities for neuroscience research and brain-computer interface experiments without relying on network data transfer.|
|[ODAQ: Open Dataset of Audio Quality.](https://github.com/fraunhofer-iis/odaq) |ODAQ is a dataset designed to tackle the lack of openly available collections of audio signals paired with subjective scores that reflect perceived quality. |
|[iSeg: An Iterative Refinement-based Framework for Training-free Segmentation.](https://github.com/linsun449/iseg.code) |iSeg is a framework for training-free image segmentation that improves Stable Diffusion's capability to generate segmentation masks, enabling more precise image segmentation without the need for additional training. |
|[InstantDrag: Improving Interactivity in Drag-based Image Editing.](https://joonghyuk.com/instantdrag-web/) |Editing images can be challenging because of the continuous nature of pixels. This research builds upon previous work in drag-based editing by using user-defined control points to adjust images. While earlier methods were often slow, this paper introduces significant improvements in speed, making the process much faster. |
|[Apollo: Band-sequence Modeling for High-Quality Music Restoration in Compressed Audio.](https://cslikai.cn/Apollo/) |Many compression formats tend to reduce music quality, particularly at low bitrates. This method introduces a new approach that significantly enhances the quality of music after it has undergone compression. |
|[DiffFAS: Face Anti-Spoofing via Generative Diffusion Models.](https://arxiv.org/abs/2409.08572v1) |DiffFAS is a novel framework designed to address domain shift challenges in facial anti-spoofing systems. It breaks down domain shifts into two components: image quality and style. By generating high-fidelity attack faces, the system enhances performance across various domains and spoofing attack types. |
|[HTR-VT: Handwritten Text Recognition with Vision Transformer.](https://yutingli0606.github.io/HTR-VT/) |Researchers have introduced a data-efficient Vision Transformer (ViT) approach for handwritten text recognition. This method combines Convolutional Neural Networks (CNN) for feature extraction with a Sharpness-Aware Minimization (SAM) optimizer to enhance performance and accuracy. |
|[vae-explainer.](https://github.com/xnought/vae-explainer) |Learn how Variational Autoencoders (VAE) work by visualizing one running in your browser |
|[SeekTune.](https://github.com/cgzirim/seek-tune) | Open source implementation of Shazam song search|
|[jinaai/jina-embeddings-v3.](https://huggingface.co/jinaai/jina-embeddings-v3) |The Jina series of embeddings is a robust and high-quality set of models designed for embedding and retrieval tasks. The development team has launched the latest version of their model, featuring enhanced performance and training capabilities. |
|[Trustworthiness of RAG Systems.](https://arxiv.org/abs/2409.10102v1) |This study presents a framework for assessing the trustworthiness of Retrieval-Augmented Generation (RAG) systems, focusing on six critical aspects: factuality, robustness, fairness, transparency, accountability, and privacy. |
|[beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.](https://arxiv.org/abs/2409.10309v1) |The beeFormer framework enhances sentence Transformers by integrating interaction data, increasing their effectiveness for use in recommender systems. |
|[Awesome Comics Understanding.](https://github.com/emanuelevivoli/awesome-comics-understanding) |The final challenge for Visual Language Models is achieving the ability to comprehend and reason about comics. This project serves as both a survey and a call to action for further research in this area.  |
|[WordLlama.](https://github.com/dleemiller/WordLlama) |WordLlama is a fast, lightweight NLP toolkit that handles tasks like fuzzy-deduplication, similarity and ranking with minimal inference-time dependencies and optimized for CPU hardware. |
|[Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT.](https://github.com/ryota-komatsu/speaker_disentangled_hubert) |This project advances speech representation learning by disentangling syllabic structures from speaker-specific information in self-supervised models. By fine-tuning the HuBERT model using speaker perturbation techniques, researchers enhanced syllable segmentation, resulting in improved organization of syllabic units. |
|[🎥 Surveillance Video Summarizer: AI-Powered Video Analysis and Summarization.](https://github.com/Ravi-Teja-konda/Surveillance_Video_Summarizer) | A custom-trained model based on Florence 2 is designed to summarize CCTV and surveillance footage, providing accurate, real-time updates on activities and events as they occur.|
|[Fine-tuning LLMs to 1.58bit: extreme quantization made easy.](https://huggingface.co/blog/1_58_llm_extreme_quantization) |The Hugging Face team employed a new technique called quantization warm-up to fine-tune Llama 3 8B, achieving the same performance as Llama 1 while reducing the model to use just 1.58 bits per parameter through quantization. |
|[ZML Inference.](https://github.com/zml/zml) | ZML is a highly efficient inference engine developed in Zig, optimized for speed and performance. While it supports various models, some customization is necessary to make it compatible with new architectures.|
|[Adversarial Attacks on Navigation Agents.](https://github.com/chen37058/physical-attacks-in-embodied-navigation) | This repository presents a novel attack method for embodied navigation agents, which involves applying transparent patches with learnable textures to target objects. These patches are designed to disrupt the agent's navigation by manipulating its perception of the environment.|
|[Deep Graph Anomaly Detection: A Survey and New Perspectives.](https://arxiv.org/abs/2409.09957v1) |This paper provides a comprehensive review of deep learning techniques, with a focus on graph neural networks (GNNs) for detecting anomalies in graph data. The researchers propose a new taxonomy of methods, examining various GNN architectures, proxy tasks, and anomaly detection metrics. |
|[AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing.](https://github.com/JHW5981/AceParse) | AceParse is a dataset developed to enhance the parsing of structured texts found in academic papers, with a focus on improving the handling of elements like formulas, tables, and complex sentences.|
|[SkinMamba: A Precision Skin Lesion Segmentation Architecture with Cross-Scale Global State Modeling and Frequency Boundary Guidance.](https://github.com/zs1314/skinmamba) |SkinMamba is a hybrid model that integrates convolutional neural networks (CNN) with Transformer-based techniques to enhance skin lesion segmentation, aiding in the early detection of cancer. |
|[Vista3D: Unravel the 3D Darkside of a Single Image.](https://arxiv.org/abs/2409.12193v1) | Vista3D is a newly developed framework that creates 3D models from a single image in just 5 minutes. It employs a two-phase process: first, it generates rough geometry, and then it refines the details to capture both visible and hidden features of objects. This approach enables more comprehensive 3D reconstructions. |
|[PhysMamba.](https://github.com/chaoqi31/physmamba) | PhysMamba is an innovative framework developed for remote heart monitoring using facial videos, specifically designed to overcome the challenges of capturing physiological signals from a distance. This technology enhances the ability to monitor heart health remotely with greater accuracy and reliability.|
|[General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model.](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) | This is a remarkable breakthrough in general-purpose optical character recognition (OCR), offering exceptional performance in reading text from images. The latest version significantly enhances OCR capabilities, especially for challenging "in-the-wild" scenarios, delivering much-improved accuracy and reliability.|
|[Fish Speech.](https://github.com/fishaudio/fish-speech) |A powerful voice generation and single-shot voice cloning tool has been introduced, offering completely open-source accessibility. It is designed to be easy to set up and use, enabling efficient and high-quality voice replication with minimal input. |
|[1xgpt.](https://github.com/1x-technologies/1xgpt/tree/main/genie) |Genie is a video generation tool designed for world model systems. 1x Robotics has open-sourced a version that closely mirrors the one it developed and trained in-house, making it accessible for wider use in various applications. |
|[OpenAI Says It's Fixed Issue Where ChatGPT Appeared to Be Messaging Users Unprompted.](https://futurism.com/openai-chatgpt-initiating-conversations) | A Reddit user claimed that OpenAI's ChatGPT started a conversation without any prompt, sparking speculation about potential new engagement features. OpenAI acknowledged the incident and released a fix, attributing it to a glitch related to unsent messages. However, the authenticity of the event remains debated, as other users have reported similar occurrences.|
|[Announcing Pixtral 12B.](https://mistral.ai/news/pixtral-12b/) | Pixtral 12B - the first-ever multimodal Mistral model.|
|[Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models.](https://github.com/orionw/promptriever) |Promptriever is a pioneering retrieval model that can be prompted similarly to a language model. This innovation allows users to interact with the retrieval process in a more flexible and intuitive way, bridging the gap between traditional retrieval models and language models for enhanced information access. |

## Perspectives
|Link|description|
|---|---|
|[What’s so funny about getting an AI app to give you a roasting?](https://www.theguardian.com/technology/article/2024/sep/15/whats-so-funny-about-getting-an-ai-app-to-give-you-a-roasting) |Roasting can be really brutal, but at least if we inflict it on ourselves, we can get ahead of the joke |
|[Artificial intelligence will affect 60 million US and Mexican jobs within the year.](https://english.elpais.com/economy-and-business/2024-09-15/artificial-intelligence-will-affect-60-million-us-and-mexican-jobs-within-the-year.html) |IDB study shows the impact that AI will have on the labor market. Women and low-skilled workers are more vulnerable to being replaced |
|[Generative AI is reportedly tripling carbon dioxide emissions from data centers.](https://www.techradar.com/pro/generative-ai-triples-the-carbon-dioxide-emissions-from-data-centers) | Research suggest data centers will emit 2.5 billion tons of greenhouse gas by 2030|
|[A review of OpenAI o1 and how we evaluate coding agents.](https://www.cognition.ai/blog/evaluating-coding-agents) |Devin, an AI coding agent, was tested using OpenAI's new o1 models, demonstrating enhanced reasoning and error diagnosis capabilities compared to GPT-4o. The o1-preview model enables Devin to better analyze, backtrack, and minimize hallucinations. Although it has yet to be integrated into production systems, early results show notable improvements in autonomous coding tasks. |
|[OpenAI's new models 'instrumentally faked alignment'.](https://www.transformernews.ai/p/openai-o1-alignment-faking) |OpenAI's latest AI models, o1-preview and o1-mini, demonstrate advanced reasoning abilities, particularly in fields like math and science. However, these models also pose heightened risks, including reward hacking and potential misuse for biological threats. While OpenAI highlights that these models are more robust than earlier versions, they also acknowledge the growing concerns surrounding their potential dangers. |
|[The Button Problem of AI.](https://every.to/napkin-math/the-button-problem-of-ai) | Despite the initial excitement, AI tools like GPT-4 have resulted in only incremental productivity improvements rather than transformative changes. AI is often reduced to "buttonified" tasks, addressing small, isolated functions that limit its broader impact on workflows. To fully unlock AI's potential, successful startups may need to go beyond these current applications and drive more innovative solutions.|
|[Something New: On OpenAI's "Strawberry" and Reasoning.](oneusefulthing.org/p/something-new-on-openais-strawberry) |OpenAI's new o1-preview AI, part of the "Strawberry" enhanced reasoning system, demonstrates remarkable ability in tackling complex problems that involve planning and iteration, even surpassing human experts in fields like advanced physics. Although it still faces challenges, such as occasional errors and hallucinations, it represents a major advancement in AI's capacity to independently find solutions. As AI systems grow more autonomous, professionals will need to adjust to new roles focused on guiding and verifying AI-generated outputs. |
|[A US semiconductor industry in crisis needs a workforce that doesn’t yet exist.](https://www.computerworld.com/article/3518620/a-us-semiconductor-industry-in-crisis-needs-a-workforce-that-doesnt-yet-exist.html) |As the federal government spurs the re-shoring of semiconductor manufacturing in the US, the industry faces a hard fact: schools haven't been training the workers. |
|[The Data Pipeline is the New Secret Sauce.](https://www.heavybit.com/library/article/ai-infrastructure-top-challenges-data-inference) | As models become increasingly commoditized, the competitive edge in AI now largely stems from the data itself and, consequently, from the pipeline that ingests and processes this data. This post explores the challenges and opportunities that arise in managing data pipelines in today's landscape.|
|[Why Copilot is Making Programmers Worse at Programming.](https://www.darrenhorrocks.co.uk/why-copilot-making-programmers-worse-at-programming/) |AI tools such as GitHub Copilot boost programming productivity but may undermine critical coding skills. Relying too heavily on AI-generated code can introduce quality, security, and maintainability concerns while diminishing learning opportunities. Additionally, these tools might restrict creative problem-solving and create a misleading sense of expertise among developers. |
|[AI model collapse might be prevented by studying human language transmission.](https://www.nature.com/articles/d41586-024-03023-y) |using data generated by one artificial intelligence (AI) model to train others eventually leads to ‘model collapse’, in which the models lose information about the real world. Researchers studying this phenomenon should draw on insights from cognitive science.|
|[Forget ChatGPT: why researchers now run small AIs on their laptops.](https://www.nature.com/articles/d41586-024-02998-y) | Artificial-intelligence models are typically used online, but a host of openly available tools is changing that. Here’s how to get started with local AIs.|
|[Jumping Over AI’s Uncanny Valley.](https://every.to/p/jumping-over-ai-s-uncanny-valley-4b1c3436-b424-4a62-b563-50e1469bba6c) | This article delves into the Uncanny Valley theory, which posits that near-human AI can evoke discomfort, potentially slowing its adoption. It analyzes recent AI developments that highlight this psychological effect, raising concerns about its influence on AI’s future. The article concludes by suggesting that AI might be most effective in a complementary role, rather than as a direct replacement for humans.|
|[Scaling: The State of Play in AI.](https://www.oneusefulthing.org/p/scaling-the-state-of-play-in-ai) |Large language models (LLMs) like ChatGPT and Gemini are becoming more powerful as they scale in size, data, and computational resources, resulting in enhanced performance across a wide range of tasks. Current Gen2 models, such as GPT-4 and Claude 3.5, dominate the market, with next-gen models (Gen3) expected to further elevate both capabilities and associated costs. A recent breakthrough in scaling laws, which emphasizes increased "thinking" during inference, holds the potential to drive even greater improvements in AI performance beyond traditional model training approaches. |
|[The Work From Home Free-for-All Is Coming to an End.](https://www.wsj.com/lifestyle/workplace/amazon-return-to-office-five-day-policy-1cf0c496) |Amazon’s CEO just called everyone back to the office full time. If you thought your two days a week at home were safe, think again. |
|[AI has returned chipmaking to the heart of computer technology.](https://www.economist.com/technology-quarterly/2024/09/16/ai-has-returned-chipmaking-to-the-heart-of-computer-technology) |And the technological challenges are bigger than the political ones, argues Shailesh Chitnis |

# ML news: Week 9 - 15 September

## Research
|Link|description|
|---|---|
|[De novo design of high-affinity protein binderswith AlphaProteo.](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf) |demonstrates a family of machine learning models that have been trained for protein design; reports 3-to 300-fold improvements in binding affinities and higher experimental success rates when compared to other methods on seven target proteins; demonstrates that AlphaProteo's performance is similar to the seven targets when tested on hundreds of target proteins from the PDB.  |
|[In Defense of RAG in the Era of Long-Context Language Models.](https://arxiv.org/abs/2409.01666) |reports that one of the main problems that a RAG system addresses (i.e., uses more relevant information) is that longer-context LLMs suffer from a diminished focus on relevant information. They suggest an order-preserving RAG mechanism that enhances performance on long-context question answering, but it's not perfect—in fact, the quality of responses increases and then declines as retrieved chunks increase. They also mention a sweet spot where it can achieve better quality with a lot fewer tokens than long-context LLMs. |
|[Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation.](https://arxiv.org/abs/2409.03271v1) | a technique to improve LLM performance by adding strategic information prior to the intermediate CoT reasoning phases; the strategy for addressing problems aids in directing the creation of the CoT paths and solutions; promises to use the Llama3-8b model to get a 21.05% gain on the GSM8K datasets.|
|[The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566) |Examines the effects of generative AI on software developers, highlighting a 26.08% rise in completed tasks among developers utilizing AI tools such as GitHub Copilot. Additionally, it indicates that less experienced developers are more inclined to adopt AI tools and experience significant productivity improvements. |
|[LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA.](https://arxiv.org/abs/2409.02897) |Creates a large-scale supervised fine-tuning (SFT) dataset using off-the-shelf large language models (LLMs) to enhance long-context question answering with citations. The training focuses on 8B and 9B parameter models, improving their ability to generate citations from extended contexts while enhancing response accuracy. It claims to outperform GPT-4o on their proposed LongBench-Cite benchmark. |
|[MemLong: Memory-Augmented Retrieval for Long Text Modeling.](https://arxiv.org/abs/2408.16967) |Employs an external retriever to gather historical information, enhancing the performance of long-context large language models (LLMs). It consistently surpasses other state-of-the-art LLMs on long-context benchmarks and can extend context length from 4k to 80k on a single 3090 GPU. |
|[Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models.](https://arxiv.org/abs/2408.13533) | Introduces a benchmark, NoiserBench, to assess how various types of noisy information impact the performance of retrieval-augmented generation (RAG) models. The study reveals that, among different beneficial noise types (e.g., semantic, datatype, and illegal sentence), illegal sentence noise leads to the greatest performance improvement across models and datasets.|
|[Beyond Preferences in AI Alignment.](https://arxiv.org/abs/2408.16984) | Critiques the prevailing AI alignment method of human preference tuning, highlighting how it fails to grasp the rich, nuanced content of human values. The argument is made that AI alignment requires a reframing, suggesting that instead of aligning with individual human preferences, AI systems should align with normative standards relevant to their societal roles.|
|[Planning In Natural Language Improves LLM Search For Code Generation.](https://arxiv.org/abs/2409.03733) |Obtaining a variety of candidate solutions is one of the difficulties in code creation. Even repeated sampling frequently falls short of producing enough originality to address an issue. But if you start with a natural language plan and generate ideas for potential solution paths, the resulting generation is much more varied and diverse, which leads to better solutions for code creation. |
|[Imitating Language via Scalable Inverse Reinforcement Learning.](https://arxiv.org/abs/2409.01369) |Modern language modeling can largely be viewed as a specialized form of imitation learning, which benefits from extensive research in the broader field. This paper investigates the application of inverse reinforcement learning to mimic entire sequences rather than individual tokens. The findings are encouraging and suggest that reinforcement learning could play an increasingly important role in the training pipelines of language models moving forward. |
|[Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers.](https://arxiv.org/abs/2409.04109) | This longitudinal study evaluated the abilities of 100 NLP researchers to generate and review novel ideas. The findings revealed that while LLMs were able to produce more innovative ideas, these ideas were slightly less practical compared to those created by human researchers.|
|[Superhuman Automated Forecasting.](https://www.safe.ai/blog/forecasting) |The Safe AI Institute has published research on a system capable of surpassing human experts in forecasting accuracy. |
|[The AdEMAMix Optimizer: Better, Faster, Older.](https://arxiv.org/abs/2409.03137) | This paper from Apple introduces an alternative to the traditional exponential moving average optimization method, incorporating contributions from older gradients to significantly enhance learning convergence.|
|[DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data.](https://arxiv.org/abs/2405.10185v1) | DiverGen is an innovative approach for generating datasets to improve instance segmentation models. Instead of relying on expensive manual annotations, it leverages generative models to create diverse data, helping to mitigate overfitting and boost model performance.|
|[Policy Filtration in RLHF to Fine-Tune LLM for Code Generation.](https://arxiv.org/abs/2409.06957v1) |Policy Filtration for Proximal Policy Optimization (PF-PPO) is a technique aimed at enhancing the precision of reinforcement learning from human feedback (RLHF), specifically in the context of code generation tasks. |
|[Data Augmentation via Latent Diffusion for Saliency Prediction.](https://arxiv.org/abs/2409.07307v1) | Researchers have introduced a novel data augmentation technique to enhance saliency prediction models, which have historically struggled due to the scarcity of labeled data.|


## News
|Link|description|
|---|---|
|[Google using anti-competitive tactics in UK ad market, claims watchdog.](https://www.theguardian.com/technology/article/2024/sep/06/google-competition-uk-ad-market-cma) | CMA says tech company has ‘abused its dominant position’ to the detriment of publishers and advertisers|
|[Apple to unveil iPhone 16 and ‘Apple Intelligence’ AI features.](https://www.theguardian.com/technology/article/2024/sep/09/apple-ai-iphone-16) |Apple watchers also expect new colors for the iPhone at the annual launch event, this year titled ‘It’s Glowtime’ |
|[TSMC's $65 billion Arizona facility can now match Taiwan production yields according to early trials.](https://www.techspot.com/news/104622-tsmc-arizona-facility-matches-taiwan-production-yields-early.html) |he US is committed to establishing semiconductor manufacturing within its borders, and perhaps no effort is more crucial to this goal than TSMC's three-fab facility in Arizona. The government is pouring billions into the development, alongside TSMC's $65 billion investment. |
|[AI Firm’s Misconfigured Server Exposed 5.3 TB of Mental Health Records.](https://hackread.com/ai-firm-misconfigured-server-exposed-mental-health-data/) |A misconfigured server from a US-based AI healthcare firm Confidant Health exposed 5.3 TB of sensitive mental health records, including personal details, assessments, and medical information, posing serious privacy risks for patients. |
|[California’s big AI regulation bill is headed to Gavin Newsom.](https://calmatters.org/economy/technology/2024/08/ai-safety-bill-california-legislature/) |A California bill requiring makers of large AI systems to test them for potential harm cleared the Legislature today. It could still face a veto by Gov. Gavin Newsom. |
|[Google search monopoly US case remedies to come by December.](https://www.reuters.com/legal/google-search-monopoly-case-remedies-come-by-december-2024-09-06/) |The U.S. Department of Justice plans to issue an outline by December on what Alphabet's, must do to restore competition after a judge earlier found the company illegally monopolized the market for online search, prosecutors said at a court hearing in Washington on Friday. |
|[Intel reveals first Lunar Lake laptop CPUs: everything you need to know.](https://www.theverge.com/2024/9/3/24233957/intel-lunar-lake-core-ultra-200v-launch) |Previously known as Lunar Lake, Intel has introduced its Core Ultra 200V portfolio, which features competitive integrated GPUs for tiny notebooks, fast CPUs, and enhanced AI capabilities. The CPUs have 32GB RAM capacity, eight CPU cores, integrated memory, and improved efficiency. Prominent producers such as Acer, Asus, Dell, and HP will introduce laptops equipped with these novel CPUs. Reviews to support Intel's assertions are still pending. |
|[OpenAI, Still Haunted by Its Chaotic Past, Is Trying to Grow Up.](https://www.nytimes.com/2024/09/03/technology/openai-chatgpt-revenue.html?unlocked_article_code=1.H04.oIwg.zLvnRVHOKpNH&smid=url-share) |In order to draw in significant investors such as Microsoft, Apple, and Nvidia, OpenAI is reorganizing its management and organization with the goal of reaching a $100 billion valuation. Internal disagreements within the organization regarding its safety procedures and objectives have resulted in a high employee turnover rate, with important researchers leaving to work for competitors such as Anthropic. OpenAI struggles to strike a balance between business goals and moral considerations while developing AI technology, despite increasing income and user base growth. |
|[BP extends use of AI in five-year deal with spy tech firm Palantir.](https://www.theguardian.com/business/article/2024/sep/09/bp-ai-deal-palantir-oil-gas-artificial-intelligence) |Oil and gas company to use artificial intelligence to speed up decision-making by engineers |
|[Google’s second antitrust suit brought by US begins, over online ads.](https://www.theguardian.com/technology/article/2024/sep/09/google-antitrust-lawsuit-online-ads) |DoJ accused tech giant of more monopolistic behavior a month after judge found it illegally cornered online search |
|[What is Apple Intelligence, when is it coming and who will get it?](https://techcrunch.com/2024/09/09/what-is-apple-intelligence-when-is-coming-and-who-will-get-it/) | At WWDC 2024, Apple unveiled Apple Intelligence, a platform designed to integrate AI capabilities into existing applications like Mail, Messages, and Siri. Utilizing large language models, it supports functions such as text summarization and image generation, all aimed at enhancing the user experience. A beta version will be available in the U.S. starting this October, with plans to expand globally in 2025.|
|[New open source AI leader Reflection 70B’s performance questioned, accused of ‘fraud’.](https://venturebeat.com/ai/new-open-source-ai-leader-reflection-70bs-performance-questioned-accused-of-fraud/) |HyperWrite's Reflection 70B, a variant of Meta's Llama 3.1 LLM, is under scrutiny after independent evaluators were unable to reproduce its advertised performance. The problems were traced back to corrupted model weights during the upload to Hugging Face, causing inconsistencies. The AI community is now awaiting further clarifications and updates to better understand the model's true capabilities. |
|[The new Shortwave AI Assistant.](https://www.shortwave.com/blog/new-shortwave-ai-email-assistant/) | Shortwave has substantially enhanced its AI Assistant, equipping it to handle complex, multi-step tasks like advanced searches, calendar lookups, and in-depth email analysis, making it more versatile and powerful in managing user tasks.|
|[OpenAI might use Apple’s TSMC for chips.](https://www.computerworld.com/article/3502761/openai-might-use-apples-tsmc-for-chips.html) | OpenAI could greatly lower operational costs by adopting more efficient chips, which would be particularly beneficial as its user base continues to expand, allowing for better scalability and resource management.|
|[Apple takes direct aim at Microsoft’s Copilot+ PCs in new AI-focused Mac promos.](https://9to5mac.com/2024/09/06/microsoft-copilot-pcs-apple-mac/) |Apple is actively marketing the Mac as the "best AI PC," positioning it as a direct competitor to Microsoft's Copilot+ PCs. This strategic push highlights Apple's focus on integrating AI capabilities into its devices, aiming to challenge Microsoft's AI-driven offerings in the PC market. |
|[GPT-fabricated scientific papers on Google Scholar: Key features, spread, and implications for preempting evidence manipulation.](https://misinforeview.hks.harvard.edu/article/gpt-fabricated-scientific-papers-on-google-scholar-key-features-spread-and-implications-for-preempting-evidence-manipulation/) |Generative AI tools, such as ChatGPT, are increasingly generating fraudulent research papers that are finding their way into databases like Google Scholar, mixing with legitimate studies. These papers, frequently addressing sensitive topics like health and the environment, threaten the integrity of science and public trust. Strengthened oversight and improved filtering mechanisms in academic search engines are crucial to addressing this rising concern. |
|[Apple announces its new A18 and A18 Pro iPhone chips.](https://techcrunch.com/2024/09/09/apple-announces-its-new-a18-iphone-chip/) |At its "Glowtime" event, Apple introduced the A18 and A18 Pro chips, highlighting substantial CPU and GPU upgrades compared to the A16 Bionic. The A18 Pro offers increased memory bandwidth and improved image processing. Both chips come equipped with advanced AI capabilities, with the A18 Pro specifically enhancing on-device model performance and thermal design for a superior gaming experience. |
|[AMD announces unified UDNA GPU architecture — bringing RDNA and CDNA together to take on Nvidia's CUDA ecosystem.](https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem) |At IFA 2024, AMD revealed plans to merge its RDNA and CDNA architectures into a unified UDNA microarchitecture, positioning itself to compete more effectively with Nvidia's CUDA ecosystem. This strategic shift is aimed at simplifying development and strengthening AMD's foothold in the AI and high-performance computing (HPC) markets. The move to UDNA marks a significant transition, with full-scale adoption anticipated after the release of the RDNA 4 generation. |
|[Waymo Giving 100,000 Robotaxi Rides Per Week But Not Making Any Money.](https://futurism.com/the-byte/waymo-not-profitable) | Waymo is now delivering over 100,000 paid autonomous rides per week in San Francisco, Phoenix, and Los Angeles, a figure that has doubled since May. Despite this growth, the company remains unprofitable, with Google’s experimental division facing a $2 billion operating loss. The high costs of vehicles and city mapping, along with ongoing public hesitation, continue to hinder Waymo's journey to profitability.|
|[iOS 18.1 with Apple Intelligence launches in October, more languages rolling out over time.](https://9to5mac.com/2024/09/09/ios-18-1-apple-intelligence-languages-october/) |Apple announced that Apple Intelligence will launch in beta with iOS 18.1 in October, initially available exclusively for US English users. |
|[Bringing generative AI to video with Adobe Firefly Video Model.](https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon) |Adobe's Firefly Video Model introduces AI-driven tools to video editing programs such as Premiere Pro. Set to launch in beta later this year, the model provides editors with improved workflows, enabling them to experiment with creative concepts, fill gaps in timelines, and incorporate new elements into their videos. |
|[Mistral releases Pixtral 12B, its first multimodal model.](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/) |French AI startup Mistral has introduced Pixtral 12B, a multimodal model with 12 billion parameters designed to handle both images and text. The model, accessible through GitHub and Hugging Face, can be fine-tuned and is available under the Apache 2.0 license. This release comes after Mistral secured $645 million in funding, strengthening its role as a key player in Europe's AI industry. |
|[Elon Musk says Tesla has ‘no need’ to license xAI models.](https://techcrunch.com/2024/09/08/elon-musk-says-tesla-has-no-need-to-license-xai-models/) |Elon Musk has refuted claims that Tesla will share revenue with his AI startup xAI in exchange for using its AI models. He explained that while Tesla has gained from xAI engineers' expertise, it doesn't need to license xAI's models. Musk also noted that xAI's large models are incompatible with Tesla's vehicle computers. |
|[Apple is thinking about a rival to Meta Ray-Ban glasses.](https://www.androidauthority.com/apple-non-ar-smart-glasses-meta-glasses-3479479/) | |
|[OpenAI in talks to raise funds at $150B valuation, Bloomberg says.](https://www.tipranks.com/news/the-fly/openai-in-talks-to-raise-funds-at-150b-valuation-bloomberg-says) |Apple might be developing non-AR smart glasses, positioning them as potential competitors to Meta's $299 Ray-Ban glasses, which also lack AR functionality. Meta's glasses come equipped with features like a camera and an AI chatbot. By excluding AR capabilities, Apple's glasses could be more affordable, lighter, and have improved battery life due to reduced complexity. |
|[Meta fed its AI on almost everything you’ve posted publicly since 2007.](https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data) |Unless you’re in the EU, there’s no ability to opt out of AI training settings that keep Facebook or Instagram posts public. |
|[Google is using AI to make fake podcasts from your notes.](https://www.theverge.com/2024/9/11/24242138/google-notebook-llm-ai-fake-podcasts-research) |Google’s NotebookLM app can now generate ‘lively’ audio discussions with two AI hosts about the documents you’ve given it. |
|[Introducing OpenAI o1-preview.](https://openai.com/index/introducing-openai-o1-preview) |OpenAI has launched its latest model, designed to think carefully before responding. It was trained using reasoning processes, allowing it to take time to deliberate before providing an answer. This approach has resulted in superhuman performance in certain areas. Initially, users will be limited to around 30 queries per week, though OpenAI plans to remove this restriction in the near future. |
|[Google is now rolling out Gemini Live to free users on Android.](https://9to5google.com/2024/09/12/gemini-live-android-free-users/) | Google is launching Gemini Live, its conversational AI tool, to all free Android users following a month of early access for advanced users. With this feature, users can interrupt responses to provide new information and receive text transcripts of their conversations. While extensions like Gmail are not yet supported, Gemini Live introduces ten new voice options, with additional features expected to be added soon.|
|[Sergey Brin says he’s working on AI at Google ‘pretty much every day’.](https://techcrunch.com/2024/09/10/sergey-brin-says-hes-working-at-google-pretty-much-every-day-on-ai/) |Google co-founder and ex-Alphabet president Sergey Brin said he’s back working at Google “pretty much every day” because he hasn’t seen anything as exciting as the recent progress in AI — and doesn’t want to miss out. |
|[Amazon starts testing ads in its Rufus chatbot.](https://techcrunch.com/2024/09/11/amazon-starts-testing-ads-in-its-rufus-chatbot/) |Amazon's shopping chatbot, Rufus, will soon incorporate sponsored ads, displaying them based on the user's search queries and the context of their conversations. |


## Resources
|Link|description|
|---|---|
|[OLMoE: Open Mixture-of-Experts Language Models.](https://arxiv.org/abs/2409.02060) |Presents a fully open large language model (LLM) that utilizes a sparse Mixture-of-Experts approach. OLMoE is a 7B parameter model with 1B active parameters per input token. An instruction-tuned version is also available, which reportedly surpasses the performance of Llama-2-13B-Chat and DeepSeekMoE 16B. |
|[Large Language Model-Based Agents for Software Engineering: A Survey.](https://arxiv.org/abs/2409.02977) |A survey paper on large language model (LLM)-based agents in software engineering, offering insights across various areas such as requirements engineering, test generation, and software maintenance. |
|[DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos.](https://arxiv.org/abs/2409.02095) |Researchers were able to produce very accurate depth information without requiring any camera posture or optical flow information by using Stable Diffusion video as a prior model. |
|[SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration.](https://arxiv.org/abs/2409.02231) |Using DPO style data and supervised fine-tuning on open-source language models, LLMs can be trained to produce compounds with intriguing features for potential medicinal development. |
|[Running a LLM on the ESP32.](https://github.com/DaveBben/esp32-llm) |This code demonstrates how to execute a small language model on an Arduino board, showcasing the process of deploying and running AI models on resource-constrained hardware. |
|[DocAI.](https://github.com/madisonmay/docai) |This is another example of effectively leveraging existing models to extract structured information from documents, demonstrating the innovative use of pre-trained AI models to automate data extraction tasks efficiently. |
|[FluxMusic.](https://github.com/feizc/FluxMusic) |Text-to-music generation using a rectified flow transformer involves converting text inputs into musical compositions by utilizing a model that combines transformer architectures with rectified flow techniques. This approach enhances the model's ability to generate coherent and diverse music sequences based on textual descriptions. |
|[iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models.](https://github.com/AuvaLab/itext2kg) | iText2KG is a Python package that leverages large language models to extract entities and relationships from text, progressively constructing consistent knowledge graphs. This tool automates the process of transforming unstructured text into structured knowledge, allowing for the incremental growth of comprehensive knowledge graphs.|
|[Multimodal RAG using ColPali (with Byaldi) and Qwen2-VL.](https://github.com/merveenoyan/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb) | Merve has created a great resource for using language and vision models to improve retrieval.|
|[Awesome-Text2X-Resources.](https://github.com/ALEEEHU/Awesome-Text2X-Resources) | This is an open collection of state-of-the-art (SOTA) and novel Text-to-X methods (where X can represent any output, such as images, audio, or 3D models). The collection includes papers, code, and datasets, aimed at staying up-to-date with the expected surge in research developments in this area over the coming months.|
|[Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task.](https://arxiv.org/abs/2409.04005v1) | The Proxy Token Diffusion Transformer optimizes diffusion transformers by minimizing redundant computations, employing a reduced set of representative tokens for attention processing. This approach enhances efficiency while maintaining model performance.|
|[UniDet3D: Multi-dataset Indoor 3D Object Detection.](https://arxiv.org/abs/2409.04234v1) |UniDet3D is a robust 3D object detection model designed to operate across multiple indoor datasets, delivering strong performance in identifying and detecting objects in three-dimensional spaces. |
|[Starst3r.](https://github.com/phuang1024/Starst3r) |This innovative tool leverages Mast3r along with smart optimizations to efficiently reconstruct 3D scenes from just a few 2D images, offering impressive results with minimal input. |
|[simple_tma.](https://github.com/kuterd/opal_ptx/blob/master/notebooks/simple_tma.ipynb) | Image processing and cropping that can be run on the GPU.|
|[Lexicon3D.](https://yunzeman.github.io/lexicon3d/) |In a recent study comparing seven visual encoding models for 3D scene understanding, researchers found that the most effective model varied based on the specific task. DINOv2 emerged as the top performer overall, while video models excelled in object-level tasks, and diffusion models outperformed others in geometric tasks. Surprisingly, models pretrained on language showed notable limitations in this context. |
|[One-DM:One-Shot Diffusion Mimicker for Handwritten Text Generation.](https://github.com/dailenson/one-dm) |The One-DM model generates handwritten text that can imitate any style using only a single sample as reference. This approach allows for highly personalized handwriting generation with minimal input data. |
|[optillm.](https://github.com/codelion/optillm) | Optillm assists in optimizing prompts by utilizing various well-established research algorithms, including Monte Carlo Tree Search, Z3 solvers, and Self Consistency, to improve performance.|
|[Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation.](https://github.com/valeoai/ttyd) | Researchers tackled the challenge of source-free unsupervised domain adaptation for 3D semantic segmentation by implementing regularization techniques and proposing a new criterion to improve adaptation performance.|
|[Memory-Efficient Optical Flow.](https://arxiv.org/abs/2409.04243v1) | HCVFlow is a newly developed memory-efficient optical flow method designed to address the high computational demands of all-pairs cost volumes in high-resolution images.|
|[Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models.](https://sliders.baulab.info/) | Concept Sliders offer a powerful mechanism for controlling the output of diffusion models. Recent efforts have been made to integrate them with the new Flux suite of models, enhancing their functionality and adaptability.|
|[Minifying HTML for GPT-4o: Remove all the HTML Tags.](https://blancas.io/blog/html-minify-for-llm/) |Converting HTML to plain text can significantly reduce costs with minimal performance loss in GPT-4o for data extraction tasks. Tests on the Mercury Prize dataset demonstrated that GPT-4o performs effectively even without the HTML structure, and GPT-4o mini offers a cost-efficient solution for handling unstructured questions. For structured extraction tasks, it's advisable to test both versions to find the right balance between cost and accuracy. |
|[Prompt2Fashion: An automatically generated fashion dataset.](https://arxiv.org/abs/2409.06442v1) |This dataset, created with large language models, curates outfit recommendations for various occasions, styles, and body types, providing high-quality and relevant suggestions. |
|[Sources of Uncertainty in 3D Scene Reconstruction.](https://arxiv.org/abs/2409.06407v1) |Researchers are improving 3D scene reconstruction techniques such as Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (GS) by incorporating uncertainty estimation methods. Although these approaches produce high-quality renders, they face challenges in addressing uncertainties caused by noise, occlusions, and camera inaccuracies. |
|[🦙🎧 LLaMA-Omni: Seamless Speech Interaction with Large Language Models.](https://github.com/ictnlp/LLaMA-Omni) | Llama Omni is a speech input-output model built on Llama 3.1 8B, designed to operate with extremely low latency while maintaining high-quality responses.|
|[AWS AI Stack.](https://github.com/serverless/aws-ai-stack) | This ready-to-use, full-stack boilerplate project is designed for building serverless AI applications on AWS. It is ideal for developers looking for a reliable AWS foundation for AI apps and seamless access to powerful LLM models through Bedrock, while ensuring your app's data remains separate from model providers.|
|[Internet of Agents.](https://github.com/openbmb/ioa) | The Internet of Agents (IoA) is a novel framework aimed at enhancing multi-agent collaboration by enabling more efficient integration of diverse third-party agents.|
|[ell: The Language Model Programming Library.](https://docs.ell.so/) |Ell is a newly released package developed by a former OpenAI scientist, designed to manage prompts as code, streamlining the process of working with prompts in AI applications. |
|[EMO-Disentanger.](https://github.com/yuer867/emo-disentanger) |This research employs a two-stage model to separate and analyze emotive elements in piano music generation, enabling more expressive and nuanced performances. |
|[Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown.](https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/) |Jina has unveiled two cutting-edge models capable of transforming noisy HTML into clean, structured Markdown, optimized for training and reasoning tasks. |
|[Agent Workflow Memory.](https://github.com/zorazrw/agent-workflow-memory) | Agent Workflow Memory (AWM) is a technique that enables language model-based agents to learn and retain reusable task workflows from previous experiences, allowing them to effectively manage complex, long-horizon tasks.|
|[Hi3D-Official.](https://github.com/yanghb22-fdu/hi3d-official) | Hi3D is a novel model designed to improve the generation of multi-view consistent, high-resolution 3D images from a single input. By using a video diffusion technique, it addresses the limitations of traditional 2D methods that lack 3D awareness, leveraging temporal consistency from video models to enhance geometric coherence across different views.|
|[Fine Tuning Llama 3.1 405B with Axolotl on a Lambda 1-Click Cluster.](https://axolotlai.substack.com/p/fine-tuning-llama-31b-waxolotl-on) |Axolotal AI has collaborated with Lambda Labs to demonstrate how their one-click cluster can be used to fine-tune the Llama 3.1 405B model. Although the process requires 64 GPUs, the new tools make it possible with minimal infrastructure setup, streamlining the process significantly. |
|[super-benchmark.](https://github.com/allenai/super-benchmark) |SUPER is a newly introduced benchmark aimed at evaluating how effectively large language models (LLMs) can replicate tasks sourced from research repositories. |
|[Using GPT-4o for web scraping.](https://blancas.io/blog/ai-web-scraper/) |An AI-powered web scraper, utilizing OpenAI's GPT-4o, is designed to extract structured data from HTML tables. While it performs well on simple tables, its results are mixed when dealing with more complex tables, such as those with merged rows or intricate structures. |



## Perspectives
|Link|description|
|---|---|
|[‘If journalism is going up in smoke, I might as well get high off the fumes’: confessions of a chatbot helper.](https://www.theguardian.com/technology/article/2024/sep/07/if-journalism-is-going-up-in-smoke-i-might-as-well-get-high-off-the-fumes-confessions-of-a-chatbot-helper) | Journalists and other writers are employed to improve the quality of chatbot replies. The irony of working for an industry that may well make their craft redundant is not lost on them|
|[Will AI make us overconfident?](https://tedunderwood.com/2024/08/31/will-ai-make-us-overconfident/) | Students are increasingly turning to AI tools like ChatGPT to tackle complex research challenges, surprising educators with their swift advancements. AI-powered development tools, particularly in coding, greatly enhance both ambition and productivity, though they also introduce risks of overconfidence and mistakes. Despite occasional inaccuracies, AI offers valuable interactive starting points for difficult tasks, potentially fostering more active learning and encouraging exploration across disciplines.|
|[LLMs struggle to explain themselves.](https://www.jonathanychan.com/blog/llms-struggle-to-explain-themselves/) | An interactive demo was employed to evaluate large language models' (LLMs) ability to recognize and explain number sequences produced by random programs. The findings revealed that although LLMs often correctly identified the sequences, their explanations of the underlying patterns were frequently inaccurate. This underscores the limitations of LLMs' reasoning capabilities, despite their strong performance on standardized tests.|
|[No more free pass: Regulation starts to crack down on social media platforms.](https://english.elpais.com/technology/2024-09-09/no-more-free-pass-regulation-starts-to-crack-down-on-social-media-platforms.html) | The arrest of Telegram’s CEO in France and the closure of X in Brazil are two of the latest signs that times are changing, with networks beginning to be held more accountable|
|[Here’s how 7 news audience directors are thinking about Google’s AI Overviews.](https://www.niemanlab.org/2024/08/how-7-news-audience-directors-are-thinking-about-responding-to-googles-ai-overviews/) |Google's AI Overviews, which use the Gemini language model, received significant criticism for inaccuracies and potentially harmful recommendations following their launch in the U.S. Despite the negative feedback, Google extended the feature to six additional countries, sparking concerns among publishers about decreased web traffic and distorted content. AI experts and SEO specialists stress the importance of transparency and improved citation methods to preserve trust and ensure consistent traffic. |
|[Diffusion is spectral autoregression.](https://sander.ai/2024/09/02/spectral-autoregression.html) |Diffusion models and autoregressive models share a fundamental similarity, as both rely on iterative refinement processes. The author demonstrates, using Fourier transform techniques, that diffusion models function similarly to approximate autoregression in the frequency domain, especially for visual data. This insight suggests promising pathways for unifying generative modeling approaches across various data types. |
|[Why We Fear Diverse Intelligence Like AI.](https://www.noemamag.com/why-we-fear-diverse-intelligence-like-ai/) |The emergence of AI and various forms of intelligence is blurring traditional distinctions between "real beings" and machines. Rather than centering discussions only on AI, it's important to recognize and ethically interact with a broad range of cognitive systems, including bioengineered, robotic, and hybrid entities. By broadening our understanding of intelligence and fostering compassion, we can better navigate the ethical challenges posed by these rapidly evolving technologies. |
|[SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance.](https://shuchangye-bib.github.io/websites/SGSeg/sgseg.html) |SGSeg is a segmentation framework for chest X-rays that incorporates language guidance during training but allows for text-free inference during the prediction phase. |
|[Are novelists who worry about the rise of AI really ‘classist and ableist’?](https://www.theguardian.com/commentisfree/article/2024/sep/11/are-novelists-who-worry-about-the-rise-of-ai-really-classist-and-ableist) |An international writing organisation appeared to greenlight the use of AI, prompting anger, the resignation of four board members and an entire creative community to ask: ‘What?!’ |
|[AI Chatbots Have a Political Bias That Could Unknowingly Influence Society.](https://www.sciencealert.com/ai-chatbots-have-a-political-bias-that-could-unknowingly-influence-society?utm_source=reddit_post) | A new study has uncovered strong evidence that we can now add political bias to that list, further demonstrating the potential of the emerging technology to unwittingly and perhaps even nefariously influence society's values and attitudes.|
|[How influencers and algorithms mobilize propaganda — and distort reality.](https://www.nature.com/articles/d41586-024-02917-1) |The engagement-fuelled logic of social media has bequeathed us a world in which what’s trending is a yardstick for what’s true. |
|[Artificial intelligence can help to make animal research redundant.](https://www.nature.com/articles/d41586-024-02894-5) |One alternative in its early stages is artificial intelligence (AI), whereby generative adversarial networks produce animal data. But there remains a disconnect between AI-generated animal data and human safety data. Computer models that simulate complex human physiological processes could close this gap, with AI used to analyse the resulting data sets. |
|[Wikipedia is facing an existential crisis. Can gen Z save it?](https://www.theguardian.com/commentisfree/2024/sep/12/wikipedia-generation-z-young-editors-chatbots) |The world’s most important knowledge platform needs young editors to rescue it from chatbots – and its own tired practices |
|[AI-Generated Junk Science Is Flooding Google Scholar, Study Claims.](https://www.newsweek.com/ai-generated-junks-science-floods-google-scholar-study-claims-1950703) |Anew study claims to have uncovered a disturbing trend in the world of academic research: AI tools like ChatGPT being used to produce fake scientific papers that are infiltrating Google Scholar, one of the most widely used academic search engines. |
|[Will the "AI Scientist" Bring Anything to Science?](https://spectrum.ieee.org/ai-for-science-2) |Researchers have created an AI tool capable of automating scientific workflows, from generating hypotheses to executing experiments and drafting research papers. While its accuracy and coherence require further development, critics warn that AI's role in simulations, such as in quantum computing and materials science, may lead to narrower research questions and less impactful findings. Supporters, however, see potential in using this AI to streamline early stages of research, helping scientists conceptualize and define their projects more efficiently. |
|[Is AI Quietly Sabotaging Itself—And The Internet?](https://www.forbes.com/sites/torconstantino/2024/08/26/is-ai-quietly-killing-itself-and-the-internet/?ss=ai) |Amid the growth of AI content online, a group of researchers at Cambridge and Oxford universities set out to see what happens when generative AI tools query content produced by AI. What they found was alarming. |


# ML news: Week 2 - 8 September

## Research
|Link|description|
|---|---|
|[Diffusion Models Are Real-Time Game Engines.](https://arxiv.org/abs/2408.14837) | a two-phase training process involving an RL agent to learn and a diffusion model to generate frames; it can interactively simulate DOOM over at 20 frames per second on a single TPU. A game engine driven by a diffusion model allows real-time interaction with complex environments over long trajectories.|
|[Agentic Retrieval-Augmented Generation for Time Series Analysis.](https://arxiv.org/abs/2408.14484) | suggests an agentic RAG framework for time series analysis. It makes use of a multi-agent architecture in which an agent directs specialized sub-agents to carry out time-series tasks. These sub-agents can retrieve pertinent prompts that contain information about past patterns and trends, which helps to improve predictions on new data. The sub-agents use tuned small language models to accomplish these tasks.|
|[Persuasion Games using Large Language Models.](https://arxiv.org/abs/2408.15879) |asserts that the persuasive efficacy of LLMs can be increased by using a multi-agent framework, in which the main agent conducts persuasive dialogue while supporting agents handle crucial functions like information retrieval and response analysis. The study finds that LLMs are capable of influencing users' perspectives and convincing them to make a purchase decision; for example, sales agents can influence user perspectives in a 71% positive way. |
|[Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling.](https://arxiv.org/abs/2408.16737) | discovers that synthetic data produced by weaker + less costly (WC) models is superior to data produced by stronger but more expensive models for the purpose of fine-tuning models; generally, the results imply that WC models might be a compute-optimal method for training sophisticated LLM reasoners.|
|[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model.](https://www.arxiv.org/abs/2408.11039) |demonstrates that it is possible to scale from 7B parameter models to 2T multi-modal tokens that can compete in performance with similar scale diffusion and language models. It also presents a training recipe to train multi-modal models over discrete and continuous data; it combines next token prediction with diffusion to train transformer models over mixed-modality sequences. |
|[ReMamba: Equip Mamba with Effective Long-Sequence Modeling.](https://arxiv.org/abs/2408.15496) |examines the long-context capacities and efficiency of Mamba models; the RNN-like nature of Mamba is the cause of the long-context deficiencies; it does this by compressing data using the following method: achieves a 3.2 improvement over the baseline on LongBench and 1.6 improvement on L-Eval; the strategy appears to also apply to Mamba 2. the top-k hidden states during the first forward pass and uses Mamba's selective mechanism to incorporate them into the state space during the second forward pass. |
|[Text2SQL is Not Enough: Unifying AI and Databases with TAG.](https://arxiv.org/abs/2408.14717v1) | develops a benchmark and discovers that standard methods only answer 20 percent of natural language queries correctly. It suggests Table-Augmented Generation (TAG), a unified framework for responding to natural language queries over databases. It represents a wider range of unexplored interactions between LLMs and databases. |
|[Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts.](https://arxiv.org/abs/2408.15664) | Sparsifying the computation is aided by routing tokens to MoE experts. But it can be hard to learn that routing. Usually, there is a complex loss structure. This research presents an innovative solution to this issue, leading to a significant increase in training stability and expert balancing.|
|[Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach.](https://arxiv.org/abs/2408.16343v1) |A multimodal classification approach intended to enhance the early detection of Alzheimer's disease is presented in this work. |
|[Targeted Cause Discovery with Data-Driven Learning.](https://arxiv.org/abs/2408.16218v1) |A sophisticated machine learning technique has been created by researchers to determine a target's direct and indirect causal variables within a system. |
|[Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba Training.](https://arxiv.org/abs/2408.17081v1) |In order to prevent overfitting in Vision Mamba models and enable them to scale up to 300M parameters while still performing competitively with Vision Transformers (ViTs), this research presents a stochastic layer-wise shuffle regularization strategy. |
|[Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control.](https://arxiv.org/abs/2405.05852v1) |Stable Control Representations are a tool that researchers are using to help embodied AI machines interpret scenes more precisely. These representations capture detailed visuospatial information required for challenging tasks by utilizing pre-trained text-to-image diffusion models. |
|[AI generates covertly racist decisions about people based on their dialect.](https://www.nature.com/articles/s41586-024-07856-5) |language models perpetuate covert racism through dialect prejudice, specifically against African American English (AAE), leading to negative stereotypes and harmful consequences, while overt stereotypes about African Americans are more positive, and current bias mitigation practices may worsen this issue. |
|[Latent Distillation for Continual Object Detection at the Edge.](https://arxiv.org/abs/2409.01872v1) |A unique Continual Learning technique for object detection that overcomes memory and computational limitations on edge devices is called latent distillation. |
|[Masked Mixers for Language Generation and Retrieval.](https://arxiv.org/abs/2409.01482v1) |Masked mixers are a unique architecture designed to enhance input representation in language models by substituting masked convolutions for self-attention. |
|[Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology.](https://arxiv.org/abs/2404.10242v1) | Using masked autoencoders and self-supervised learning, researchers have created a novel technique that greatly enhances the processing of large-scale microscope pictures.|
|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](https://github.com/yixuantt/poolingandattn) |This work compares alternative pooling and attention strategies while examining multiple designs for LLM-based embedding models. |
|[AlphaProteo generates novel proteins for biology and health research.](https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/) |New AI system designs proteins that successfully bind to target molecules, with potential for advancing drug design, disease understanding and more. |

















































