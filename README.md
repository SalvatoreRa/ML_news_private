# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

# ON WORKING

# ML news: 

## Research
|Link|description|
|---|---|
|[The Geometry of Concepts: Sparse Autoencoder Feature Structure.](https://arxiv.org/abs/2410.19750) |This study investigates the geometric structure of concept representations in sparse autoencoders (SAEs) across three scales: (1) atomic-level parallelogram patterns among related concepts (e.g., man:woman::king:queen), (2) brain-like functional "lobes" dedicated to different knowledge types such as math or code, and (3) galaxy-level eigenvalue distributions, revealing a specialized structure within the middle layers of the model. |
|[Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics.](https://arxiv.org/abs/2410.21272) | This approach employs causal analysis to identify neurons that reveal an LLM's behavior when performing basic arithmetic logic. It discovers and theorizes that a combination of heuristic neurons serves as the mechanism for generating accurate arithmetic answers, with the unordered blend of various heuristic types accounting for most of the model's accuracy on arithmetic prompts.|
|[Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA.](https://arxiv.org/abs/2410.20672) | The Relaxed Recursive Transformer introduces a novel method for reducing LLM size by sharing parameters across layers without sacrificing performance. Initialized from standard pretrained Transformers, it employs a single block of unique layers repeated multiple times in a loop, adding flexibility through depth-wise low-rank adaptation (LoRA) modules. This approach demonstrates potential for significant (2-3√ó) improvements in inference throughput.|
|[What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective.](https://github.com/mingliiii/layer_gradient) | This project examines how varying "thinking" styles‚Äîfast (concise) versus slow (detailed, such as chain-of-thought reasoning)‚Äîaffect layer-wise gradients and stability in LLMs.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[Elon Musk‚Äôs ‚Äòelection integrity community‚Äô on X is full of baseless claims.](https://www.theguardian.com/technology/2024/oct/31/elon-musk-election-integrity-community-misinformation-disinformation) | Feed is rife with posts of individuals deemed suspicious and calls for doxxing with little evidence provided of fault|
|[Microsoft sails as AI boom fuels double-digit growth in cloud business.](https://www.theguardian.com/technology/2024/oct/30/microsoft-earnings-increase-ai) |Revenue from Azure cloud business increased by 22% as company focuses attention on artificial intelligence |
|[Apple reports robust demand for iPhone 16 even as overall sales in China slow.](https://www.theguardian.com/technology/2024/oct/31/apple-quarterly-earnings-iphone-16-china) | Company reports $94.9bn in revenue, slightly beating Wall Street projections in first look at demand for its new phone|
|[Distinguishing Ignorance from Error in LLM Hallucinations.](https://arxiv.org/abs/2410.22071) |This report describes efforts to replicate the capabilities of OpenAI's o1 model, introducing a journey learning technique that promotes a comprehensive exploration process rather than shortcut-based learning. This approach includes trial and error, reflection, and backtracking. With just 327 training samples, the journey learning technique outperformed shortcut learning by 8.0% on the MATH dataset. |
|[Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models.](https://arxiv.org/abs/2410.19385) |This study evaluates various prompting strategies and frameworks to minimize hallucinations in LLMs, finding that simpler prompting techniques outperform more complex approaches. It also reports that LLM agents show higher hallucination rates due to the increased complexity involved in using tools. |
|[Introducing the First AMD 1B Language Models: AMD OLMo.](https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html) |AMD utilized the OLMo codebase to train and release a language model on its accelerators. The OLMo (Open Language Model) project, developed by the Allen Institute for AI (AI2), provides an open-source framework for training and using state-of-the-art language models.  |
|[OpenAI will start using AMD chips and could make its own AI hardware in 2026.](https://www.theverge.com/2024/10/29/24282843/openai-custom-hardware-amd-nvidia-ai-chips) | Reuters reports an updated hardware strategy to run ChatGPT and OpenAI‚Äôs other projects involves using AMD chips via Microsoft Azure in addition to Nvidia.|
|[Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?](https://arxiv.org/abs/2410.23856v1) | This study addresses a specific challenge in LLMs: evaluating how effectively they process reasoning prompts that include irrelevant or incorrect rationale snippets.|
|[What is Wrong with Perplexity for Long-context Language Modeling?](https://arxiv.org/abs/2410.23771v1) | This study uncovers a significant limitation of using perplexity (PPL) to assess LLMs' long-context abilities, as PPL averages across all tokens, overlooking critical ones necessary for interpreting extended inputs. To address this, the authors propose LongPPL, a metric that emphasizes these essential tokens, providing a more accurate measure of long-context performance.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[AFlow: Automating Agentic Workflow Generation.](https://arxiv.org/abs/2410.10762) |A novel framework for automating agentic workflow generation, AFlow, reframes workflow optimization as a search problem over code-based workflows, where nodes invoking LLMs are linked by edges. It efficiently navigates the search space using a modified MCTS, refining workflows through code adjustments, tree-structured experience, and execution feedback. Tests on six benchmark datasets show AFlow‚Äôs effectiveness, with a 5.7% improvement over manual methods and a 19.5% boost over other automated approaches. AFlow also allows smaller models to outperform GPT-4 on specific tasks, requiring only 4.55% of its inference cost. |
|[O1 Replication Journey: A Strategic Progress Report -- Part 1.](https://arxiv.org/abs/2410.18982) | This report describes efforts to replicate the capabilities of OpenAI's o1 model, introducing a journey learning technique that promotes a comprehensive exploration process rather than shortcut-based learning. This approach includes trial and error, reflection, and backtracking. With just 327 training samples, the journey learning technique outperformed shortcut learning by 8.0% on the MATH dataset.|
|[Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications.](https://arxiv.org/abs/2410.21943) |This work offers insights on effectively integrating multimodal models into Retrieval-Augmented Generation (RAG) systems for the industrial sector. It also delves into evaluating these systems, utilizing LLM-as-a-Judge for comprehensive assessment. |
|[You won't believe this.](https://www.science.org/content/article/can-people-be-inoculated-against-misinformation) |Researchers are trying to ‚Äúinoculate‚Äù people against misinformation by giving them small doses ahead of time |
|[3D Scene Reconstruction Without Camera Pose.](https://noposplat.github.io/) | NoPoSplat is a feed-forward model capable of reconstructing 3D scenes from sparse, multi-view images without requiring precise camera poses.|
|[ImOV3D: Learning Open Vocabulary Point Clouds 3D Object Detection from Only 2D Images.](https://github.com/yangtiming/imov3d) |ImOV3D is a framework that enhances open-vocabulary 3D object detection (OV-3Det) by utilizing 2D images to address the limited availability of 3D annotations. |
|[Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning.](https://pr-ryan.github.io/DEMO-project/) | DEMO is a framework that divides text and conditioning into content and motion elements. By employing separate encoders and conditioning for static content and dynamic motion, DEMO improves its ability to interpret and generate motion based on text prompts.|
|[Project Sid.](https://threadreaderapp.com/thread/1852397383939960926.html) |Project Sid demonstrates civilizational progress, specialization, governance, and the creation and dissemination of memes and religion. These developments are enabled by Altera's innovative cognitive architecture, PIANO. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |


## Perspectives
|Link|description|
|---|---|
|[The chatbot optimisation game: can we trust AI web searches?](https://www.theguardian.com/technology/2024/nov/03/the-chatbot-optimisation-game-can-we-trust-ai-web-searches) |Google and its rivals are increasingly employing AI-generated summaries, but research indicates their results are far from authoritative and open to manipulation |
|[Addicted to love: how dating apps ‚Äòexploit‚Äô their users.](https://www.theguardian.com/lifeandstyle/2024/nov/03/addicted-to-love-how-dating-apps-exploit-their-users) | Online services that promise to find people romantic matches have been likened to gambling products designed to keep customers hooked|
|[Concerned about your data use? Here is the carbon footprint of an average day of emails, WhatsApps and more.](https://www.theguardian.com/environment/2024/oct/31/concerned-about-your-data-use-here-is-the-carbon-footprint-of-an-average-day-of-emails-whatsapps-and-more) | Vast datacentres are being built worldwide, amid growing concerns about the environmental costs. So should we all be considering a data diet ‚Äì if not complete digital sobriety?|
|[A field‚Äôs dilemmas.](https://www.science.org/content/article/five-biggest-challenges-facing-misinformation-researchers) |Misinformation research has exploded. But scientists are still grappling with fundamental challenges |
|[We're forking Flutter. This is why.](https://flutterfoundation.dev/blog/posts/we-are-forking-flutter-this-is-why/) |Google's strategic shift towards AI has led to a deprioritization of Flutter's desktop platforms, resulting in a labor shortage for this previously fast-growing UI toolkit. In response, a fork named Flock is being developed to incorporate essential bug fixes and features that the Flutter team is unable to address, aiming to accelerate Flutter's growth through community involvement. Flock plans to enhance contribution processes and streamline PR reviews, bridging the gap in support and development pace left by the main Flutter team.  |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |


# ML news: Week 28 October - 3 November

## Research

|Link|description|
|---|---|
|[A Theoretical Understanding of Chain-of-Thought.](https://arxiv.org/abs/2410.16540) |reveals that incorporating both correct and incorrect reasoning paths in demonstrations enhances the accuracy of intermediate steps and Chain-of-Thought (CoT) processes. The new approach, called Coherent CoT, substantially boosts performance across multiple benchmarks. Specifically, Gemini Pro shows a 6.60% improvement on the Tracking Shuffled Objects dataset (rising from 58.20% to 64.80%), while DeepSeek 67B achieves a 6.17% increase on the Penguins in a Table dataset (from 73.97% to 80.14%). |
|[LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering.](https://arxiv.org/abs/2410.18050) |improves RAG's comprehension of long-context knowledge, incorporating global insights and factual specifics. It features a hybrid retriever, an LLM-enhanced information extractor, a Chain-of-Thought (CoT) guided filter, and an LLM-augmented generator. These core components empower the RAG system to extract global long-context information and accurately capture factual details. LongRAG demonstrates superior performance, surpassing long-context LLMs by 6.94%, advanced RAG by 6.16%, and Vanilla RAG by 17.25%. |
|[Evaluating feature steering: A case study in mitigating social biases.](https://www.anthropic.com/research/evaluating-feature-steering) |examines feature steering in LLMs through an experiment that adjusts various features to observe shifts in model outputs, specifically focusing on 29 features related to social biases to determine if feature steering can reduce these biases. Findings reveal that while feature steering can sometimes cause unintended effects, incorporating a neutrality feature effectively reduces social biases across 9 social dimensions without compromising text quality. |
|[Large Language Models Reflect the Ideology of their Creators.](https://arxiv.org/abs/2410.18417) |reveals that LLMs display varied ideological perspectives, often mirroring the worldview of their creators. It observes consistent normative differences in responses when the same LLM operates in Chinese versus English and highlights normative disagreements between Western and non-Western LLMs regarding prominent figures in geopolitical conflicts. |
|[Scalable watermarking for identifying large language model outputs.](https://www.nature.com/articles/s41586-024-08025-4) |introduces SynthID-Text, a text-watermarking approach designed to maintain text quality in LLM outputs, achieve high detection accuracy, and reduce latency. It incorporates watermarking through speculative sampling, using a final score pattern for model word choices alongside adjusted probability scores. The authors evaluate the method's feasibility and scalability by analyzing feedback on nearly 10 million Gemini responses. |
|[A Comparative Study on Reasoning Patterns of OpenAI's o1 Model.](https://arxiv.org/abs/2410.13639) |outperformed other test-time compute methods across most datasets. The authors note that the primary reasoning patterns in o1 are divide and conquer and self-refinement, with the model adapting its reasoning strategy to specific tasks. For commonsense reasoning, o1 frequently employs context identification and focuses on constraints, while for math and coding tasks, it predominantly utilizes method reuse and divide and conquer approaches. |
|[Sparse Crosscoders for Cross-Layer Features and Model Diffing.](https://transformer-circuits.pub/2024/crosscoders/index.html) |Crosscoders are an advanced form of sparse autoencoders designed to enhance the understanding of language models' internal mechanisms. |
|[Distill Visual Chart Reasoning Ability
from LLMs to MLLMs.](https://github.com/hewei2001/reachqa) | Code-as-Intermediary Translation (CIT) is an innovative technique aimed at improving visual reasoning in multimodal language models (MLLMs) by leveraging code to convert chart visuals into textual descriptions.|
|[Probabilistic Language-Image Pre-Training.](https://arxiv.org/abs/2410.18857v1) |Probabilistic Language-Image Pre-training (ProLIP) is a vision-language model (VLM) designed to learn probabilistically from image-text pairs. Unlike traditional models that rely on a strict one-to-one correspondence, ProLIP captures the complex many-to-many relationships inherent in real-world data. |
|[A faster, better way to train general-purpose robots.](https://news.mit.edu/2024/training-general-purpose-robots-faster-better-1028) | MIT researchers have developed Heterogeneous Pretrained Transformers (HPT), a novel model architecture inspired by large language models, designed to train adaptable robots by utilizing data from multiple domains and modalities.|
|[A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs.](https://arxiv.org/abs/2410.18779) |In this work, DeepMind demonstrates how a small language model can be used to provide soft supervision labels and identify informative or challenging data points for pretraining, significantly accelerating the pretraining process. |
|[NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction.](https://arxiv.org/abs/2410.19452v1) |The NeuroClips framework introduces advancements in reconstructing continuous videos from fMRI brain scans by decoding both high-level semantic information and fine-grained perceptual details. |
|[Machine-guided design of cell-type-targeting cis-regulatory elements.](https://www.nature.com/articles/s41586-024-08070-z) |A generalizable framework to prospectively engineer cis-regulatory elements from massively parallel reporter assay models can be used to write fit-for-purpose regulatory code. |


## News
|Link|description|
|---|---|
|[Keir Starmer says media firms should have control of output used in AI.](https://www.theguardian.com/media/2024/oct/28/keir-starmer-says-media-firms-should-have-control-of-output-used-in-ai) | PM says content creators must be paid and vows to ensure technology ‚Äòdoes not begin to chip away‚Äô at press freedoms|
|[Waymo raises $5.6B.](https://waymo.com/blog/2024/10/investing-to-bring-the-waymo-driver-to-more-riders/) | Waymo's driverless taxi service has gained significant popularity. The company has secured additional funding to extend its reach beyond the current cities and millions of miles it already covers.|
|[Meta Introduces Spirit LM open source model that combines text and speech inputs/outputs.](https://venturebeat.com/ai/meta-introduces-spirit-lm-open-source-model-that-combines-text-and-speech-inputs-outputs/?utm_source=tldrai) | Just in time for Halloween 2024, Meta has unveiled Meta Spirit LM, the company‚Äôs first open-source multimodal language model capable of seamlessly integrating text and speech inputs and outputs.|
|[IBM debuts open source Granite 3.0 LLMs for enterprise AI.](https://venturebeat.com/ai/ibm-debuts-open-source-granite-3-0-llms-for-enterprise-ai/) |IBM is enhancing its enterprise AI suite with Granite 3.0 LLMs, prioritizing open-source options and optimized performance. Available across various platforms, these models come with built-in safety features and are customized for diverse enterprise applications. IBM highlights the significance of true open-source licensing with Apache 2.0, enabling flexible adoption and fostering enterprise-driven innovation. |
|[Microsoft introduces ‚ÄòAI employees‚Äô that can handle client queries.](https://www.theguardian.com/technology/2024/oct/21/microsoft-launches-ai-employees-that-can-perform-some-business-tasks) |US company gives customers the ability to build own virtual agents as well as releasing 10 off-the-shelf bots |
|[Microsoft Excel‚Äôs bloopers reel: 40 years of spreadsheet errors.](https://www.theguardian.com/technology/2024/oct/28/microsoft-excels-bloopers-reel-40-years-of-spreadsheet-errors) | As the software used by millions around the world celebrates its birthday, here are some of the low points|
|[Google Expands Voice Technology Support to 15 More African Languages .](https://blog.google/around-the-globe/google-africa/africas-digital-decade/) | Google has expanded voice recognition support to include 15 more African languages across its platforms, such as Voice Search, Gboard talk-to-type, and Translate dictation. This enhancement enables an estimated 300 million additional Africans to engage with digital content in their native languages.|
|[Cohere releases state-of-the-art multimodal AI search model.](https://cohere.com/blog/multimodal-embed-3) |Cohere has unveiled that its Embed 3 AI model is now multimodal, allowing for rapid and precise search across essential enterprise image data sources such as graphs, charts, product catalogs, and design files. This enhancement makes Embed 3 the most broadly capable multimodal embedding model available today. |
|[Bringing developer choice to Copilot with Anthropic‚Äôs Claude 3.5 Sonnet, Google‚Äôs Gemini 1.5 Pro, and OpenAI‚Äôs o1-preview.](https://github.blog/news-insights/product-news/bringing-developer-choice-to-copilot/) | You can now access models like Claude, Gemini, and o1, among others, through GitHub Copilot.|
|[Apple releases first batch of Apple Intelligence features, debuts new iMac.](https://siliconangle.com/2024/10/28/apple-releases-first-batch-apple-intelligence-features-debuts-new-imac/) |Apple introduced new AI features, branded as Apple Intelligence, on its latest devices, focusing on text processing and photo editing capabilities. The updated iMac now runs on the M4 chip, which includes a Neural Engine that delivers three times the AI performance of previous models. Upcoming AI updates aim to improve Siri's capabilities and incorporate ChatGPT for handling more advanced queries. |
|[How Advex creates synthetic data to improve machine vision for manufacturers.](https://techcrunch.com/2024/10/28/how-advex-creates-synthetic-data-to-improve-machine-vision-for-manufacturers/) |Advex AI addresses data shortages in AI training by leveraging generative AI to create synthetic images tailored for computer vision systems. |
|[Coframe raises $9 million for websites that optimize themselves using AI.](https://www.reuters.com/technology/artificial-intelligence/coframe-raises-9-million-websites-that-optimize-themselves-using-ai-2024-10-29/) |AI startup Coframe has raised $9.3 million in seed funding to further develop its platform, which leverages generative AI to optimize websites and deliver personalized marketing experiences. |
|[Google unveils invisible ‚Äòwatermark‚Äô for AI-generated text.](https://www.nature.com/articles/d41586-024-03462-7) | Real-world demonstration in chatbot responses could encourage other firms to label material produced by AI.|
|[Reddit shares soar after company turns first-ever profit.](https://www.theguardian.com/technology/2024/oct/30/reddit-stock) |Monthly users rose by nearly half thanks to AI translation feature, and deals for AI training with Google and OpenAI boosted revenue |
|[Google parent Alphabet sees double-digit growth as AI bets boost cloud business.](https://www.theguardian.com/technology/2024/oct/29/alphabet-google-earnings-report) |Analysts expected 12% year-on-year revenue gains, but company reports 15%, buoyed by performance in ads and cloud services |
|[EU events on curbing big tech ‚Äòdistorted‚Äô by attenders with industry links.](https://www.theguardian.com/world/2024/oct/29/eu-events-curbing-big-tech-distorted-attenders-hidden-industry-links) |Campaigners say 21% of people at workshops did not disclose on their applications relationships with firms being discussed |
|[Indonesia blocks Apple iPhone 16 sales over lack of investment.](https://www.theguardian.com/technology/2024/oct/28/indonesia-apple-iphone-16-ban) |Marketing and sale of model prohibited after tech giant fails to meet rule 40% of phones be made from local parts |
|[25% of Smartphone Owners Don't Want AI as Apple Intelligence Debuts.](https://www.cnet.com/tech/mobile/25-of-smartphone-owners-dont-want-ai-as-apple-intelligence-debuts/) | What's a bigger priority? Longer battery life, according to a new CNET survey.|
|[Google preps ‚ÄòJarvis‚Äô AI agent that works in Chrome.](https://9to5google.com/2024/10/26/google-jarvis-agent-chrome) |Google's Project Jarvis, powered by Gemini 2.0, aims to automate web-based tasks in Chrome by using AI agents capable of reasoning and planning. |
|[OpenAI‚Äôs Whisper transcription tool has hallucination issues, researchers say.](https://techcrunch.com/2024/10/26/openais-whisper-transcription-tool-has-hallucination-issues-researchers-say/) | OpenAI's Whisper, an AI transcription tool, has been found to produce hallucinations‚Äîfabricated text not present in the original audio‚Äîeven in medical settings. Despite OpenAI's advisories against using Whisper in high-risk domains, over 30,000 medical professionals across 40 health systems have adopted it for transcribing patient consultations|
|[Forerunner K2 humanoid robot can carry 33 lb in each dexterous hand.](https://newatlas.com/ai-humanoids/kepler-forerunner-k2-humanoid-robot/) |Kepler has introduced the Forerunner K2, a humanoid robot featuring advanced AI, upgraded hardware, and enhanced vision and navigation systems for improved real-time interaction. |
|[Introducing ChatGPT search.](https://openai.com/index/introducing-chatgpt-search/) |ChatGPT now offers an improved web search capability, providing quick, current answers with links to relevant sources‚Äîanswers you'd typically seek through a search engine. This feature combines the ease of a natural language interface with access to real-time information, such as sports scores, news, stock prices, and more. |
|[Advancing embodied AI through progress in touch perception, dexterity, and human-robot interaction.](https://ai.meta.com/blog/fair-robotics-open-source/) | This work features several components, including vision-based tactical sensing, innovative hardware touch sensors, and noteworthy strategic partnerships within robotics.|
|[Elon Musk‚Äôs xAI adds image understanding capabilities to Grok.](https://techcrunch.com/2024/10/28/xai-adds-image-understanding-capabilities-to-grok/) | This means that paid users on his social platform X, who have access to the AI chatbot, can upload an image and ask the AI questions about it.|
|[OpenAI CFO Says 75% of Its Revenue Comes From Paying Consumers.](https://finance.yahoo.com/news/openai-cfo-says-75-revenue-163713534.html) |OpenAI generates the vast majority of its revenue from consumers who pay for its products, Chief Financial Officer Sarah Friar said, even as the artificial intelligence startup competes in a crowded market to sign up more corporate customers. |
|[Hello Patient.](https://www.hellopatient.com/) | Hello Patient has emerged from stealth mode, securing a $6.3 million seed funding round led by 8VC. The company, founded by Alex Cohen, is based in Austin, Texas. |
|[Google plans to announce its next Gemini model soon.](https://www.theverge.com/2024/10/25/24279600/google-next-gemini-ai-model-openai-december) |December is shaping up to be a month of dueling announcements from OpenAI and Google.  |
|[Meta is reportedly developing a search engine for its chatbot.](https://www.engadget.com/ai/meta-is-reportedly-developing-a-search-engine-for-its-chatbot-172505704.html) | The company wants to decrease Meta AI‚Äôs reliance on Google and Microsoft.|
|[A mysterious new image generation model has appeared.](https://techcrunch.com/2024/10/28/a-mysterious-new-image-generation-model-has-appeared/) |A mysterious new image generation model is beating models from Midjourney, Black Forest Labs, and OpenAI on the crowdsourced Artificial Analysis benchmark. The model, which goes by the name ‚Äúred_panda,‚Äù is around 40 Elo points ahead of the next-best-ranking model, Black Forest Labs‚Äô Flux1.1 Pro, on Artificial Analysis‚Äô text-to-image leaderboard.|


## Resources
|Link|description|
|---|---|
|[Agentic Information Retrieval.](https://arxiv.org/abs/2410.09713) | offers an overview of agentic information retrieval, driven by the abilities of LLM agents; explores various advanced applications of agentic information retrieval and addresses related challenges.|
|[Aya Expanse.](https://cohere.com/blog/aya-expanse-connecting-our-world) |introduces a suite of open-weight foundation models designed for multilingual proficiency, featuring 8B and 32B parameter models and one of the largest multilingual datasets to date, containing 513 million examples. The release also includes Aya-101, which is claimed to be the most extensive multilingual model, supporting 101 languages. Aya Expanse 32B surpasses the performance of Gemma 2 27B, Mistral 8x22B, and Llama 3.1 70B, even though it is half the size of the latter. |
|[A Survey on Data Synthesis and Augmentation for Large Language Models.](https://arxiv.org/abs/2410.12896) |offers an in-depth overview of data generation techniques throughout the LLM lifecycle, covering topics such as data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and practical applications. |
|[granite-3.0-language-models.](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf) | introduces a range of lightweight foundation models from 400 million to 8 billion parameters, optimized for tasks such as coding, retrieval-augmented generation (RAG), reasoning, and function calling. Designed for enterprise applications, these models support on-premise and on-device deployment, showing robust performance across academic benchmarks in language understanding, reasoning, coding, function calling, and safety.|
|[Pixtral-12B-Base-2409.](https://huggingface.co/mistralai/Pixtral-12B-Base-2409) | Pixtral 12B base model weights have been released on Hugging Face.|
|[Arcade, a new AI product creation platform, designed this necklace.](https://techcrunch.com/2024/10/27/arcade-a-new-ai-product-creation-platform-designed-this-necklace/) |Arcade AI has developed a generative platform that allows users to create distinctive, high-quality jewelry items simply from text prompts‚Äîand the exciting part is, you can purchase the designs you generate. |
|[Retrieval-Augmented Diffusion Models for Time Series Forecasting.](https://arxiv.org/abs/2410.18712v1) | The Retrieval-Augmented Time Series Diffusion model (RATD) introduces a retrieval and guidance mechanism to enhance stability and performance in time series diffusion models. RATD operates in two steps: first, it retrieves relevant historical data from a database, then uses this information as a reference to guide the denoising phase.|
|[NotebookLlama: An Open Source version of NotebookLM.](https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama) |Meta has published a quick start guide to help users build a simplified version of Google‚Äôs popular NotebookLM system. |
|[How I Studied LLMs in Two Weeks: A Comprehensive Roadmap.](https://towardsdatascience.com/how-i-studied-llms-in-two-weeks-a-comprehensive-roadmap-e8ac19667a31) |This article presents a 14-day roadmap for mastering LLM fundamentals, covering key topics such as self-attention, hallucinations, and advanced methods like Mixture of Experts. It offers resources for building an LLM from the ground up, alongside curated literature and online materials, all organized within a GitHub repository. Emphasizing a tailored learning experience, the article underscores the importance of foundational skills in math, programming, and deep learning. |
|[Marly.](https://github.com/marly-ai/marly) | Marly is an open-source data processor enabling agents to query unstructured data using JSON, streamlining data interaction and retrieval.|
|[LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias.](https://haian-jin.github.io/projects/LVSM/) | It was previously believed that novel view synthesis depended heavily on strong 3D inductive biases. This study demonstrates that, with scale and a minimal inductive bias, it's possible to significantly surpass these previously assumed limitations.|
|[Continuous Speech Synthesis using per-token Latent Diffusion.](https://arxiv.org/abs/2410.16048) |Autoregressive models continue to excel in many applications, yet recent advancements with diffusion heads in image generation have led to the concept of continuous autoregressive diffusion. This research broadens the scope of per-token diffusion to accommodate variable-length outputs. |
|[CDChat: A Large Multimodal Model for Remote Sensing Change Description.](https://arxiv.org/abs/2409.16261v1) | This paper presents a change description instruction dataset aimed at fine-tuning large multimodal models (LMMs) to enhance change detection in remote sensing.|
|[IC-Light V2 (Flux-based IC-Light models).](https://github.com/lllyasviel/IC-Light/discussions/98) |IC Light currently offers the most effective method for associating images with a pretrained text-to-image backbone. This discussion marks the initial steps toward expanding that capability to the robust Flux models. |
|[The Scene Language: Representing Scenes with Programs, Words, and Embeddings.](https://github.com/zzyunzhi/scene-language) |Creating 3D scenes from scratch presents significant challenges, including data limitations. This research introduces a programming-like language for describing 3D scenes and demonstrates that Claude Sonnet can produce highly realistic scenes even without specific training for this task. |
|[3D Semantic Segmentation.](https://arxiv.org/abs/2410.19446v1) |FtD++ is a cross-modal learning approach designed to enhance unsupervised domain adaptation in 3D semantic segmentation tasks. |
|[Open source replication of crosscoder on Gemma 2B.](https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for) | Anthropic recently published two studies showcasing its novel interpretability method. This post provides an open replication of the crosscoder on the Gemma 2B model.|
|[Awesome-Graph-OOD-Learning.](https://github.com/kaize0409/awesome-graph-ood) |This repository lists papers on graph out-of-distribution learning, covering three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. |
|[OpenWebVoyager: Building Multimodal Web Agents.](https://github.com/minorjerry/openwebvoyager) |OpenWebVoyager offers tools, datasets, and models designed to build multimodal web agents that can navigate and learn from real-world web interactions. |
|[Automated Colorization for Animation.](https://ykdai.github.io/projects/InclusionMatching) | Researchers have introduced an innovative inclusion-matching technique that overcomes challenges in automated colorization, particularly for animations where occlusions and wrinkles complicate traditional segment matching.|
|[Lofi Music Dataset.](https://huggingface.co/datasets/vikhyatk/lofi) | A dataset containing music clips paired with detailed text descriptions, generated by a music creation model.|
|[Learning to Handle Complex Constraints for Vehicle Routing Problems.](https://arxiv.org/abs/2410.21066v1) | Researchers have developed a Proactive Infeasibility Prevention (PIP) framework designed to enhance neural network performance on Vehicle Routing Problems (VRPs) that involve challenging constraints.|
|[Unleashing the Power of AI on Mobile: LLM Inference for Llama 3.2 Quantized Models with ExecuTorch and KleidiAI.](https://pytorch.org/blog/unleashing-ai-mobile/) |PyTorch has made significant strides with ExecuTorch, a tool that enables AI model deployment at the edge, greatly enhancing the performance and efficiency of various end systems. |
|[CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution.](https://arxiv.org/abs/2410.16256v1) |CompassJudger-1 is the first open-source, comprehensive judge model created to enhance the evaluation process for large language models (LLMs). |
|[MINT-1T.](https://github.com/mlfoundations/MINT-1T) | MINT-1T, a vast open-source multimodal dataset, has been released with one trillion text tokens and 3.4 billion images, incorporating diverse content from HTML, PDFs, and ArXiv papers. This dataset, roughly ten times larger than previous collections, is intended to accelerate advancements in large-scale multimodal machine learning research.|
|[LARP: Tokenizing Videos üé¨ with a Learned Autoregressive Generative Prior üöÄ.](https://hywang66.github.io/larp/) |LARP is a novel video tokenizer designed to enhance video generation in autoregressive (AR) models by prioritizing global visual features over individual patch-based details. |
|[OpenAI's new hallucination benchmark.](https://openai.com/index/introducing-simpleqa/) |OpenAI has released the SimpleQA benchmark, which measures models' abilities around simple factual questions. |
|[ThunderKittens.](https://hazyresearch.stanford.edu/blog/2024-10-29-tk2) |Thunder Kittens is a framework designed for creating highly efficient GPU kernels. It leverages the principle that GPUs are optimized for working with compact 16x16 data tiles, resulting in high usability. With this approach, achieving 40% faster kernels requires only a few hundred lines of code.  |
|[Skinned Motion Retargeting with Dense Geometric Interaction Perception.](https://abcyzj.github.io/MeshRet/) |MeshRet has developed an innovative method for enhancing motion retargeting for 3D characters, prioritizing the preservation of body geometry interactions from the outset. |
|[Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance.](https://arxiv.org/abs/2410.13136v1) | ÓàÉResearchers have improved Masked Generative Models (MGMs) by introducing a self-guidance sampling technique, which enhances image generation quality without compromising diversity.|
|[Speeding Up Transformers with Token Merging.](https://github.com/hchautran/PiToMe) |This project presents PiToMe, an algorithm that compresses Vision Transformers by gradually merging tokens after each layer, thereby decreasing the number of tokens processed. |
|[PF3plat : Pose-Free Feed-Forward 3D Gaussian Splatting.](https://cvlab-kaist.github.io/PF3plat/) |PF3plat addresses the challenge of 3D reconstruction and novel view synthesis from RGB images without requiring additional data. |
|[Fine-tuning LLMs to 1.58bit: extreme quantization made easy.](https://huggingface.co/blog/1_58_llm_extreme_quantization) |BitNet, created by Microsoft Research, presents a transformer architecture that lowers the computational and memory demands of large language models by employing ternary precision (-1, 0, 1), equating to 1.58 bits per parameter. This architecture requires models to be trained from scratch, but it can also fine-tune existing models to this low-precision format while retaining high performance on downstream tasks. This technique greatly reduces energy consumption and enhances inference speed through specialized kernels that enable efficient matrix multiplication. |
|[SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Recognition.](https://github.com/jimmyxu123/select) | SELECT is the inaugural extensive benchmark designed to evaluate various data curation methods in image classification. ImageNet++ is a newly developed dataset that augments ImageNet-1K by incorporating five additional training-data variations, each curated through distinct techniques.|
|[ODRL: A Benchmark for Off-Dynamics Reinforcement Learning.](https://arxiv.org/abs/2410.20750v1) |ODRL is the first standardized benchmark designed to assess reinforcement learning methods in environments with differing dynamics. |
|[Text-to-Image Model to Generate Memes.](https://arxiv.org/abs/2410.22901v1) | Researchers have created an innovative adapter method for text-to-image models, enabling them to tackle complex tasks such as meme video generation while preserving the base model's strong generalization abilities.|
|[Anomaly Classification in Industry.](https://arxiv.org/abs/2410.14379v1) | AnomalyNCD is a multi-class anomaly classification framework intended to enhance traditional anomaly detection techniques in industrial environments.|
|[MrT5: Dynamic Token Merging for Efficient Byte-level Language Models.](https://github.com/jkallini/mrt5) |Byte-level language models represent a move toward a token-free future, but the challenge of sequence length remains significant. Dynamically merging tokens can help increase the number of tokens within the context. |
|[BART vectoriZed.](https://github.com/gattocrucco/bartz) |A new GPU-enabled implementation of Bayesian Additive Regression Trees (BART) significantly accelerates processing speed, making it up to 200 times faster than conventional CPU-based versions. |
|[Huge new Diffusers release.](https://github.com/huggingface/diffusers/releases/tag/v0.30.0) |The Hugging Face Diffusers package now includes new pipelines like Flux, Stable Audio, Kolors, CogVideoX, Latte, and others, alongside new methods such as FreeNoise and SparseCtrl, plus various refactors. |
|[4 experiments with voice AI models to help you explore culture.](https://blog.google/outreach-initiatives/arts-culture/4-experimentations-with-voice-ai-models-to-help-you-explore-culture/) |Google‚Äôs voice AI models allow users to engage with culture in innovative ways. Projects like Talking Tours provide AI-guided virtual tours, Mice in the Museum offers art narration, and Lip Sync animates lips to discuss cultural topics. These entertaining tools offer new perspectives on art and design. |


## Perspectives
|Link|description|
|---|---|
|[ByteDance intern fired for planting malicious code in AI models.](https://arstechnica.com/tech-policy/2024/10/bytedance-intern-fired-for-planting-malicious-code-in-ai-models/) |After rumors swirled that TikTok owner ByteDance had lost tens of millions after an intern sabotaged its AI models, ByteDance issued a statement this weekend hoping to silence all the social media chatter in China. |
|[Thinking Like an AI.](https://www.oneusefulthing.org/p/thinking-like-an-ai) |Large language models (LLMs) operate as advanced autocomplete systems, generating the next token based on a combination of their training data and current input. Small variations in input can influence predictions, resulting in different responses to the same question. Gaining insight into token prediction, training data context, and memory constraints can enhance effective AI usage. |
|[An Interview with Salesforce CEO Marc Benioff about AI Abundance.](https://stratechery.com/2024/an-interview-with-salesforce-ceo-marc-benioff-about-ai-abundance) | Salesforce CEO Marc Benioff recently spoke about the company's new AI initiative, Agentforce, showcasing its potential to transform enterprise applications and customer interactions. He contrasted Salesforce's approach with Microsoft‚Äôs Copilot, describing Salesforce‚Äôs solution as more cohesive and impactful, thanks to its strong platform and data infrastructure. During the interview, Benioff stressed the significance of AI-driven "agentic" layers designed to boost customer service and improve operational efficiency across various industries.|
|[How GPU Access Helps Startups Be Agile.](https://a16z.com/podcast/how-gpu-access-helps-startups-be-agile/) |Andreessen Horowitz's Oxygen program tackles GPU shortages by offering startups in its portfolio more accessible and flexible GPU resources, allowing them to bypass price surges and supply limitations. This initiative enables AI startups to concentrate on product development without the pressure of long-term capital expenditure, emphasizing the need for equitable access to critical resources in the competitive AI field. |
|[The Mask Comes Off: At What Price?](https://thezvi.substack.com/p/the-mask-comes-off-at-what-price) | OpenAI is approaching its shift to a Public Benefit B-Corporation, a move that could impact its investor dynamics and collaboration with Microsoft. This transition brings up questions around control and valuation, particularly concerning the nonprofit's stake, which could be substantial given OpenAI's role in advancing AGI. The company‚Äôs future profitability and strategic course are closely tied to the safe development of AGI, a pursuit with enormous potential value.|
|[What's so special about the human brain?.](https://www.nature.com/immersive/d41586-024-03425-y/index.html) |Torrents of data from cell atlases, brain organoids and other methods are finally delivering answers to an age-old question. |
|[‚ÄòEducational‚Äô apps are worth billions. We need to make sure they work.](https://www.nature.com/articles/d41586-024-03471-6) |Partnerships between developers and researchers could help to improve the quality of educational apps and other technologies. |
|[The huge protein database that spawned AlphaFold and biology‚Äôs AI revolution.](https://www.nature.com/articles/d41586-024-03423-0) |Pioneering crystallographer Helen Berman helped to set up the massive collection of protein structures that underpins the Nobel-prize-winning tool‚Äôs success. |
|[Extreme fire seasons are looming ‚Äî science can help us adapt.](https://www.nature.com/articles/d41586-024-03433-y) | Not all wildfires can be averted, but data, models and collaborations can help to chart a course to a fire-resilient future.|
|[AI-designed DNA sequences regulate cell-type-specific gene expression.](https://www.nature.com/articles/d41586-024-03170-2) |Researchers have used artificial-intelligence models to create regulatory DNA sequences that drive gene expression in specific cell types. Such synthetic sequences could be used to target gene therapies to particular cell populations. |
|[Pushing the frontiers of audio generation.](https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/) | DeepMind has shared additional details about the audio generation models behind NotebookLM.|
|[Evaluating feature steering: A case study in mitigating social biases.](https://www.anthropic.com/research/evaluating-feature-steering) | This study investigates the use of feature steering in AI models to adjust outputs in an interpretable way. It identifies a "steering sweet spot," where modifications do not compromise performance. Results demonstrate that steering can adjust social biases within specific areas but may also produce unintended effects outside those targets. Continued research is necessary to enhance feature steering, aiming for safer and more dependable AI outcomes.|
|[How we saved hundreds of engineering hours by writing tests with LLMs.](https://www.assembled.com/blog/how-we-saved-hundreds-of-engineering-hours-by-writing-tests-with-llms) | Assembled leverages LLMs to speed up and enhance software testing, allowing tests to be generated in minutes rather than hours. This approach boosts engineering productivity, saving time and enabling a stronger focus on feature development. LLMs create thorough and precise tests that uphold code quality and sustain development speed.|
|[How to train LLM as a judge to drive business value.](https://hamel.dev/blog/posts/llm-judge/) |"LLM As a Judge" is an approach for leveraging an existing language model to rank and score natural language. This post provides guidelines for effectively using this method to process or assess data. |

# ML news: Week 21 - 27 October

## Research
|Link|description|
|---|---|
|[Thinking LLMs: General Instruction Following with Thought Generation.](https://arxiv.org/abs/2410.10630) | The proposed training method aims to enhance LLMs with thinking capabilities for general instruction-following without relying on human-annotated data. It employs an iterative search and optimization process to facilitate thought generation, allowing the model to learn without direct supervision. For each user instruction, potential thoughts are evaluated using a judge model, which scores only the responses to identify the best and worst options. The resulting full outputs are then used as selected and rejected pairs for DPO (termed Thought Preference Optimization in this paper). This approach demonstrates superior performance on AlpacaEval and Arena-Hard.|
|[Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence.](https://arxiv.org/abs/2410.11163) | A new collaborative search algorithm is proposed to adapt LLMs using swarm intelligence, where a group of LLM experts collaboratively navigates the weight space to optimize a utility function that reflects various adaptation objectives. Experiments show that Model Swarms can effectively adjust LLM experts for a single task, multi-task domains, reward models, and a range of human interests. This approach outperforms 12 model composition baselines by up to 21.0% across different tasks and contexts.|
|[First-Person Fairness in Chatbots.](https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf) |This study explores first-person fairness, focusing on the fairness of interactions between users and ChatGPT, particularly examining any biases related to users' names. It utilizes a model powered by GPT-4o to analyze patterns and name sensitivity in the chatbot's responses based on different user names. The findings suggest that post-training significantly reduces harmful stereotypes overall. However, in areas such as entertainment and art, especially with open-ended tasks, the study reveals a higher level of bias, indicating a tendency to create narratives featuring protagonists whose gender aligns with the gender inferred from the user's name. |
|[Looking Inward: Language Models Can Learn About Themselves by Introspection.](https://arxiv.org/abs/2410.13787) |The report indicates that LLMs can gain knowledge through introspection that is not directly derivable from their training data. It suggests that these models possess privileged information about themselves, which could contribute to creating more interpretable and controllable systems. However, it also notes that this introspective ability has limitations, as models often struggle to predict their own behavior on tasks that require reasoning over extended outputs. |
|[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation.](https://arxiv.org/abs/2410.13848) |This proposal introduces a unified autoregressive framework for multimodal understanding and generation, which decouples visual encoding into independent pathways. By utilizing a single transformer architecture, it enhances flexibility and performance in both visual understanding and generation tasks. The framework claims to mitigate the trade-offs typically associated with vision tasks found in methods relying on a single visual encoder. As a result, it outperforms previous unified models and matches or exceeds the performance of task-specific models. |
|[Inference Scaling for Long-Context Retrieval Augmented Generation.](https://arxiv.org/abs/2410.04343) |This study employs two strategies to explore scaling laws for Retrieval-Augmented Generation (RAG): in-context learning (DRAG) and iterative prompting (IterRAG). It discovers that RAG performance steadily enhances with an increase in effective context length when configurations are optimized. Additionally, under optimal conditions, increasing inference computation yields linear improvements in long-context RAG performance. This insight leads to the creation of a computation allocation model designed to offer practical guidance for optimal computation distribution in long-context RAG situations. |
|[Agent S: An Open Agentic Framework that Uses Computers Like a Human.](https://arxiv.org/abs/2410.08164v1) |A novel open agentic framework has been developed to facilitate autonomous interactions with computers via a graphical user interface (GUI). Named Agent S, this framework addresses challenges such as knowledge acquisition, long-horizon planning, and managing dynamic interfaces. It introduces experience-augmented hierarchical planning that combines search and retrieval methods. Additionally, it utilizes an agent-computer interface to enable reasoning and control over GUI agents. Evaluation on the OSWorld benchmark demonstrates that Agent S surpasses the baseline by 9.37% in success rate, representing an 83.6% relative improvement, and sets a new state-of-the-art performance. |
|[Exploring Model Kinship for Merging Large Language Models.](https://arxiv.org/abs/2410.12613) |The study introduces the concept of model kinship to assess the similarity between LLMs. This measure is utilized to develop a model merging strategy called Top-k Greedy Merging with Model Kinship, which enhances performance. The authors discover that this new criterion allows for effective and continuous model merging. |
|[On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability.](https://www.arxiv.org/abs/2409.19924) |The report highlights that the o1-preview model excels in self-evaluation and constraint-following. However, it also points out that these o1 models exhibit bottlenecks in decision-making and memory management, particularly in the context of spatial reasoning. Specifically, the models tend to generate redundant actions and face challenges in generalizing across spatially complex tasks. |
|[Sabotage evaluations for frontier models.](https://www.anthropic.com/research/sabotage-evaluations) |Anthropic has conducted several innovative evaluations to identify vulnerabilities and assess misalignment in large, powerful models. |
|[Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities.](https://arxiv.org/abs/2410.11190) |A powerful open-source initiative aimed at replicating GPT-4's speech capabilities has emerged. This model was trained by aligning multiple modalities using pre-trained audio and speech encoders, allowing it to achieve advanced speech recognition and generation functionalities. |
|[Automatically Interpreting Millions of Features in Large Language Models.](https://arxiv.org/abs/2410.13928) |Interpreting SAE features on a large scale can be difficult. To address this, Eleuther has introduced a set of automatic interpreter features designed to help understand the meaning of elements within their context. |
|[Mitigating Object Hallucination via Concentric Causal Attention.](https://github.com/xing0047/cca-llava) |Object hallucination in vision-language models has been associated with Rotary Position Encoding (RoPE), which faces challenges in managing long-term dependencies between visual and textual inputs. To overcome this, the authors introduce Concentric Causal Attention (CCA), a novel positional alignment method that enhances the interaction between visual elements and instruction tokens. |
|[Simplifying, stabilizing, and scaling continuous-time consistency models.](https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/) | OpenAI has published work focusing on enhancing consistency models, which operate in two steps rather than the 1,000 steps typically used in diffusion models. While these models still depend on distillation from an existing diffusion model, the research seeks to improve their performance and stability as they scale.|
|[All you need are 32 tokens to represent video.](https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html) |Salesforce's new approach introduces a novel video encoder that significantly reduces the number of tokens needed for accurate representation. While similar attempts in the past have seen limited success, the breakthrough appears to come from combining an explicit temporal encoder with a spatial encoder, enabling more efficient video processing. |
|[CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing.](https://github.com/uclaml/cops) | CoPS is a novel algorithm that improves agents' sequential reasoning by allowing them to share experiences across various tasks, enhancing their overall learning and adaptability.|

## News
|Link|description|
|---|---|
|[US investigates 2.4m Tesla self-driving vehicles after reported collisions.](https://www.theguardian.com/technology/2024/oct/18/tesla-self-driving-car-investigation) | Road safety agency opens evaluation over reported collisions in low visibility|
|[Anthropic just made it harder for AI to go rogue with its updated safety policy.](https://venturebeat.com/ai/anthropic-just-made-it-harder-for-ai-to-go-rogue-with-its-updated-safety-policy/) | Anthropic has revised its Responsible Scaling Policy to incorporate Capability Thresholds for AI models that present substantial risks, including bioweapons and autonomous AI research. This policy is designed to establish industry standards by introducing AI Safety Levels, which mandate stricter safeguards according to the model's capabilities. By transparently sharing safety practices and appointing a Responsible Scaling Officer, Anthropic aims to take a leadership role in AI governance and encourage similar initiatives across the industry.|
|[Sam Altman‚Äôs Worldcoin becomes World and shows new iris-scanning Orb to prove your humanity.](https://techcrunch.com/2024/10/17/sam-altmans-worldcoin-becomes-world-and-shows-new-iris-scanning-orb-to-prove-your-humanity/) | The World project, co-founded by Sam Altman, seeks to authenticate human identity online through iris-scanning technology, addressing privacy issues and ongoing investigations in the EU. The initiative plans to integrate human verification into AI platforms and may redistribute wealth generated by AI through Worldcoins. Recent updates include the launch of a new blockchain, an app, and tools such as Deep Face to help combat deep fakes.|
|[Google - Gemini Long Context.](https://www.kaggle.com/competitions/gemini-long-context/overview) | The Gemini team has set aside $100,000 for the most effective applications of their long context model capabilities.|
|[Unleashing System 2 Thinking? AlphaCodium Outperforms Direct Prompting of OpenAI o1.](https://www.qodo.ai/blog/system-2-thinking-alphacodium-outperforms-direct-prompting-of-openai-o1/?utm_source=tldrai) |OpenAI's o1 model, demonstrating System 1.5 thinking, exhibits improved reasoning abilities compared to earlier LLMs but still lacks the comprehensive problem-solving capabilities of full System 2 thinking. AlphaCodium enhances o1's coding performance by offering a structured framework that supports reasoning and iterative refinement, resulting in greater accuracy on Codeforces benchmarks. Although the combination of o1 and AlphaCodium shows potential for advancing AI toward more profound reasoning, significant effort is still needed to incorporate complete System 2 thinking in AI models. |
|[Amazon's AI Generator Tool Can Now Create Audio Ads.](https://www.adweek.com/commerce/amazons-ai-generator-tool-can-now-create-audio-ads/) | Soon, you‚Äôll hear more audio ads on Amazon‚Äôs properties that were created with generative AI.|
|[Google Shopping is getting a ‚Äòfor you‚Äô feed of products.](https://www.theverge.com/2024/10/15/24268117/google-shopping-personalized-feed-products-ai) | Google Shopping is rolling out a personalized feed that shows you a stream of products you might like. The new feature, which is coming to mobile and desktop devices, shows up when you head to shopping.google.com.|
|[TikTok owner sacks intern for allegedly sabotaging AI project.](https://www.theguardian.com/technology/2024/oct/21/tiktok-owner-bytedance-sacks-intern-for-allegedly-sabotaging-ai-project) | ByteDance dismissed person in August it says ‚Äòmaliciously interfered‚Äô with training of artificial intelligence models |
|[AlphaFold reveals how sperm and egg hook up in intimate detail.](https://www.nature.com/articles/d41586-024-03319-z) |Three sperm proteins work together as matchmakers to enable fertilization in vertebrates. |
|[xAI, Elon Musk‚Äôs AI startup, launches an API.](https://techcrunch.com/2024/10/21/xai-elon-musks-ai-startup-launches-an-api/) | In August, Elon Musk‚Äôs xAI promised to make Grok, the company‚Äôs flagship generative AI model powering a number of features on X, available via an API. Now, that API has arrived ‚Äî albeit a bit bare-bones at the moment.|
|[Jane Street Real-Time Market Data Forecasting.](https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting) |This competition, hosted by Jane Street, challenges participants to build models using real-world data from production systems. The goal is to provide insights into the complexities of financial markets, requiring participants to apply their skills in data analysis and modeling to navigate the dynamic nature of market behavior. |
|[OCP Summit 2024: The open future of networking hardware for AI.](https://engineering.fb.com/2024/10/15/data-infrastructure/open-future-networking-hardware-ai-ocp-2024-meta/) | At OCP 2024, Meta unveiled a next-generation disaggregated network fabric and new network hardware specifically designed for AI clusters. The company introduced the Disaggregated Scheduled Fabric (DSF), aimed at improving scalability and performance in AI training systems. Both the newly developed and existing hardware are optimized for high throughput and efficiency, providing open, vendor-agnostic solutions to support advanced AI applications.|
|[Serve confirms delivery by robot expansion plans with Gen3 rollout.](https://newatlas.com/technology/serve-robotics-third-generation-autonomous-delivery-robot/) | Serve Robotics' third-generation delivery robot is equipped with NVIDIA's Jetson Orin module, significantly boosting its AI processing capabilities. This upgrade allows the robot to make faster, real-time autonomous navigation decisions, improving its efficiency and performance in delivery tasks.|
|[Boston Dynamics teams with TRI to bring AI smarts to Atlas humanoid robot.](https://techcrunch.com/2024/10/16/boston-dynamics-teams-with-tri-to-bring-ai-smarts-to-atlas-humanoid-robot/) |Boston Dynamics and Toyota Research Institute are partnering to integrate advanced AI and large behavior models into the electric Atlas humanoid robot. This collaboration aims to enhance the robot's capabilities, enabling more sophisticated and autonomous behaviors in tasks that require human-like movement and decision-making. |
|[Microsoft introduces ‚ÄòAI employees‚Äô that can handle client queries.](https://www.theguardian.com/technology/2024/oct/21/microsoft-launches-ai-employees-that-can-perform-some-business-tasks) |US company gives customers the ability to build own virtual agents as well as releasing 10 off-the-shelf bots |
|[Thom Yorke and Julianne Moore join thousands of creatives in AI warning.](https://www.theguardian.com/film/2024/oct/22/thom-yorke-and-julianne-moore-join-thousands-of-creatives-in-ai-warning) | Statement comes as tech firms try to use creative professionals‚Äô work to train AI models|
|[Claude AI tool can now carry out jobs such as filling forms and booking trips, says creator.](https://www.theguardian.com/technology/2024/oct/23/claude-ai-anthropic-computer-tasks-form-filling-booking-trips) |Anthropic says model is able to carry out computer tasks ‚Äì as fears mount such technology will replace workers |
|[Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku.](https://www.anthropic.com/news/3-5-models-and-computer-use) |Anthropic has enhanced Sonnet 3.5's capabilities and introduced a more affordable version that delivers the same performance as the previous Claude 3 Opus. Furthermore, Sonnet 3.5 has been trained with screen recordings, enabling it to operate computers and interact with user interfaces. |
|[ChatGPT has a Windows app now .](https://www.theverge.com/2024/10/17/24273040/chatgpt-windows-app-subscribers-openai) | The app, which is currently in testing, is only available to ChatGPT subscribers for now.|
|[Adobe's new image rotation tool is one of the most impressive AI concepts we've seen.](https://www.creativebloq.com/design/adobes-new-image-rotation-tool-is-one-of-the-most-impressive-ai-concepts-weve-seen) |Adobe's Project Turntable leverages AI to rotate 2D vector art in 3D, allowing the artwork to be viewed from various angles while preserving its 2D look and design integrity. This innovative technique ensures that the visual style remains consistent, even as the artwork is transformed in three-dimensional space. |
|[Perplexity lets you search your internal enterprise files and the web.](https://venturebeat.com/ai/perplexity-lets-you-search-your-internal-enterprise-files-and-the-web/) |Enterprises can use their Perplexity dashboards to search for internal information and combine it with knowledge from the internet, but this will only be limited to specific files they deem important. |
|[OpenAI, Microsoft reportedly hire banks to renegotiate partnership terms.](https://siliconangle.com/2024/10/18/openai-microsoft-reportedly-hire-banks-renegotiate-partnership-terms/) |OpenAI and Microsoft are in discussions regarding the terms of their partnership, with Microsoft aiming to acquire a substantial stake in OpenAI following its restructuring. |
|[Former OpenAI CTO Mira Murati is reportedly fundraising for a new AI startup.](https://techcrunch.com/2024/10/19/former-openai-cto-mira-murati-is-reportedly-fundraising-for-a-new-ai-startup/) |This startup will reportedly focus on building AI products based on proprietary models and could raise more than $100 million in this round. |
|[Midjourney plans to let anyone on the web edit images with AI.](https://techcrunch.com/2024/10/19/midjourney-plans-to-let-anyone-on-the-web-edit-images-with-ai/) | Midjourney is planning to release an upgraded web tool that‚Äôll let users edit any uploaded images from the web using Midjourney‚Äôs generative AI.|
|[Intel wins lengthy EU legal battle over ¬£880m competition fine.](https://www.theguardian.com/technology/2024/oct/24/intel-legal-battle-against-eu-880m-fine-competition) |Chipmaker disputed 2009 decision that it abused its market position in case dating back two decades |
|[Cohere's multilingual model's dramatic improvement.](https://cohere.com/blog/aya-expanse-connecting-our-world) |The Aya project, a standout initiative in multilingual language model training, has made impressive strides since its launch earlier this year. Much of its performance improvement is attributed to effective post-training strategies. Additionally, Aya can handle audio input and create images, all from non-English sources. |
|[Introducing the analysis tool in Claude.ai.](https://www.anthropic.com/news/analysis-tool) | Claude can now write and execute code as part of artifacts.|
|[Gurman: Apple internally believes that it‚Äôs at least two years behind in AI development.](https://9to5mac.com/2024/10/20/gurman-apple-intelligence-ai-two-years/) |According to the latest edition of Mark Gurman‚Äôs Power On newsletter, some employees at Apple believe that the company is around two years behind in artificial intelligence development. |
|[Perplexity is reportedly looking to fundraise at an $8B valuation.](https://techcrunch.com/2024/10/20/perplexity-is-reportedly-looking-to-fundraise-at-an-8b-valuation/) | AI search engine Perplexity is in fundraising talks and hopes to raise around $500 million at an $8 billion valuation, according to The Wall Street Journal.|
|[Chinese humanoid robot is the 'fastest in the world' thanks to its trusty pair of sneakers.](https://www.livescience.com/technology/robotics/chinese-scientists-build-fastest-humanoid-robot-in-the-world-watch-it-run-across-the-gobi-desert) |The STAR1 robot can reach a top speed of 8 mph with the added help of a pair of sneakers. |
|[From Rupert Murdoch to Thom Yorke: the growing backlash to AI.](https://www.theguardian.com/technology/2024/oct/25/unjust-threat-murdoch-and-artists-align-in-fight-over-ai-content-scraping) | Media mogul and leading artists join fight to stop tech firms using creative works for free as training data|
|[Talk to your plants? Now the first AI-powered garden will allow them to talk back.](https://www.theguardian.com/lifeandstyle/2024/oct/25/ai-powered-garden-chelsea-flower-show) |Collaboration between leading garden designer and Microsoft to go on display at Chelsea flower show 2025 |

## Resources
|Link|description|
|---|---|
|[CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos.](https://arxiv.org/abs/2410.11831) |This proposal introduces a new point tracking model along with a semi-supervised training recipe that allows for the use of real videos without annotations during training. It generates pseudo-labels using readily available teacher models. This approach simplifies the architecture and training scheme, resulting in improved outcomes while utilizing 1000 times less data. |
|[Meta's latest open source releases.](https://ai.meta.com/blog/fair-news-segment-anything-2-1-meta-spirit-lm-layer-skip-salsa-lingua/) |Meta has introduced a significant array of valuable research tools, including a speech-to-speech model, enhancements to SAM, and numerous other intriguing developments. |
|[One-Step Diffusion via Shortcut Models.](https://kvfrans.com/shortcut-models/) |Shortcut models represent a new category of consistency models that can produce continuous signals with minimal inference steps. |
|[Zero-Shot 3D Visual Grounding.](https://runsenxu.com/projects/VLM-Grounder/) | VLM-Grounder is a novel approach to 3D visual grounding that addresses the shortcomings of conventional methods by leveraging vision-language models (VLMs) and 2D images.|
|[DeepSeek's natively Multimodal model.](https://github.com/deepseek-ai/Janus) | DeepSeek has developed and launched a powerful 1.3 billion parameter model capable of processing interleaved text and images for both generation and comprehension.|
|[Meta Lingua.](https://github.com/facebookresearch/lingua) | Meta has developed an easy-to-use and research-friendly codebase that can replicate Llama 2 7B within 24 hours.|
|[Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization.](https://github.com/achernarwang/LiVO) | LiVO (Lightweight Value Optimization) is an innovative approach designed to align Text-to-Image models with human values.|
|[Easily hackable vision language model.](https://github.com/vikhyat/moondream/tree/main/moondream/torch) |A simple and performant VLM implementation in pure PyTorch |
|[Anthropic Quickstarts.](https://github.com/anthropics/anthropic-quickstarts) |Anthropic Quickstarts provides developers with projects like a customer support agent and a financial data analyst to help them swiftly utilize the Anthropic API. These projects leverage Claude for natural language processing and incorporate interactive data visualization. Each quickstart comes with setup instructions and encourages contributions from the community. |
|[BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities.](https://haoosz.github.io/BiGR/) | BiGR is an innovative image generation model that leverages compact binary latent codes to enhance both its generation and representation capabilities. It is the first model to integrate both generative and discriminative tasks within a unified framework. Key features of the model include binary tokenization and a distinctive entropy-ordered sampling technique, which contribute to its improved performance.|
|[LongPiBench.](https://github.com/Rachum-thu/LongPiBench) |LongPiBench is a benchmark created to evaluate positional biases in large language models (LLMs) when handling long contexts. It focuses on identifying biases that stem from the spacing between multiple relevant pieces of information, providing a targeted way to assess how well models handle long-range dependencies in text. |
|[CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models.](https://github.com/sanderwood/clamp2) | Clamp2 is a contrastive model designed for aligning music and text. It uses contrastive learning techniques to match and relate musical elements with corresponding textual descriptions, enhancing the ability to process and generate music-related text in alignment with audio.|
|[bitnet.cpp.](https://github.com/microsoft/BitNet) | Microsoft has released an inference repository for its 1.58 Bit models, which, when properly trained, are capable of running efficiently on consumer hardware. This development allows for more accessible deployment of advanced AI models without requiring high-end computational resources.|
|[Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning.](https://github.com/cxcscmu/montessori-instruct) | Montessori-Instruct is a novel framework designed to generate synthetic data that aligns with a student language model's learning process. It adapts the data produced by the teacher model to fit the student's learning preferences by leveraging local data influence and Direct Preference Optimization (DPO), optimizing the training experience for the student model.|
|[Stable Diffusion 3.5.](https://stability.ai/news/introducing-stable-diffusion-3-5) | 
Stability AI has launched a new series of models featuring enhanced performance and faster speeds. These models come with built-in Diffusers support, allowing for immediate training capabilities|
|[3D-GANTex: 3D Face Reconstruction with StyleGAN3-based Multi-View Images and 3DDFA based Mesh Generation.](https://arxiv.org/abs/2410.16009v1) |This paper presents a novel approach for estimating face texture and geometry from a single image by combining StyleGAN with 3D Morphable Models. |
|[Moonshine.](https://github.com/usefulsensors/moonshine) | Moonshine is a family of speech-to-text models optimized for fast and accurate automatic speech recognition (ASR) on resource-constrained devices. It is well-suited to real-time, on-device applications like live transcription and voice command recognition.|
|[PocketPal AI.](https://github.com/a-ghorbani/pocketpal-ai) |PocketPal AI is a pocket-sized AI assistant powered by small language models (SLMs) that run directly on your phone. Designed for both iOS and Android, PocketPal AI lets you interact with various SLMs without the need for an internet connection. |
|[Introducing the prompt() Function: Use the Power of LLMs with SQL!.](https://motherduck.com/blog/sql-llm-prompt-function-gpt-models/) |The costs of operating LLMs have dropped considerably, making it feasible to incorporate smaller models like gpt-4o-mini into SQL functions. MotherDuck's PROMPT() function simplifies tasks such as text generation, summarization, and structured data extraction using OpenAI models. It provides flexibility in balancing cost and performance, while also supporting bulk operations with improved concurrency for more efficient processing. |
|[Anthropic Computer Use Demo.](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo) |A quick example of Claude Sonnet's 3.5 new computer use capabilities. |
|[Introducing SynthID Text.](https://huggingface.co/blog/synthid-text) |SynthID is a method for statistically watermarking generated text. It employs a pseudorandom function after the top-k and top-p sampling steps to embed a mark within the text. A probabilistic Bayesian approach is then used to detect whether the text has been watermarked, indicating it was produced by a language model. |
|[Transformers.js v3: WebGPU Support, New Models & Tasks, and More‚Ä¶.](https://huggingface.co/blog/transformersjs-v3) |Transformers JS is a JavaScript library designed to run machine learning models, and it now supports WebGPU, offering up to 1,000x faster performance in some cases. The latest version provides access to over 1,200 models, making it well-suited for edge and browser-based applications. |
|[Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages.](https://neulab.github.io/Pangea/) |We present Pangea-7B, an open multilingual multimodal language model (MLLM) developed to address multilingual and multicultural challenges in visual understanding tasks. Pangea-7B is trained on PangeaIns, a comprehensive dataset consisting of 6 million instructions across 39 languages. |
|[SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree.](https://mark12ding.github.io/project/SAM2Long/) | SAM2Long solves the "error accumulation" problem found in SAM 2's memory design by implementing a training-free strategy for video object segmentation.|
|[Agent.exe.](https://github.com/corbt/agent.exe) |A convenient wrapper for Anthropic's computer use system simplifies its usage and execution, making it more user-friendly and accessible. |
|[TALoS: Enhancing Semantic Scene Completion via Test-time Adaptation on the Line of Sight.](https://github.com/blue-531/talos) |TALoS is a method that enhances scene completion for autonomous vehicles by leveraging observations from different time points as supervision for making more accurate predictions. |
|[OmniParser for Pure Vision Based GUI Agent.](https://microsoft.github.io/OmniParser/) | Screenshot parsing tool for models to use digital interfaces.|
|[Introducing quantized Llama models with increased speed and a reduced memory footprint.](https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/) |Meta has optimized its 1B and 3B language models by applying quantization, achieving a 2-4x speed increase and reducing model size by over 50% with minimal quality loss. This improvement is made possible by its quantization-aware training setup, allowing the models to adapt to lower precision effectively. |
|[Joint Point Cloud Upsampling and Cleaning with Octree-based CNNs.](https://arxiv.org/abs/2410.17001v1) |An effective and straightforward approach for upsampling and refining point clouds utilizes a modified octree-based 3D U-Net, known as OUNet. |
|[ExecuTorch.](https://github.com/pytorch/executorch) |ExecuTorch supports on-device inference across mobile and edge devices, including wearables, embedded systems, and microcontrollers. It facilitates the efficient deployment of PyTorch models to edge environments and is compatible with various computing platforms, leveraging hardware capabilities like CPUs, NPUs, and DSPs. Comprehensive tutorials provide guidance on using ExecuTorch step-by-step. |
|[Federated Transformer (FeT).](https://github.com/xtra-computing/fet) |The Federated Transformer (FeT) is a novel framework aimed at enhancing both performance and privacy in Vertical Federated Learning (VFL) across multiple collaborating parties. |
|[ADEM-VL.](https://github.com/hao840/adem-vl) |ADEM-VL is an innovative vision-language model created to address hardware constraints found in current models. |
|[Predicting Weight Loss with Machine Learning.](https://www.feelingbuggy.com/p/predicting-weight-loss-with-machine) | The author utilized a straightforward feedforward DNN model to monitor and forecast weight loss on a ketogenic diet. This model effectively captured the non-linear weight loss trends, fit a predictive function to the data, and visualized calorie metrics. For added insights, the Harris-Benedict Equation was applied to compare estimated calorie needs with actual weight loss.|
|[Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent.](https://simonwillison.net/2024/Oct/17/video-scraping/) |Google Gemini's AI Studio can accurately extract numerical data from video screen recordings of emails. This process leverages the cost-effective Gemini 1.5 Flash model, resulting in minimal expense. This innovative "video scraping" technique provides a practical alternative to conventional data extraction methods. |

## Perspectives
|Link|description|
|---|---|
|[Duolingo CEO Luis von Ahn wants you addicted to learning.](https://www.theverge.com/24267841/luis-von-ahn-duolingo-owl-language-learning-gamification-generative-ai-android-decoder) |Duolingo's CEO, Luis von Ahn, talks about utilizing AI and gamification to improve language learning through features such as chat interactions with AI avatars and AI-generated video game-like adventures. The company has recently launched Duolingo Max, a premium subscription plan that provides AI-driven conversation practice, capitalizing on the lower costs and faster development associated with AI-generated content. Although AI has limitations in engagement, Duolingo prioritizes maintaining user motivation by balancing effective learning with gamified, entertaining experiences. |
|[State of AI Report 2024.](https://www.stateof.ai/2024-report-launch) | The 2024 State of AI Report notes that foundational models are increasingly being integrated into practical applications, with OpenAI leading the way in significant revenue generation. Key developments include the alignment of performance among leading research labs, a growing emphasis on planning and reasoning in large language model (LLM) research, and the extension of foundational models into multimodal domains. Despite facing regulatory hurdles, AI companies have seen a surge in valuation, though questions about their long-term sustainability remain.|
|[How gen AI can help doctors and nurses ease their administrative workloads.](https://blog.google/products/google-cloud/generative-ai-healthcare-administration/) | Doctors and nurses spend nearly 28 hours a week on administrative tasks.|
|[Elon Musk‚Äôs global political goals.](https://www.theguardian.com/global/2024/oct/21/elon-musk-global-political-goals) |Over the weekend, Musk pledged to give away $1m a day to registered voters in battleground states in the US who sign his Pac‚Äôs petition in support of the first and second amendments. He awarded the first prize, a novelty check the size of a kitchen island, at a Pennsylvania rally on Saturday and the second on Sunday in Pittsburgh. He says he‚Äôll keep doing it until the election on 5 November. The stunt is potentially illegal, experts say. |
|[The Second $100B AI Company.](https://www.digitalnative.tech/p/the-second-100b-ai-company) |This article forecasts that by 2034, emerging AI companies fueled by advancements in AI applications, particularly in consumer AI, will join OpenAI in exceeding a $100B market cap. While established tech giants currently dominate the AI infrastructure and model layers, the application layer offers significant potential for innovation and expansion, providing fertile ground for consumer AI to flourish. The prospects for large-scale success in consumer AI, especially in areas such as video creation, online shopping, and gaming, resemble the transformative impact seen in past tech revolutions like cloud computing and mobile technology. |
|[Use Prolog to improve LLM's reasoning.](https://shchegrikovich.substack.com/p/use-prolog-to-improve-llms-reasoning) |Current methods such as Chain-of-Thought (CoT) reasoning and the integration of programming languages like Prolog can enhance the reasoning abilities of LLMs, helping to mitigate the limitations of autoregressive models. The paper "Reliable Reasoning Beyond Natural Language" introduces a neurosymbolic approach that employs Prolog to translate requests into symbolic logic, enhancing both explainability and problem-solving capabilities. ProSLM, the model developed in this research, has shown substantial improvements on various datasets, highlighting the potential of combining Prolog with LLMs for tackling complex reasoning tasks. |
|[AI watermarking must be watertight to be effective.](https://www.nature.com/articles/d41586-024-03418-x) |Scientists are closing in on a tool that can reliably identify AI-generated text without affecting the user‚Äôs experience. But the technology‚Äôs robustness remains a challenge. |
|[AI scans RNA ‚Äòdark matter‚Äô and uncovers 70,000 new viruses.](https://www.nature.com/articles/d41586-024-03320-6) |Many are bizarre and live in salt lakes, hydrothermal vents and other extreme environments. |
|[Build an international AI ‚Äòtelescope‚Äô to curb the power of big tech companies.](https://www.nature.com/articles/d41586-024-03436-9) | Artificial intelligence (AI) technologies have reached a crucial juncture. The vast computing clusters required to train the most advanced generative AI systems are available only to a few large corporations.|
|[Was the Nobel prize for physics? Yes ‚Äî not that it matters.](https://www.nature.com/articles/d41586-024-03435-w) |The award of the 2024 Nobel Prize in Physics to John Hopfield and Geoffrey Hinton for their groundbreaking research on artificial neural networks  has caused consternation in some quarters. Surely this is computer science, not physics? |
|[How I peer into the geometry behind computer vision.](https://www.nature.com/articles/d41586-024-03407-0) |Minh Ha Quang‚Äôs work at a Japanese AI research centre aims to understand how machines extract image data from the real world. |
|[AI Dreams: Microsoft @ 50, Chapter 1.](https://www.geekwire.com/2024/ai-dreams-microsoft-50-chapter-1/) |Microsoft's research on AI robustness led the company to invest billions in AI infrastructure, driving breakthroughs with partners such as OpenAI. This investment has played a key role in Microsoft's rapid growth in AI-powered products, highlighted by the success of GitHub Copilot. Despite facing competition and balancing sustainability goals, Microsoft remains committed to AI, with record capital expenditures on its AI and cloud infrastructure. |
|[Future of Internet in the age of AI.](https://crazystupidtech.com/archive/future-of-internet-in-the-age-of-ai/) | In this article, Cloudflare CEO Matthew Prince explores AI's influence on internet infrastructure, emphasizing the need for AI-capable edge computing and local inference to minimize network latency. He underscores the significance of regionalization in AI services to address regulatory challenges and outlines Cloudflare's strategy of developing a connectivity-focused network. Cloudflare's goal is to enhance internet connectivity by making it faster, more secure, and more efficient, closely aligning its efforts with advancements in AI technologies.|
|[How Jacob Collier helped shape the new MusicFX DJ.](https://blog.google/technology/ai/jacob-collier-labs-sessions/) |Grammy-winning musician Jacob Collier has partnered with Google DeepMind and Google Labs to develop MusicFX DJ, an AI-driven music tool. The tool‚Äôs interface has been revamped to foster creativity, making it easy for users to tap into a "flow state" of artistic inspiration. MusicFX DJ is now available, featuring user-friendly controls suitable for all experience levels. |
|[The AI Investment Boom.](https://www.apricitas.io/p/the-ai-investment-boom) |The AI boom is spurring substantial US investments in data centers, computing infrastructure, and advanced hardware, with annual data center construction reaching an unprecedented $28.6 billion. This growth is driven by rising demand for high-powered computing resources essential for training and deploying sophisticated AI models. Although tech sector revenue is recovering, job growth is primarily centered on semiconductor manufacturing and infrastructure, shifting attention away from traditional programming roles. |

## Research
|Link|description|
|---|---|
|[Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models.](https://arxiv.org/abs/2410.07176) | Introduces a novel RAG method to address the challenges of imperfect retrieval augmentation and knowledge conflicts in LLMs. Astute RAG adaptively extracts critical information from the internal knowledge of LLMs, then iteratively merges this with external knowledge while maintaining source awareness. Its interactive consolidation mechanism enhances the integration of internal and external information by identifying consistent passages, detecting conflicting data, and filtering out irrelevant content.|
|[ToolGen: Unified Tool Retrieval and Calling via Generation.](https://arxiv.org/abs/2410.03439) |Incorporates tool knowledge directly into LLMs by encoding tools as unique tokens, allowing the model to generate tool calls and arguments, facilitating smooth tool invocation alongside natural language generation. Experiments involving over 47,000 tools demonstrate that ToolGen outperforms in both tool retrieval and autonomous task execution. |
|[Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG.](https://arxiv.org/abs/2410.05983) | Finds that in many long-context LLMs, output quality diminishes as the number of passages increases, with the performance decline attributed to retrieved hard negatives. The authors propose two methods to enhance long-context LLM-based RAG: retrieval reordering and RAG-specific tuning with intermediate reasoning to improve relevance identification. These approaches show marked improvements in both accuracy and robustness in long-context RAG performance.|
|[GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models.](https://arxiv.org/abs/2410.05229) |Evaluates several state-of-the-art (SoTA) models using a benchmark built with symbolic templates that allow for a range of mathematical problems. The results show that LLMs display variability when answering different versions of the same questions, and their performance drops when numerical values in the questions are adjusted. As the complexity of the questions increases (e.g., adding more clauses), performance deteriorates significantly. The authors suggest that this decline in performance is likely due to a lack of logical reasoning capabilities in current LLMs. |
|[Addition is All You Need for Energy-efficient Language Models.](https://arxiv.org/abs/2410.00907) | Introduces an algorithm that approximates floating-point multiplication using integer addition operations, making it computationally less intensive than 8-bit floating-point arithmetic while achieving higher precision. The authors report that implementing the proposed L-Mul operation in tensor processing hardware could potentially reduce energy consumption by 95% for elementwise floating-point tensor multiplications and by 80% for dot product operations.|
|[I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy.](https://arxiv.org/abs/2410.07109) |Examines the interaction patterns of LLMs within a multi-agent setting involving a social hierarchy, specifically in a scenario where a guard and a prisoner interact, with the prisoner either seeking extra yard time or attempting to escape. The study finds that when power dynamics are present, LLMs struggle to maintain coherent conversations. Additionally, the authors highlight that agents' personas significantly influence their behaviors. Interestingly, even without explicit prompting, merely assigning roles to agents resulted in the emergence of anti-social behaviors. |
|[Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201) |The paper revisits RNNs and demonstrates that removing the hidden states from the input, forget, and update gates allows for efficient parallel training. This adjustment eliminates the need for architectures like LSTMs and GRUs to rely on backpropagation through time (BPTT). They introduce new variants, called minLSTMs and minGRUs, which are 175 times faster for sequences of length 512. |
|[LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations.](https://arxiv.org/abs/2410.02707) |The study finds that "truthfulness" information in LLMs is concentrated in specific tokens, offering a way to improve error detection and address related challenges. They also suggest that the internal representations of LLMs can be used to predict the types of errors these models are prone to making. |
|[Archon: An Architecture Search Framework for Inference-Time Techniques.](https://arxiv.org/abs/2409.15254) | The paper presents a modular framework for constructing and optimizing LLMs by integrating various inference-time techniques. This approach redefines the task of LLM system design as a hyperparameter optimization problem. Tested on benchmarks like MT-Bench and CodeContests, the framework, named Archon, outperforms top models such as GPT-4o and Claude 3.5 Sonnet, achieving a 15.1% average accuracy improvement.|
|[RATIONALYST: Pre-training Process-Supervision for Improving Reasoning.](https://arxiv.org/abs/2410.01044) | RATIONALYST is a model designed for process-supervision of reasoning, enabling it to generalize across a wide range of reasoning tasks. This is accomplished by pre-training on a dataset of 79k rationales from the Pile and a variety of reasoning datasets, with minimal human involvement. Fine-tuned from LLaMa-3-8B, the model achieves a 3.9% average accuracy improvement across seven reasoning benchmarks. |
|[Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation.](https://arxiv.org/abs/2409.12941) | The paper introduces a unified framework to evaluate an LLM‚Äôs capability to provide factual responses, assess retrieval skills, and reason through the generation of final answers. The framework includes multi-hop questions that require combining information from multiple sources. It reports that state-of-the-art LLMs struggle with this task, achieving only 40% accuracy without retrieval. However, the proposed multi-step retrieval method improves performance to 66% accuracy.|
|[Not All LLM Reasoners Are Created Equal.](https://arxiv.org/abs/2410.01748) | The paper introduces a unified framework to evaluate an LLM‚Äôs capability to provide factual responses, assess retrieval skills, and reason through the generation of final answers. The framework includes multi-hop questions that require combining information from multiple sources. It reports that state-of-the-art LLMs struggle with this task, achieving only 40% accuracy without retrieval. However, the proposed multi-step retrieval method improves performance to 66% accuracy.|
|[Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis.](https://arxiv.org/abs/2409.17439) |Training generative models like GANs with limited data is challenging. Existing Implicit Maximum Likelihood Estimation (IMLE) methods suffer from poor alignment between the latent codes used during training and those used during inference. The proposed approach, RS-IMLE, modifies the prior distribution during training, resulting in better test-time performance and higher quality image generation. |
|[Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models.](https://arxiv.org/abs/2410.11081) | This study introduces a unified framework aimed at enhancing training stability in continuous-time consistency models, leading to substantial improvements in the performance of generative models.|
|[DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection.](https://arxiv.org/abs/2410.11181v1) | DARNet is an innovative model for auditory attention detection (AAD) that improves the decoding of brain signals, such as EEG, by integrating spatiotemporal and dual attention mechanisms.|
|[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads.](https://github.com/mit-han-lab/duo-attention) | DuoAttention is a framework designed to optimize memory usage and reduce latency in long-context large language models (LLMs) by selectively applying full key-value (KV) caching to only the most essential attention heads.|
|[Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement.](https://arxiv.org/abs/2410.11448) | Meta Decision Transformer (Meta-DT) aims to enhance generalization in reinforcement learning by integrating transformer-based sequential modeling with effective task representation learning.|


## News
|Link|description|
|---|---|
|[AI gives voice to dead animals in Cambridge exhibition.](https://www.theguardian.com/science/2024/oct/14/ai-gives-voice-to-dead-animals-in-cambridge-exhibition) | Creatures can converse and share their stories by voice or text through visitors‚Äô mobile phones at Museum of Zoology |
|[Three-armed robot conductor makes debut in Dresden.](https://www.theguardian.com/world/2024/oct/13/three-armed-robot-maira-pro-s-conductor-makes-debut-dresden) | German city‚Äôs Sinfoniker says aim is not to replace humans but to play music human conductors would find impossible |
|[Tesla‚Äôs value drops $60bn after investors fail to hail self-driving ‚ÄòCybercab‚Äô.](https://www.theguardian.com/business/2024/oct/11/teslas-value-drops-60bn-after-self-driving-cybercab-fails-to-excite-investors) | Analysts criticise lack of detail about the ‚Äòrobotaxi‚Äô showcased by CEO Elon Musk|
|[Microsoft may have an audio-to-image generator in the works, new patent shows.](https://www.zdnet.com/article/microsoft-may-have-an-audio-to-image-generator-in-the-works-new-patent-shows/) |Microsoft has submitted a patent for an AI system that transforms live audio into images using large language models (LLMs). The system is intended to improve communication by creating real-time visuals from audio streams. Once developed, it could potentially be incorporated into Microsoft Teams through Copilot integration. |
|[Australia‚Äôs spy chief warns AI will accelerate online radicalisation.](https://www.theguardian.com/australia-news/2024/oct/11/australias-spy-chief-warns-ai-will-accelerate-online-radicalisation) |Asio boss Mike Burgess says social media impact is a ‚Äòstep-change‚Äô in the threat posed by extremism |
|[Google to buy nuclear power for AI datacentres in ‚Äòworld first‚Äô deal.](https://www.theguardian.com/technology/2024/oct/15/google-buy-nuclear-power-ai-datacentres-kairos-power) | Tech company orders six or seven small nuclear reactors from California‚Äôs Kairos Power|
|[Silicon Valley is debating if AI weapons should be allowed to decide to kill.](https://techcrunch.com/2024/10/11/silicon-valley-is-debating-if-ai-weapons-should-be-allowed-to-decide-to-kill/) | In late September, Shield AI co-founder Brandon Tseng swore that weapons in the U.S. would never be fully autonomous ‚Äî meaning an AI algorithm would make the final decision to kill someone. ‚ÄúCongress doesn‚Äôt want that,‚Äù the defense tech founder told TechCrunch. ‚ÄúNo one wants that.‚Äù |
|[Zoom‚Äôs custom AI avatar tool may come with risks.](https://techcrunch.com/2024/10/09/zooms-custom-ai-avatar-tool-may-come-with-risks/) | The upcoming feature, announced today at Zoom‚Äôs annual dev conference, will translate a video clip that users record of themselves into a digital clone ‚Äî complete with a head, upper arms, and shoulders. Users will be able to type a script of what they want the digital double to say, and Zoom will generate audio that syncs with the avatar‚Äôs lip movements.|
|[Generate Video (beta) on Firefly Web App.](https://blog.adobe.com/en/publish/2024/10/14/generate-video-beta-on-firefly-web-app) | During the Adobe MAX conference, Adobe revealed the extension of its Firefly series of creative generative AI models to include video. |
|[OpenAI appoints international expansion boss.](https://www.theregister.com/2024/10/09/openai_appoints_international_expansion_boss/) |OpenAI has named Oliver Jay as the head of its international expansion, with a focus on AI strategy and operations. The company also revealed the opening of a new APAC office in Singapore and is working on developing datasets for local languages. The o1 model, which incorporates "chain of thought" methods, is designed to improve AI accuracy. |
|[Anthropic challenges OpenAI with affordable batch processing.](https://venturebeat.com/ai/anthropic-challenges-openai-with-affordable-batch-processing/) |Anthropic has introduced a Message Batches API, enabling businesses to handle large data volumes at half the cost of traditional API calls. The API allows for up to 10,000 asynchronous queries within 24 hours, providing a cost-efficient solution by shifting AI processing from real-time to "right-time." This approach encourages AI adoption among mid-sized companies but may draw attention away from the advancement of real-time AI capabilities. |
|[OpenAI Projections Imply Losses Tripling To $14 Billion In 2026.](https://www.xm.com/research/markets/allNews/reuters/openai-projections-imply-losses-tripling-to-14-billion-in-2026-the-information-53942296) | OpenAI projects losses to rise to $14 billion in 2026, with total losses reaching $44 billion by 2028.|
|[AMD launches AI chip to rival Nvidia's Blackwell.](https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html) | AMD has introduced the Instinct MI325X AI chip, targeting competition with Nvidia's leading data center GPUs.|
|[Meta‚Äôs open AI hardware vision.](https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/) |Meta unveiled its open AI hardware designs, including the Catalina rack and the enhanced Grand Teton platform, at the OCP Global Summit. Notably, training the Llama 3.1 405B model required 16,000 NVIDIA H100 GPUs, demonstrating Meta's robust scaling infrastructure. These open AI hardware systems are essential for driving further advancements in AI capabilities. |
|[The New York Times warns AI search engine Perplexity to stop using its content.](https://www.theverge.com/2024/10/15/24270774/new-york-times-cease-and-desist-letter-perplexity-ai-search-engine) |The New York Times has sent a cease and desist letter to AI startup Perplexity, accusing the company of using its content without authorization for AI search purposes. Perplexity asserts that it does not scrape content for training but instead indexes web pages to provide factual information. The company is currently in discussions with publishers and seeks to resolve the matter by collaborating with the Times and other media organizations. |
|[Decagon raises $65m Series B led by Bain Capital Ventures to bring total funding to $100m.](https://decagon.ai/blog/series-b) |Decagon has secured $65 million in Series B funding to further develop its AI customer support agents, which are already utilized by companies such as Duolingo and Eventbrite to streamline customer interactions. These AI agents automate routine tasks, allowing customer support teams to focus on more strategic roles. The funding will be used to strengthen Decagon's engineering team and extend its AI solutions into new markets and industry sectors. |
|[New high quality AI video generator Pyramid Flow launches ‚Äî and it‚Äôs fully open source!](https://venturebeat.com/ai/new-high-quality-ai-video-generator-pyramid-flow-launches-and-its-fully-open-source/) |The number of AI video generation models continues to grow with a new one, Pyramid Flow, launching this week and offering high quality video clips up to 10 seconds in length ‚Äî quickly, and all open source. |
|[This three-person robotics startup is working with designer Yves B√©har to bring humanoids home.](https://techcrunch.com/2024/10/13/this-three-person-robotics-startup-is-working-with-designer-yves-behar-to-bring-humanoids-home/) |Kind Humanoid's three-person team is developing a whimsical humanoid robot named Mona, specifically designed for home use rather than industrial applications. The team aims to conduct field tests with a dozen initial prototypes next year. Unlike many AI-driven robotics companies that focus on industrial markets and heavy fundraising, Kind prioritizes innovation and efficiency, setting its approach apart from competitors in the robotics space. |
|[INTELLECT‚Äì1: Launching the First Decentralized Training of a 10B Parameter Model.](https://www.primeintellect.ai/blog/intellect-1) | INTELLECT-1 is the first decentralized model with 10 billion parameters, designed to harness global contributions for open-source AGI development. It utilizes OpenDiLoCo scaling to train large models across distributed devices, with innovations in bandwidth efficiency and fault tolerance. The new Prime framework further enhances decentralized training by optimizing compute utilization, achieving a 98% utilization rate during INTELLECT-1's 10-billion-parameter training run. This marks a significant advancement in decentralized AI model training.|
|[Elon Musk Shows Off Tesla ‚ÄòRobotaxi‚Äô That Drives Itself.](https://www.nytimes.com/2024/10/10/business/tesla-robotaxi-elon-musk.html) |‚ÄúYou could fall asleep and wake up at your destination,‚Äù said Mr. Musk, Tesla‚Äôs C.E.O., but some experts are skeptical that such cars will be ferrying passengers soon. |
|[ByteDance lays off hundreds of TikTok employees in shift to AI content moderation.](https://techcrunch.com/2024/10/11/bytedance-lays-off-hundreds-of-tiktok-employees-in-shift-to-ai-content-moderation/) |ByteDance‚Äôs TikTok is laying off hundreds of employees, mainly in Malaysia, according to Reuters. The cuts come as the social network is increasingly turning to AI for content moderation. The cuts do not impact employees in the U.S. |
|[Microsoft Artificial Intelligence VP Bubeck to Join OpenAI.](https://finance.yahoo.com/news/microsoft-artificial-intelligence-vp-bubeck-193734013.html) | Microsoft Corp. said one of its artificial intelligence vice presidents, Sebastien Bubeck, is leaving to join OpenAI, where Microsoft is both the largest investor and a rival.|
|[‚ÄòIt‚Äôs not me, it‚Äôs just my face‚Äô: the models who found their likenesses had been used in AI propaganda.](https://www.theguardian.com/technology/2024/oct/16/its-not-me-its-just-my-face-the-models-who-found-their-likenesses-had-been-used-in-ai-propaganda) |London-based Synthesia‚Äôs technology was employed to make deepfake videos for authoritarian regimes |
|[Amazon.com joins push for nuclear power to meet data center demand.](https://www.theguardian.com/technology/2024/oct/16/amazon-nuclear-power-data-center) |Company says it signed three agreements on developing small modular reactor nuclear power technology |
|[Un Ministral, des Ministraux.](https://mistral.ai/news/ministraux/) | On the first anniversary of Mistral 7B, Mistral launched two advanced models designed for on-device and edge computing: Ministral 3B and Ministral 8B. These models are optimized for tasks under 10 billion parameters, offering superior knowledge, reasoning, and efficiency. They also support a context length of up to 128k and deliver faster inference.|
|[Former Palantir CISO Dane Stuckey joins OpenAI to lead security.](https://techcrunch.com/2024/10/15/former-palantir-ciso-dane-stuckey-joins-openai-to-lead-security/) |Dane Stuckey, the former CISO of analytics firm Palantir, has joined OpenAI as its newest CISO, serving alongside OpenAI head of security Matt Knight. |
|[Can AI really compete with human data scientists? OpenAI‚Äôs new benchmark puts it to the test.](https://venturebeat.com/ai/can-ai-really-compete-with-human-data-scientists-openai-new-benchmark-puts-it-to-the-test/) |OpenAI has introduced a new tool to measure artificial intelligence capabilities in machine learning engineering. The benchmark, called MLE-bench, challenges AI systems with 75 real-world data science competitions from Kaggle, a popular platform for machine learning contests. |
|[Adobe‚Äôs AI video model is here, and it‚Äôs already inside Premiere Pro.](https://www.theverge.com/2024/10/14/24268695/adobe-ai-video-generation-firefly-model-premiere-pro) | New beta tools allow users to generate videos from images and prompts and extend existing clips in Premiere Pro.|
|[Customize Audio Overviews with Google's NotebookLM.](https://blog.google/technology/ai/notebooklm-update-october-2024/) | NotebookLM now enables users to customize their Audio Overview experience, providing greater control over the areas of focus and expertise of the AI hosts. Companies can apply for the new NotebookLM Business pilot program, which includes improved tools designed for professional applications.|
|[Combining next-token prediction and video diffusion in computer vision and robotics.](https://news.mit.edu/2024/combining-next-token-prediction-video-diffusion-computer-vision-robotics-1016) | A new method can train a neural network to sort corrupted data while anticipating next steps. It can make flexible plans for robots, generate high-quality video, and help AI agents navigate digital environments.|
|[Nvidia just dropped a new AI model that crushes OpenAI‚Äôs GPT-4‚Äîno big launch, just big results.](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/) |Nvidia quietly unveiled a new artificial intelligence model on Tuesday that outperforms offerings from industry leaders OpenAI and Anthropic, marking a significant shift in the company‚Äôs AI strategy and potentially reshaping the competitive landscape of the field. |
|[Invisible text that AI chatbots understand and humans can‚Äôt? Yep, it‚Äôs a thing.](https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/) |A quirk in the Unicode standard harbors an ideal steganographic code channel. |
|[Google supercharges Shopping tab with AI and personalized recommendation feed.](https://techcrunch.com/2024/10/15/google-supercharges-shopping-tab-with-ai-and-personalized-recommendation-feed/) | After bringing generative AI to Search in 2023, Google is supercharging its Shopping tab with the technology. The company announced on Tuesday that it will use AI to help users shop for products based on exactly what they‚Äôre looking for. It also launched a new scrollable feed of personalized, shoppable products. |
|[Adobe‚Äôs Project Super Sonic uses AI to generate sound effects for your videos.](https://techcrunch.com/2024/10/15/adobes-project-super-sonic-uses-ai-to-generate-sound-effects-for-your-videos/) | Adobe's Project Super Sonic leverages text-to-audio technology, object recognition, and voice input to create audio effects for video projects.|
|[White House considers expanding Nvidia‚Äôs and AMD‚Äôs AI chip export limits to additional countries.](https://techcrunch.com/2024/10/15/white-house-considers-expanding-nvidia-and-amds-ai-chip-export-limits-to-additional-countries/) | The Biden administration is contemplating limitations on AI chip sales from Nvidia and AMD to countries in the Persian Gulf, citing national security concerns.|


## Resources
|Link|description|
|---|---|
|[MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering.](https://arxiv.org/abs/2410.07095) | It introduces a new benchmark to assess machine learning agents' proficiency in machine learning engineering tasks. The benchmark consists of 75 Kaggle competitions focused on key MLE skills, including model training, dataset preparation, and experiment execution. OpenAI's o1-preview model, utilizing the AIDE scaffolding, reaches a bronze medal level in 16.9% of the competitions.|
|[Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System.](https://arxiv.org/abs/2410.08115) |Presents a novel framework aimed at improving both communication efficiency and task effectiveness in LLM-based multi-agent systems through targeted LLM training. It introduces an iterative "generate, rank, select, and train" approach, enhanced by a reward function to optimize performance, token usage, and communication efficiency. The framework integrates Monte Carlo Tree Search-inspired techniques for DPO data generation, promoting diverse exploration. Experimental results show consistent improvements over single-agent baselines and standard multi-agent systems (MAS) using Llama 3 8B, achieving a 2.8x performance boost while utilizing fewer than 10% of tokens on tasks involving extensive information exchange. |
|[Zyphra's Mamba 2 based model beats Mistral.](https://zyphra.webflow.io/post/zamba2-7b) | Introduces the first state space-style model that surpasses transformers at the 7B scale. It excels in understanding and generating long-context data, thanks to the linear time scaling of the Mamba 2 blocks, which significantly enhances its efficiency and performance.|
|[OpenAI's Swarm.](https://github.com/openai/swarm) |OpenAI has introduced a lightweight framework designed to facilitate communication between agents. While it will not receive further updates, the framework could still offer valuable ideas and inspiration for future developments. |
|[EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models.](https://arxiv.org/abs/2410.07133v2) |EvolveDirector aims to develop a competitive text-to-image generation model using open, publicly available resources, avoiding the limitations imposed by proprietary models. |
|[Rethinking the Evaluation of Visible and Infrared Image Fusion.](https://arxiv.org/abs/2410.06811v1) |Researchers propose the Segmentation-oriented Evaluation Approach (SEA) to improve the evaluation of Visible and Infrared Image Fusion (VIF) techniques, which play a critical role in applications such as object detection and semantic segmentation. |
|[A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research.](https://arxiv.org/abs/2410.07066v1) | A gentle introduction and tutorial on deep generative models in transportation research provides a comprehensive overview of how these models can be applied to solve transportation problems.|
|[Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis.](https://github.com/yangling0818/trans4d) |Trans4D is a new framework developed to address the challenges of realistic 4D scene transitions, enhancing text-to-4D synthesis. It offers improved capabilities in generating coherent, dynamic 4D scenes from textual descriptions, making it more suitable for tasks that require accurate spatial and temporal scene transitions. |
|[DocMTAgent.](https://github.com/yutongwang1216/docmtagent) |DelTA, short for Document-levEL Translation Agent, is an online translation tool designed for handling document-level translations. It leverages a multi-level memory architecture to improve translation accuracy and coherence across larger texts, providing more context-aware translations compared to sentence-level models. |
|[Fast Feedforward 3D Gaussian Splatting Compression.](https://yihangchen-ee.github.io/project_fcgs/) |Fast Compression of 3D Gaussian Splatting (FCGS) is a new model designed to eliminate the need for the slow, per-scene optimization required by earlier methods. Instead, FCGS achieves rapid compression using a quick feed-forward pass, reducing the processing time from minutes to just seconds. This significantly accelerates the compression process while maintaining high-quality results for 3D data. |
|[OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling.](https://arxiv.org/abs/2410.08021v1) |OneRef presents an optimized framework for referring segmentation by integrating visual and language feature spaces within a unified transformer architecture. |
|[SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction.](https://arxiv.org/abs/2410.08669v1) |SmartPretrain offers a versatile, model-agnostic, and dataset-agnostic self-supervised learning framework designed to enhance motion prediction in autonomous vehicles. |
|[UvA - An Introduction to Group Equivariant Deep Learning.](https://uvagedl.github.io/) | Resources for studying deep learning techniques applied to specific types of geometric data while addressing architectural limitations.|
|[Diffusion model simulating CS:GO.](https://github.com/eloialonso/diamond/tree/csgo) |An open-source replication of a diffusion model that generates visual simulations of a video game, using keyboard and mouse inputs to influence the output. |
|[Reward-Augmented Data Enhances Direct Preference Alignment of LLMs.](https://github.com/shenao-zhang/reward-augmented-preference) | This study addresses the shortcomings of current alignment algorithms in large language models (LLMs), which tend to overfit to relative preferences and neglect response quality. The authors introduce reward-conditioned LLM policies and a novel data relabeling method that incorporates response quality, enabling the model to better generalize to optimal responses.|
|[entropix.](https://github.com/samefarrar/entropix_mlx/tree/metrics_viz) |Entropix is a tool designed to modify the sampling behavior of language models. |
|[LoLCATs Blog Part 2: How to Linearize LLMs for Me and You.](https://hazyresearch.stanford.edu/blog/2024-10-14-lolcats-p2) |Hazy Research has published another insightful post that delves into techniques for linearizing existing language models while maintaining much of their performance. This exploration highlights methods to simplify model architectures, making them more efficient, without significantly compromising their effectiveness in tasks like text generation and understanding. |
|[TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control.](https://arxiv.org/abs/2410.10133v1) | TextCtrl is a newly introduced diffusion-based method designed to enhance scene text editing. It achieves a balance between maintaining content accuracy and preserving the original style, ensuring that both the textual content and the visual appearance remain consistent during edits.|
|[Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies.](https://arxiv.org/abs/2410.10803v1) | iDP3 is an advanced 3D visuomotor policy designed to enable humanoid robots to autonomously navigate and perform tasks in a variety of real-world environments. This improved policy enhances the robot's ability to perceive and interact with its surroundings, making it more adaptable and efficient in complex and dynamic settings.|
|[tabled.](https://github.com/VikParuchuri/tabled) |Tabled is a small library for detecting and extracting tables. It uses surya to find all the tables in a PDF, identifies the rows/columns, and formats cells into markdown, csv, or html. |
|[HART: Efficient Visual Generation with Hybrid Autoregressive Transformer.](https://hanlab.mit.edu/projects/hart) |HART is a cutting-edge visual generation model designed to produce high-quality 1024x1024 images, presenting a challenge to the capabilities of diffusion models. It enhances image reconstruction and reduces training costs by employing a hybrid tokenizer that integrates both discrete and continuous tokens, resulting in more efficient and effective image generation. |
|[DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention.](https://github.com/maclong01/DeBiFormer) | The Deformable Bi-level Routing Attention (DBRA) module is an innovation designed to enhance attention mechanisms in vision transformers. DeBiFormer, which is built upon DBRA, optimizes the selection of key-value pairs in the attention process, resulting in more efficient computations and better interpretability of queries within attention maps. This leads to improved performance and understanding of how the model attends to different parts of an image. |
|[Six tips for going public with your lab‚Äôs software.](https://www.nature.com/articles/d41586-024-03344-y) | It‚Äôs not enough to write high-quality programs. If you want to make your apps public ‚Äî and usable ‚Äî you should also follow these steps. |
|[CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos.](https://cotracker3.github.io/) | CoTracker is a newly developed tracking model that bridges the performance gap between synthetic and real video data by employing semi-supervised training techniques.|
|[A Consistency-Aware Spot-Guided Transformer for Versatile and Hierarchical Point Cloud Registration.](https://github.com/renlanghuang/cast) | Researchers have developed a novel consistency-aware spot-guided Transformer designed to improve the efficiency and accuracy of point cloud registration.|
|[Ditto - the simplest self-building coding agent.](https://github.com/yoheinakajima/ditto) |Ditto is a user-friendly tool that allows you to generate a multi-file Flask application from simple natural language descriptions using a no-code interface. By leveraging a simple LLM loop with a few tools, Ditto automates the coding process, (occasionally) turning your ideas into functional web applications (or at least trying and getting close). |
|[F5 Text-to-Speech System.](https://github.com/lucasnewman/f5-tts-mlx) | F5-TTS is a non-autoregressive, zero-shot text-to-speech system featuring a flow-matching mel spectrogram generator and a diffusion transformer. Developed on the MLX framework, F5 outperforms earlier systems such as E2 TTS by incorporating ConvNeXT v2 blocks for improved text alignment, enabling high-quality speech generation in approximately 11 seconds on modern hardware.|
|[Movie Gen Bench.](https://github.com/facebookresearch/MovieGenBench) |"Movie Gen Bench" is an evaluation benchmark designed to assess performance in both video (Video Bench) and audio (Audio Bench). It includes 1,003 prompts that encompass a variety of testing aspects and concepts. |
|[LongAlign.](https://github.com/luping-liu/longalign) |LongAlign enhances the capability of text-to-image (T2I) diffusion models to process lengthy text inputs by incorporating segment-level encoding and a decomposed preference optimization approach. |
|[Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective.](https://github.com/DAMO-NLP-SG/DiGIT) |DiGIT is an auto-regressive generative model that forecasts tokens in a latent space through self-supervised learning. This discrete tokenizer enhances image generation on ImageNet by clustering hidden states derived from DINOv2. |
|[FL-Launching (Fling).](https://github.com/FLAIR-Community/Fling) | The FedPart method tackles the layer mismatch problem in federated learning by limiting model updates to designated layers in each training round.|
|[Distributed Training Guide.](https://github.com/LambdaLabsML/distributed-training-guide) |This is an in-depth guide on best practices for distributed training, troubleshooting errors, and maximizing the use of available resources. |

## Perspectives
|Link|description|
|---|---|
|[Nobel winner Geoffrey Hinton is the ‚Äògodfather of AI‚Äô. Here‚Äôs an offer he shouldn‚Äôt refuse‚Ä¶](https://www.theguardian.com/commentisfree/2024/oct/12/nobel-winner-geoffrey-hinton-is-the-godfather-of-ai-heres-an-offer-he-shouldnt-refuse) | The computer scientist‚Äôs dogged belief in the potential of neural networks helped unlock machine learning. But he‚Äôd be wise to remember the experience of a fellow laureate |
|[Machines of Loving Grace.](https://darioamodei.com/machines-of-loving-grace) |Dario Amodei, CEO of Anthropic, often writes internal memos, and one of them was published externally. In this memo, he explores the potential extreme positive impact of successfully building powerful AI systems. He envisions how AI could radically transform the world for the better, improving areas like science, economics, and societal well-being, while acknowledging the immense responsibility in ensuring AI development is aligned with human interests and safety. |
|[This AI-Powered Invention Machine Automates Eureka Moments.](https://spectrum.ieee.org/ai-invention) |Iprova's AI-driven software analyzes diverse technical literature to generate patentable inventions by linking previously unrelated ideas. It uses semantic search and generative AI to identify novel inventions for companies like Procter & Gamble and Panasonic. Although AI plays a key role, human insight remains essential for applying the inventions practically, especially in fast-evolving industries. Iprova highlights the importance of human creativity in refining and validating invention ideas, ensuring that AI serves as a tool to enhance rather than replace human innovation. |
|[Burn the Playbooks.](https://www.notboring.co/p/burn-the-playbooks) | AI excels at tasks that follow structured rulesets, such as automating tax processes or solving math problems, where it can often outperform humans. However, relying too much on playbook-driven approaches in our work risks stifling human creativity, a key trait that differentiates us from machines. Overemphasizing formulaic tasks could make us more dependent on AI's strengths, limiting our own unique creative potential and inadvertently making us more "machine-like" in areas where creativity and flexibility are crucial.|
|[Hurricane Helene and the ‚ÄòFuck It‚Äô Era of AI-Generated Slop.](https://www.404media.co/hurricane-helene-and-the-fuck-it-era-of-ai-generated-slop/) | An AI-generated image depicting Hurricane Helene has gone viral, despite viewers being fully aware that it isn't real. The image has sparked widespread attention and discussion, highlighting the power of AI-generated content to captivate audiences even when the authenticity is known. This trend reflects the growing influence of AI in shaping public perception and the viral nature of digital content.|
|[OpenAI pursues public benefit structure to fend off hostile takeovers.](https://www.ft.com/content/5649b66e-fdb3-46d3-84e0-23e33bdaf363) |OpenAI is planning to restructure as a public benefit corporation (PBC) to safeguard against hostile takeovers and ensure its mission of benefiting humanity remains intact. This change will help OpenAI maintain its commitment to ethical AI development, prioritizing public good over profit while allowing the organization to continue innovating in a sustainable and mission-driven way. |
|[Al Will Take Over Human Systems From Within.](https://www.noemamag.com/al-will-take-over-human-systems-from-within/) | In this post, Yuval Noah Harari, the Israeli historian and author of ‚ÄúSapiens,‚Äù ‚ÄúHomo Deus,‚Äù and ‚ÄúNexus,‚Äù explores the impact of information networks and AI on societal narratives, which can either unite or fragment communities. He cautions that AI, functioning as an "alien intelligence," could centralize power due to its lack of self-correcting mechanisms, potentially threatening democratic systems. Harari stresses the importance of strong institutions to uphold truth in a world increasingly influenced by AI-driven decision-making across different sectors.|
|[Sticky humans in a post-AGI world.](https://www.theintrinsicperspective.com/p/sticky-humans-in-a-post-agi-world) | AI tutors encounter considerable difficulties in replicating the social and intellectual interactions offered by human teachers. Although AI has made progress, it still falls short in handling complex educational tasks and cannot deliver the nuanced socio-intellectual experiences that human educators provide. A hybrid approach, where AI complements rather than replaces human teachers, may be more effective, given the essential social and cultural elements of the learning process.|
|[AI has dreamt up a blizzard of new proteins. Do any of them actually work?](https://www.nature.com/articles/d41586-024-03335-z) | Emerging protein-design competitions aim to sift out the functional from the fantastical. But researchers hope that the real prize will be a revolution for the field. |
|[Considerations for governing open foundation models.](https://www.science.org/doi/10.1126/science.adp1848) |Foundation models drive AI innovation, but debates on their release‚Äîwhether open or closed‚Äîraise concerns about potential risks and the impact of regulations on innovation. |
|[I AI-generated some podcasts ‚Äì and the results are uncanny.](https://www.theguardian.com/tv-and-radio/2024/oct/16/i-ai-generated-some-podcasts-and-the-results-are-uncanny) | Google‚Äôs new tool NotebookLM lets you create podcasts at the click of the button. They‚Äôre way more realistic than you‚Äôd think ‚Ä¶|
|[SB 1047: Our Side Of The Story.](https://www.astralcodexten.com/p/sb-1047-our-side-of-the-story) |California's proposed SB 1047, which sought to require AI companies to address existential risks posed by their technologies, was vetoed by Governor Newsom. He argued that the bill did not adequately regulate smaller, potentially dangerous AI models. Despite strong support from AI safety advocates like Dan Hendrycks and high-profile figures such as Elon Musk, the bill faced opposition from major AI companies, including OpenAI and Google. Newsom's veto has sparked discussions within the AI community about future regulatory strategies and potential collaborations with broader political groups to create comprehensive AI safety measures. |
|[Overview of strong human intelligence amplification methods.](https://www.lesswrong.com/posts/jTiSWHKAtnyA723LE/overview-of-strong-human-intelligence-amplification-methods) | Advancements in AI depend on developing humans with enhanced cognitive abilities to effectively manage the complexities of AGI development. Approaches such as brain emulation, genomic modifications, adult brain gene editing, and brain-brain interfaces are being explored, each presenting distinct challenges and risks. These efforts are aimed at solving deep philosophical issues, significantly amplifying human intelligence, and addressing the potential threats posed by AGI.|
|[LLMs don‚Äôt do formal reasoning - and that is a HUGE problem.](https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and) | A study conducted by Apple raises questions about the effectiveness of large language models (LLMs), revealing that they primarily depend on pattern matching instead of formal reasoning. This reliance results in fragile and inconsistent outcomes, challenging the robustness of LLMs in tasks requiring deeper cognitive processes.|
|[Why ChatGPT maker OpenAI is at fight with Open AI.](https://timesofindia.indiatimes.com/technology/tech-news/why-chatgpt-maker-openai-is-at-fight-with-open-ai/articleshow/114220808.cms) |OpenAI is currently engaged in a legal dispute with Guy Ravine's company, Open AI, over the rights to the "Open AI" name and the original open-source AI vision. The conflict centers on ownership of the name and the direction of the open-source principles that initially defined the AI development approach. |
|[AI mediation tool may help reduce culture war rifts, say researchers.](https://www.theguardian.com/technology/2024/oct/17/ai-mediation-tool-may-help-reduce-culture-war-rifts-say-researchers) |System built by Google DeepMind team takes individual views and generates a set of group statements |
|[Here‚Äôs the deal: AI giants get to grab all your data unless you say they can‚Äôt. Fancy that? No, neither do I.](https://www.theguardian.com/commentisfree/2024/oct/18/ai-systems-big-tech-data-ministers) | Data is vital to AI systems, so firms want the right to take it and ministers may let them. We must wake up to the danger|
|[Where‚Äôs The Generative AI ROI? Start With The Supply Chain.](https://www.bigtechnology.com/p/wheres-the-generative-ai-roi-start) | Generative AI is revolutionizing supply chain operations by effectively managing unstructured documents, resulting in substantial time and cost savings. Flexport, a technology company focused on supply chain solutions, has effectively implemented AI to automate and optimize document management, cutting processing time by 80%. This use of AI highlights its practical value in revenue-generating activities rather than merely in theoretical advancements.|


# ML news: Week 7 - 13 October

## Research
|Link|description|
|---|---|
|[A multimodal generative AI copilot for human pathology.](https://www.nature.com/articles/s41586-024-07618-3) |PathChat is a vision-language AI assistant designed for pathology, combining a foundational vision encoder and a large language model, achieving state-of-the-art performance on diagnostic tasks and outperforming other multimodal AI systems, with potential applications in education, research, and clinical decision-making. |
|[Meta Movie Gen.](https://ai.meta.com/research/movie-gen/) | Meta has developed a cutting-edge movie model with 30 billion parameters, which required 6,144 H100 GPUs for training. The model was trained using 1 billion images and 100 million carefully selected videos. Notably, it is based on a Temporal Autoencoder and incorporates Flow matching Llama. Meta also published a highly detailed 92-page research paper, making it one of the most comprehensive reports on the subject.|
|[When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1.](https://arxiv.org/abs/2410.01792) |Large language models face limitations because they rely on next token prediction. Although OpenAI's o1 model was trained with a new objective focused on reasoning traces, it still exhibits some of the same constraints associated with next token prediction. |
|[Contextual Document Embeddings.](https://arxiv.org/abs/2410.02525) |This paper presents a method similar to a neutral TF/IDF, as it gathers information from the entire corpus rather than relying on individual document embeddings. It effectively captures contextual information from surrounding documents and has achieved state-of-the-art results on the MTEB benchmark. |
|[PairDistill: Pairwise Relevance Distillation for Dense Retrieval.](https://github.com/miulab/pairdistill) | This project introduces a novel technique called Pairwise Relevance Distillation (PairDistill), aimed at enhancing the accuracy of dense retrieval methods.|
|[Modeling relationships to solve complex problems efficiently.](https://news.mit.edu/2024/julian-shun-solves-complex-problems-efficiently-1004) | Associate Professor Julian Shun develops high-performance algorithms and frameworks for large-scale graph processing.|
|[Factual Accuracy in AI.](https://arxiv.org/abs/2410.01556v1) |Integrative Decoding is a technique designed to improve the factual accuracy of large language models, particularly for open-ended tasks. This method helps ensure more reliable and accurate outputs by refining the model's ability to integrate information during generation. |
|[Dynamic Diffusion Transformer.](https://arxiv.org/abs/2410.03456v1) |The Dynamic Diffusion Transformer (DyDiT) improves the efficiency of diffusion models in image generation by building on the Diffusion Transformer (DiT). It achieves this by dynamically adjusting computational resources across different timesteps and spatial regions, minimizing redundancy and optimizing performance. |
|[Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach.](https://arxiv.org/abs/2410.03160v1) |The Frame-Aware Video Diffusion Model (FVDM) enhances video generation by overcoming the limitations of existing models. Instead of using a single timestep for the entire video clip, FVDM introduces a vectorized timestep variable, enabling each frame to follow its own noise schedule. This approach improves the quality and coherence of generated videos. |
|[What Matters for Model Merging at Scale?](https://arxiv.org/abs/2410.03617) |Model merging is a technique that allows the combination of two models to achieve the performance benefits of both. However, it does not always scale effectively with larger model sizes. This paper investigates the requirements and challenges for making model merging work efficiently with very large models, addressing issues related to scalability, performance trade-offs, and optimal merging strategies. |
|[nGPT: Normalized Transformer with Representation Learning on the Hypersphere.](https://arxiv.org/abs/2410.01131) | A significant amount of research effort is focused on normalizing the internal representations of language models. This study demonstrates that by placing every internal vector on a hypersphere, convergence time is significantly reduced for models of reasonable size, leading to more efficient training.|
|[Genomic Foundation Model Benchmarking.](https://arxiv.org/abs/2410.01784v1) | GFMBench is a newly developed framework aimed at tackling challenges in the development of genomic foundation models (GFMs) by offering standardized benchmarking tools. It supports the evaluation of GFMs with millions of genomic sequences and hundreds of tasks, automating the benchmarking process for open-source GFMs to streamline their development and comparison.|
|[LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations.](https://arxiv.org/abs/2410.02707) |This study provides further evidence that language models internally encode signals when they produce non-factual information. Understanding these internal cues can help guide models more effectively and reduce the occurrence of hallucinations, offering a potential strategy for improving their reliability. |
|[Differential Transformer.](https://arxiv.org/abs/2410.05258) | Transformers often over-allocate attention to irrelevant context, leading to inefficiencies. This research presents the Diff Transformer, which enhances attention to relevant information while filtering out noise. It introduces a differential attention mechanism that computes attention scores by subtracting two separate softmax attention maps. This subtraction effectively cancels out noise and encourages sparse, more focused attention patterns, improving the model's performance on tasks requiring precise context understanding.|


## News
|Link|description|
|---|---|
|[Brave New World: Leo AI and Ollama Bring RTX-Accelerated Local LLMs to Brave Browser Users.](https://blogs.nvidia.com/blog/rtx-ai-brave-browser/) |Nvidia's RTX-Acceleration combined with Ollama allows for running local models in the browser. |
|[Liquid Foundation Models.](https://www.liquid.ai/liquid-foundation-models) | Liquid AI has introduced its first generation of Liquid Foundation Models (LFMs), offering state-of-the-art performance while minimizing memory consumption. The LFMs, which are optimized for different hardware platforms, include 1B, 3B, and 40B parameter models. These models are already accessible on platforms like LIQUID PLAYGROUND and will soon be available on Cerebras. They are particularly adept at processing sequential data and provide innovations in efficiency and scalability across industries like financial services and biotechnology.|
|[Introducing Copilot Labs and Copilot Vision.](https://www.microsoft.com/en-us/microsoft-copilot/blog/2024/10/01/introducing-copilot-labs-and-copilot-vision/) | Microsoft is launching Copilot Labs to test advanced AI tools, including Think Deeper and Copilot Vision. These tools aim to expand the capabilities of their AI systems, offering enhanced functionality and deeper insights.|
|[OpenAI‚Äôs DevDay brings Realtime API and other treats for AI app developers.](https://techcrunch.com/2024/10/01/openais-devday-brings-realtime-api-and-other-treats-for-ai-app-developers/) |It‚Äôs been a tumultuous week for OpenAI, full of executive departures and major fundraising developments, but the startup is back at it, trying to convince developers to build tools with its AI models at its 2024 DevDay. The company announced several new tools Tuesday, including a public beta of its ‚ÄúRealtime API‚Äù, for building apps with low-latency, AI-generated voice responses. It‚Äôs not quite ChatGPT‚Äôs Advanced Voice Mode, but it‚Äôs close. |
|[Microsoft brings AI-powered overviews to Bing.](https://techcrunch.com/2024/10/01/microsoft-brings-ai-powered-overviews-to-bing/) |Microsoft has introduced Bing generative search, an AI-driven feature that gathers and summarizes information from the web, offering users more concise and aggregated search results. |
|[KoBold Metals, which uses AI to help find critical minerals for the energy transition, raises $491M.](https://techcrunch.com/2024/10/07/ai-powered-critical-mineral-startup-kobold-metals-has-raised-491m-filings-reveal/) |Earlier this year, KoBold Metals found what might be one of the largest high-grade copper deposits of all time, with the potential to produce hundreds of thousands of metric tons per year, the company‚Äôs CEO said. |
|[OpenAI gets $4 billion revolving credit line, giving it more than $10 billion in liquidity.](https://www.cnbc.com/2024/10/03/openai-gets-4-billion-revolving-credit-line-on-top-of-latest-funding.html) |OpenAI has secured over $10 billion in liquidity, achieving a valuation of $157 billion following its latest funding round. The company raised $6.6 billion from key investors, including Microsoft and Nvidia, but is contending with substantial operational costs, particularly the need for additional GPUs to support large language model (LLM) training. OpenAI is currently exploring restructuring strategies to enhance financial growth and sustainability within the AI industry. |
|[Black Forest Labs, the startup behind Grok‚Äôs image generator, releases an API.](https://techcrunch.com/2024/10/03/black-forest-labs-the-startup-behind-groks-image-generator-releases-an-api/) |Black Forest Labs, the Andreessen Horowitz-backed startup behind the image generation component of xAI‚Äôs Grok assistant, has launched an API in beta ‚Äî and released a new model. |
|[DataPelago raises $47M to optimize hardware for analytical workloads.](https://siliconangle.com/2024/10/01/datapelago-raises-47m-optimize-hardware-analytical-workloads/) | LLMs depend on vast amounts of unstructured data for training, but this data requires extensive cleaning and processing before it becomes useful. Traditional data processing systems, which are based on CPUs and current software architectures, were not designed to handle the scale and complexity of such data, resulting in slow and costly data preparation that hinders AI development. To address these challenges, DataPelago has introduced a Universal Data Processing Engine, designed to overcome performance, cost, and scalability limitations, making AI development faster and more affordable.|
|[Google brings ads to AI Overviews as it expands AI‚Äôs role in search.](https://techcrunch.com/2024/10/03/google-brings-ads-to-ai-overviews-and-rolls-out-ai-organized-pages/) | Google will begin to show ads in AI Overviews, the AI-generated summaries it supplies for certain Google Search queries, and will add links to relevant web pages for some of those summaries as well. It‚Äôs also rolling out AI-organized search results pages in the U.S. this week.|
|[Nobel Physics Prize Awarded for Pioneering A.I. Research by 2 Scientists.](https://www.nytimes.com/2024/10/08/science/nobel-prize-physics.html) |Two scientists who contributed to the development of neural networks have been awarded the Nobel Prize in Physics, recognizing their groundbreaking work in advancing artificial intelligence and neural network technologies. |
|[Introducing the Message Batches API.](https://www.anthropic.com/news/message-batches-api) |Anthropic has introduced a new batch processing API that allows developers to submit batches of up to 10,000 queries at once. Each batch is processed within 24 hours and is 50% cheaper than standard API calls, making it a more efficient and cost-effective solution for handling non-time-sensitive tasks. |
|[Update on Reflection-70B.](https://glaive.ai/blog/post/reflection-postmortem) |A detailed post-mortem analysis of the highly anticipated Reflection-70B model revealed issues with its benchmark code, which inflated its performance claims. Although the team has since corrected these bugs, and the model's performance remains impressive, it does not quite reach the originally advertised levels. |
|[Four-legged robot learns to climb ladders.](https://techcrunch.com/2024/10/02/four-legged-robot-learns-to-climb-ladders/) | The proliferation of robots like Boston Dynamics‚Äô Spot has showcased the versatility of quadrupeds. These systems have thrived at walking up stairs, traversing small obstacles, and navigating uneven terrain. Ladders, however, still present a big issue ‚Äî especially given how ever present they are in factories and other industrial environments where the systems are deployed.|
|[Braintrust raises $36M Series A.](https://threadreaderapp.com/thread/1843653246612873701.html) |Braintrust, which helps Airtable, Brex, Notion, and Stripe build AI products, has raised $36M in a Series A led by a16z. |
|[Clout Kitchen raises $4.45M for AI gaming pal that mimics content creators.](https://venturebeat.com/games/clout-kitchen-raises-4-45m-for-ai-gaming-pal-that-mimics-content-creators/) |Clout Kitchen announced today that it has raised $4.45 million in its seed funding round, which it plans to put towards its new creator-powered products and experiences. The first of these is Backseat AI, an AI-powered buddy for League of Legends that the company created with Tyler ‚ÄúTyler1‚Äù Steinkamp ‚Äî an AI buddy that can take on the aspect of popular gaming content creators. Clout Kitchen plans to use its funding to expand its team and build out its shared internal tech stack. |
|[AlphaFold wins Nobel Prize in Chemistry.](https://www.nobelprize.org/prizes/chemistry/2024/press-release/) |Demis Hassabis, John Jumper, and David Baker were awarded the Nobel Prize in Chemistry for their groundbreaking work in protein folding, particularly through innovations like AlphaFold. Their contributions have significantly advanced the understanding of protein structures and their implications for science and medicine. |
|[OpenAI reducing dependency on Microsoft data centers.](https://www.tipranks.com/news/the-fly/openai-reducing-dependency-on-microsoft-data-centers-the-information-reports?utm_source=tldrai) | OpenAI is decreasing its reliance on Microsoft's data centers by acquiring its own compute infrastructure, allowing greater independence in its operations. Simultaneously, Microsoft is reducing its dependence on OpenAI as it develops and competes with its own AI products, signaling a shift in the dynamics of their partnership.|
|[TikTok parent company ByteDance has a tool that's scraping the web 25 times faster than OpenAI.](https://mashable.com/article/tiktok-parent-company-bytedance-web-crawler-25-times-faster-than-openai) |TikTok parent company ByteDance is amassing huge volumes of web data way faster than the other major web crawlers. ByteDance may be planning to release its own LLM, and is aggressively using its web crawler, "Bytespider," to scrape up data to train its models, Fortune reported. |
|[Sonair takes a cue from dolphins to build autonomous 3D vision without lidar.](https://techcrunch.com/2024/10/07/sonair-takes-a-cue-from-dolphins-to-build-autonomous-3d-vision-sans-lidar/) | Ultrasound is perhaps best known as the technology that enables noninvasive body scans and underwater communication and can help us park our cars. A young startup called Sonair out of Norway wants to employ it for something else: 3D computer vision used in autonomous hardware applications. |
|[Tesla‚Äôs head of vehicle programs jumps to Waymo ahead of robotaxi reveal.](https://techcrunch.com/2024/10/07/teslas-head-of-vehicle-programs-jumps-to-waymo-ahead-of-robotaxi-reveal/) | Tesla has lost a top executive to Waymo in the lead-up to the EV maker‚Äôs robotaxi unveiling on Thursday.|
|[Autism ABA Therapy with Llama.](https://ai.meta.com/blog/neuromnia-autism-aba-therapy-built-with-llama/) |Meta shares a use case of its Llama model for medical and therapeutic benefit. |
|[Uber‚Äôs EV ridehailing business is maturing.](https://www.theverge.com/2024/10/8/24264282/uber-green-ev-driver-mentor-chatgpt) |The company also announced it was adding ChatGPT to its driver app to handle EV questions. |
|[Amazon‚Äôs new AI guides can help shoppers find what they need.](https://www.theverge.com/2024/10/9/24266204/amazon-ai-shopping-guides-catalog-feature-availability) |The new AI Shopping Guides feature aims to help users find what they need with more informed product suggestions. |
|[TikTok joins the AI-driven advertising pack to compete with Meta for ad dollars.](https://digiday.com/marketing/tiktok-joins-the-ai-driven-advertising-pack-to-compete-with-meta-for-ad-dollars/) | TikTok's Smart+ is an AI-powered ad-buying tool designed to automate and optimize ad campaigns, giving marketers the option to selectively utilize its features for enhanced performance. The tool seeks to rival Meta's Advantage+ by offering streamlined ad management and improved return on investment (ROI). Early results indicate significant gains in ad spend efficiency and conversion rates, positioning TikTok as a strong contender in the digital advertising market.|
|[OpenAI partners with Cosmopolitan and Elle publisher Hearst.](https://www.engadget.com/ai/openai-partners-with-cosmopolitan-and-elle-publisher-hearst-180517248.html) |ChatGPT will provide citations and direct links to the company's content. |
|[Meta debuts new generative AI tools for creating video-based ads.](https://siliconangle.com/2024/10/08/meta-debuts-new-generative-ai-tools-creating-video-based-ads/) |Meta Platforms Inc. today said it‚Äôs rolling out a full-screen video tab on Facebook in recognition of the fact that its users spend more time watching videos than anything else on its platforms. |



## Resources
|Link|description|
|---|---|
|[Introducing the Open FinLLM Leaderboard.](https://huggingface.co/blog/leaderboard-finbench) | The Open FinLLM Leaderboard provides a dedicated evaluation platform designed specifically for financial language models. It emphasizes key financial tasks like predicting stock movements, analyzing sentiment, and extracting information from financial reports.|
|[Infinite-Fractal-Stream: Small Scale Proxy for Scaling-Centric ML.](https://github.com/cloneofsimo/infinite-fractal-stream) |Model testing in the image domain is often constrained by low-quality, small datasets like CIFAR10. This GitHub repository provides a tool that generates infinite, complex fractals in the form of images or videos, offering a new approach for testing models. |
|[Auto Jobs Applier.](https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk) | A highly viral repository leverages language models to automate the job application process, adding an extra layer of personalization to tailor applications for each position.|
|[Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models.](https://arxiv.org/abs/2410.03640v1) |This study uncovers major weaknesses in existing membership inference attacks (MIAs) used to detect unauthorized data usage in diffusion models. It introduces CopyMark, a more realistic benchmark for assessing MIAs on pre-trained models, providing unbiased datasets and fair evaluation techniques to improve the accuracy and reliability of these attacks. |
|[ImageFolder: Autoregressive Image Generation with Folded Tokens.](https://lxa9867.github.io/works/imagefolder/index.html) |ImageFolder is a semantic tokenizer developed to balance the trade-off between image reconstruction accuracy and generation quality in visual generative models, improving the overall performance of these models in both tasks. |
|[Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models.](https://github.com/whb139426/grounded-video-llm) | Grounded-VideoLLM is a novel Video-Large Language Model (Video-LLM) created to enhance the fine-grained understanding of specific moments in videos. By incorporating a temporal stream and discrete temporal tokens, the model more effectively captures the relationships between frames and timestamps, improving its ability to interpret and analyze detailed video content.|
|[Autoregressive Action Sequence Learning for Robotic Manipulation.](https://github.com/mlzxy/arp) |The Chunking Causal Transformer (CCT) is a new autoregressive architecture developed specifically for robotic manipulation tasks. It is designed to improve the model's ability to process sequential data efficiently, optimizing performance in real-time robotic control and manipulation scenarios. |
|[FacePoke.](https://github.com/jbilcke-hf/FacePoke) |FacePoke is a tool designed for rapid editing of faces in both videos and images, allowing users to make quick adjustments and modifications with ease. |
|[pipeline_parallel.py.](https://gist.github.com/3outeille/a3d4d91bb07af64c8f33d5aaee5145fe) |A large model training lead at Hugging Face has shared an excellent 200-line example of parallelism built from scratch, demonstrating efficient techniques for distributing computational tasks, which is particularly useful for large-scale model training. |
|[CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs.](https://arxiv.org/abs/2410.01999) | As language models become increasingly proficient at writing code, many existing benchmarks are approaching saturation. This paper proposes a more challenging benchmark designed to assess how well models perform on reasoning and code generation tasks, pushing beyond basic code-writing capabilities to evaluate deeper problem-solving skills.|
|[Intensify.](https://github.com/swairshah/Intensify) |Intensify is a Python package that allows you to colorize text based on intensity values. It provides an easy-to-use interface for applying color gradients to text or background colors in the terminal. |
|[Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality.](https://oooolga.github.io/JEDi.github.io/) | JEDi is a new metric built on the Joint Embedding Predictive Architecture (JEPA), designed to enhance evaluation accuracy with fewer samples. It better aligns with human assessments, making it a more robust alternative to the FVD (Fr√©chet Video Distance) metric for evaluating generative models.|
|[PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion.](https://arxiv.org/abs/2410.04939v1) | PRFusion and PRFusion++ are multimodal models developed to enhance place recognition in robotics and computer vision. By combining information from multiple sensory inputs, these models improve the accuracy and robustness of place recognition tasks, making them more effective in real-world applications.|
|[Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia.](https://arxiv.org/abs/2410.05270v1) | This paper presents ProLIP, a novel method for adapting vision-language models such as CLIP without adding additional parameters. ProLIP fine-tunes only the final projection matrix of the vision encoder, enabling it to deliver strong performance in few-shot classification tasks while maintaining the model's efficiency.|
|[ScienceAgentBench.](https://github.com/OSU-NLP-Group/ScienceAgentBench) | The benchmark code for the science agent test is designed to evaluate how effectively models can contribute to novel scientific discoveries. It provides a framework for assessing a model's ability to generate innovative ideas, solve complex scientific problems, and make meaningful advances in various scientific fields.|
|[Controlled Visual Generation.](https://arxiv.org/abs/2410.04671v1) | Controllable AutoRegressive Modeling (CAR) is a novel framework that introduces precise control mechanisms to pre-trained visual autoregressive models. This method enables more refined and targeted image generation by progressively improving control representations, allowing for fine-tuned outputs with reduced computational resources.|
|[PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners.](https://arxiv.org/abs/2410.04733v1) |PredFormer is a newly developed transformer-based method for spatiotemporal predictive learning, offering superior performance in both accuracy and efficiency compared to existing approaches. It excels in tasks that involve predicting changes over time and space, making it a powerful tool for various applications in fields like video analysis, weather forecasting, and robotics. |
|[GenSim2: Scaling Robotic Data Generation with Multi-modal and Reasoning LLMs.](https://gensim2.github.io/) |This paper presents an innovative approach to scaling robotic data collection by utilizing an enhanced, high-quality physics simulation dataset. The improved simulation environment enables more efficient data generation for training robots, offering a scalable and cost-effective method to collect large amounts of accurate and diverse data for robotic learning and development. |
|[Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration.](https://zhu-zhiyu.github.io/FLUX-IR/) |This project introduces a novel differential equation-based approach for image restoration. By leveraging mathematical models grounded in differential equations, the method enhances the ability to recover and restore degraded or noisy images, providing improved accuracy and performance in image restoration tasks. |
|[Pixtral 12B.](https://arxiv.org/abs/2410.07073) |The Mistral team has provided detailed insights into the training process and architecture of their vision-language model, which has demonstrated solid performance. The model incorporates advanced techniques for effectively integrating visual and linguistic data, allowing it to perform well on a variety of tasks that require understanding both images and text. The shared information includes specifics on data preprocessing, model architecture, and the optimization strategies employed during training. |
|[MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering.](https://arxiv.org/abs/2410.07095v1) |MLE-bench is a benchmark created to evaluate AI agents' capabilities in machine learning engineering. It includes a curated selection of 75 Kaggle competitions to test various skills, such as model training, dataset preparation, and optimization. The benchmark aims to assess how well AI agents can handle practical machine learning tasks, providing a comprehensive evaluation of their engineering proficiency. |
|[Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate.](https://arxiv.org/abs/2410.07167v1) |The Modality Integration Rate (MIR) is a new metric designed to evaluate the effectiveness of multi-modal pre-training in Large Vision Language Models. It measures how well different modalities, such as visual and textual data, are integrated during the pre-training process, offering insights into the model's ability to leverage information from both sources to improve performance on multi-modal tasks.|
|[Aria: First Open Multimodal Native MoE Model.](https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model) |A highly impressive new vision-language model has been released with open weights, code, and a comprehensive research report. It achieves performance on par with closed models for long video understanding, a challenge that has proven difficult for other open models like Pixtral and Molmo. This advancement represents a significant breakthrough in the field of open-source vision-language models. |
|[IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation.](https://github.com/yangling0818/itercomp) |IterComp is a new framework developed to enhance compositional text-to-image generation by integrating the strengths of multiple advanced diffusion models, including RPG, Stable Diffusion 3, and FLUX. By leveraging these models, IterComp improves the quality and coherence of generated images, especially when handling complex textual prompts that require multiple elements to be composed accurately. |
|[MatMamba.](https://github.com/scaledfoundations/matmamba) |MatMamba is a novel architecture for sequence processing, building upon the Mamba2 framework by incorporating a Matryoshka-like design. This approach allows a single model to be trained at multiple granularities, enabling the extraction of various smaller, nested submodels. This hierarchical structure enhances flexibility and efficiency, allowing the model to adapt to different levels of complexity and resource constraints. |
|[O1 replication progress report.](https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report.pdf) |Researchers from GAIR and NYU have been investigating the critical algorithmic advancements behind OpenAI's o1 model's exceptional performance. In their report, they introduce the concept of "Journey Learning" data, a novel approach that, when used in training, boosts math performance by 8% in absolute terms. This innovation highlights how specific data types can significantly enhance a model's reasoning and problem-solving abilities. |


## Perspectives
|Link|description|
|---|---|
|[Nuclear power for AI: what it will take to reopen Three Mile Island safely.](https://www.nature.com/articles/d41586-024-03162-2) |As Microsoft strikes a deal to restart a reactor at the notorious power station, Nature talks to nuclear specialists about the unprecedented process. |
|[‚ÄòIn awe‚Äô: scientists impressed by latest ChatGPT model o1.](https://www.nature.com/articles/d41586-024-03169-9) |The chatbot excels at science, beating PhD scholars on a hard science test. But it might ‚Äòhallucinate‚Äô more than its predecessors. |
|[Can AI have common sense? Finding out will be key to achieving machine intelligence.](https://www.nature.com/articles/d41586-024-03262-z) | The advent of LLMs has reopened a debate about the limits of machine intelligence ‚Äî and requires new benchmarks of what reasoning consists of.|
|[How your brain detects patterns in the everyday: without conscious thought.](https://www.nature.com/articles/d41586-024-03116-8) | Neurons in certain brain areas integrate ‚Äòwhat‚Äô and ‚Äòwhen‚Äô information to discern hidden order in events in real time.|
|[AI to the rescue: how to enhance disaster early warnings with tech tools.](https://www.nature.com/articles/d41586-024-03149-z) | Artificial intelligence can help to reduce the impacts of natural hazards, but robust international standards are needed to ensure best practice.|
|[Before Mira Murati‚Äôs surprise exit from OpenAI, staff grumbled its o1 model had been released prematurely.](https://fortune.com/2024/10/01/openai-sam-altman-mira-murati-gpt-4o-o1-chatgpt-turbulent-year/) | OpenAI's accelerated development and safety testing of its latest models, such as GPT-4o and o1, have led to internal friction, resulting in the departure of several senior staff members. The rapid pace of development has raised concerns about the thoroughness of the safety protocols, contributing to tensions within the organization.|
|[I Quit Teaching Because of ChatGPT.](https://time.com/7026050/chatgpt-quit-teaching-ai-essay/) |This professor resigned from teaching due to the widespread use of large language models (LLMs) like ChatGPT among students, which they felt undermined academic integrity and the traditional learning process. |
|[Three Subtle Examples of Data Leakage.](https://www.lesswrong.com/posts/rzyHbLZHuqHq6KM65/three-subtle-examples-of-data-leakage) |This article examines the risks of data leakage in machine learning, showcasing two real-world cases where improper data handling resulted in misleading model performance. In one instance, a company incorrectly filtered data by an upper price limit before modeling, while another organization encountered problems by not following a strict chronological split. The key lessons emphasize the critical need for detecting data leakage and understanding its detrimental effects on model accuracy and reliability. |
|[The real data wall is billions of years of evolution.](https://dynomight.substack.com/p/data-wall) | AI development is encountering a potential obstacle known as the "data wall," as language models near the limit of available textual data for training. This article challenges the idea of using human analogies to overcome these data constraints, pointing out that human intelligence results from vast amounts of data and long evolutionary processes, which differ fundamentally from AI. While human learning strategies may not directly translate to AI, this doesn't preclude progress through other modalities, such as multimodal data, or advancements in algorithms that could push AI capabilities further.|
|[AI will use a lot of energy. That's good for the climate.](https://climate.benjames.io/ai-go-brrr/) |AI data centers are significantly increasing the demand for clean, 24/7 energy, prompting tech giants to invest heavily in renewable and nuclear power solutions. This growing demand is expected to accelerate the cost reduction of clean energy technologies, driven by their learning rates. Over time, the energy needs of AI could lead to policy shifts and advancements in clean energy infrastructure, fostering faster adoption and development of sustainable energy sources. |
|[I want to break some laws too.](https://snats.xyz/pages/articles/breaking_some_laws.html) | This article explores the use of an automated data cleaning pipeline inspired by the Minipile method, which prunes datasets to deliver significant performance gains with only a fraction of the original data size. By leveraging techniques such as few-shot prompting and clustering, the approach streamlines dataset refinement for AI training, challenging traditional scaling laws by prioritizing data quality over quantity. The results indicate that using foundational datasets with more refined data can optimize AI model training, reducing resource consumption while boosting performance.|


# ML news: 

## Research
|Link|description|
|---|---|
|[PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting.](https://arxiv.org/abs/2409.17703v1) | The Parallel Gated Network (PGN) is an innovative architecture developed to address the challenges that traditional RNNs face in managing long-term dependencies. By shortening the information propagation path and incorporating gated mechanisms, it efficiently captures both past and present time step data.|
|[Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs.](https://arxiv.org/abs/2409.17778v1) |DoSSR is a diffusion-based super-resolution model that improves both performance and efficiency by utilizing pretrained diffusion models and initiating the process with low-resolution images. This approach accelerates the super-resolution process while maintaining high-quality results. |
|[MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models.](https://vainf.github.io/maskllm-project-page/) | MaskLLM is a pruning technique designed to decrease the computational load of large language models by introducing learnable sparsity. This method optimizes performance while maintaining model efficiency by selectively reducing the number of active parameters.|
|[Law of the Weakest Link: Cross Capabilities of Large Language Models.](https://www.llm-cross-capabilities.org/) | This project emphasizes the importance of evaluating large language models (LLMs) based on their combined abilities rather than focusing solely on individual skills. While most models are trained on specialized datasets that target specific capabilities, real-world tasks frequently demand a blend of expertise across different areas, known as cross-capabilities. This approach ensures that models are better suited to handle complex, multifaceted challenges.|
|[Scaling Optimal LR Across Token Horizon.](https://arxiv.org/abs/2409.19913) |This paper investigates how to adjust the learning rate as the amount of training data increases for a model. While LLaMA applied an exponential scaling factor of -0.28, the paper proposes using an exponential scaling factor of -0.33 for improved performance during training with larger datasets. |
|[Knowledge Graph Embedding by Normalizing Flows.](https://arxiv.org/abs/2409.19977v1) | This paper presents a novel approach to knowledge graph embedding by leveraging group theory to incorporate uncertainty into the process. This method allows for more nuanced and flexible representations of relationships within knowledge graphs, enhancing the model's ability to handle uncertain or ambiguous information.|
|[How AI is improving simulations with smarter sampling techniques.](https://news.mit.edu/2024/how-ai-improving-simulations-smarter-sampling-techniques-1002) | MIT CSAIL researchers created an AI-powered method for low-discrepancy sampling, which uniformly distributes data points to boost simulation accuracy.|

## News
|Link|description|
|---|---|
|[Apple not investing in OpenAI after all, new report says.](https://9to5mac.com/2024/09/27/apple-not-investing-in-openai-after-all-new-report-says/) |Apple is no longer planning to invest in OpenAI, according to a new report from The Wall Street Journal. This comes as OpenAI plans to close a $6.5 billion funding round next week, with investments possible from both Microsoft and Nvidia. |
|[Arcade AI raises 17M to transform commerce.](https://x.com/mnaficy/status/1839342011788439580) |Arcade AI, a generative product company that launched this week, has announced securing funding from prominent investors as it aims to develop its "prompt to product" system. This system enables the immediate creation of products that are ready for purchase, streamlining the process from concept to consumer. |
|[They stole my voice with AI.](https://www.jeffgeerling.com/blog/2024/they-stole-my-voice-ai) |Elecrow is suspected of using AI to clone a voice for promotional videos without consent. |
|[Amazon-backed Anthropic in talks to raise money at $40B valuation: report.](https://seekingalpha.com/news/4152344-amazon-backed-anthropic-in-talks-to-raise-money-at-40b-valuation-report) | Anthropic, a generative AI startup backed by Amazon and other major tech companies, is in discussions to raise additional funding that could potentially value the company at $40 billion.|
|[OpenAI Reportedly Slated for $500 Million SoftBank Investment.](https://www.pymnts.com/news/investment-tracker/2024/openai-reportedly-slated-for-500-million-softbank-investment/) | SoftBank is planning to invest $500 million in OpenAI's latest funding round, which could raise OpenAI's valuation to as high as $150 billion. Microsoft is also participating in this round, highlighting OpenAI's rapid 1,700% revenue growth, despite the company anticipating losses of around $5 billion.|
|[OpenAI Is Growing Fast and Burning Through Piles of Money.](https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html?unlocked_article_code=1.OU4.99io.cnWQdfPhAjl2&smid=url-share) |As the company looks for more outside investors, documents reviewed by The New York Times show consumer fascination with ChatGPT and a serious need for more cash. |
|[Altman reportedly asks Biden to back a slew of multi-gigawatt-scale AI datacenters.](https://www.theregister.com/2024/09/25/altman_5gw_dc/) | OpenAI CEO Sam Altman is calling on the Biden administration to establish AI data centers in the US that could consume up to five gigawatts of power, aiming to maintain US technological leadership over China. The proposal includes building several large-scale data centers across the country. Meanwhile, other tech giants, such as Microsoft and Amazon, are securing nuclear power deals to support their growing AI operations.|
|[Samsung's Galaxy Tab S10 Ultra and Galaxy Tab S10+ are tablets built for AI.](https://www.engadget.com/mobile/tablets/samsungs-galaxy-tab-s10-ultra-and-galaxy-tab-s10-are-tablets-built-for-ai-162633747.html) |Samsung is once again expanding its tablet lineup, and this time, the company is doing so with AI at the forefront. Today, Samsung revealed the Galaxy Tab S10 series, two models that it says are "built with AI enhancements available right out of the box."  |
|[Tesla Full Self Driving requires human intervention every 13 miles.](https://arstechnica.com/cars/2024/09/tesla-full-self-driving-requires-human-intervention-every-13-miles/) |It gave pedestrians room but ran red lights and crossed into oncoming traffic. |
|[OpenAI Dev Day 2024.](https://openai.com/devday/) |OpenAI's Dev Day 2024 featured several exciting announcements, including the introduction of vision model fine-tuning, a real-time API, prompt caching for faster responses, and model distillation for more efficient deployment of large models. These advancements aim to enhance the capabilities and performance of AI applications across various domains. |
|[Pika 1.5.](https://threadreaderapp.com/thread/1841143349576941863.html) |Pika has released version 1.5 with more realistic movement, big screen shots, and Pikaffects. |
|[Gov. Newsom vetoes California‚Äôs controversial AI bill, SB 1047.](https://techcrunch.com/2024/09/29/gov-newsom-vetoes-californias-controversial-ai-bill-sb-1047/) | Governor Gavin Newsom has vetoed SB 1047, a proposed bill intended to regulate AI development and enforce safety protocols for high-cost models. Newsom expressed concerns that the bill's broad application to all large, computation-heavy models was not the most effective method for regulating AI. However, he reaffirmed his commitment to AI safety by signing several other AI-related bills and consulting with experts to ensure thoughtful regulation in the future.|
|[OpenAI to remove non-profit control and give Sam Altman equity, sources say.](https://finance.yahoo.com/news/exclusive-openai-remove-non-profit-201413475.html) |hatGPT-maker OpenAI is working on a plan to restructure its core business into a for-profit benefit corporation that will no longer be controlled by its non-profit board, people familiar with the matter told Reuters, in a move that will make the company more attractive to investors. |
|[OpenAI's latest funding .](https://openai.com/index/scale-the-benefits-of-ai/) | OpenAI has secured $6.6 billion in new funding, bringing its post-money valuation to $157 billion. Notable investors in this round include Microsoft and Nvidia, with the funds aimed at further scaling AI development and innovation.|
|[Google adds a multi-functional quick insert key and new AI features to Chromebook Plus.](https://techcrunch.com/2024/10/01/google-adds-a-multi-functional-quick-insert-key-and-new-ai-features-to-chromebook-plus/) |Google is announcing new Chromebook models today with Samsung and Lenovo. With Samsung‚Äôs Galaxy Chromebook Plus model in particular, the company is also introducing a new multifunctional quick insert key. But Google doesn‚Äôt want to leave existing Chromebook users behind as it added new AI-powered features for existing devices. |
|[Brain-like Computers Tackle the Extreme Edge.](https://spectrum.ieee.org/neuromorphic-computing) |Start-up BrainChip announces a new chip design for a milliwatt-level AI inference |
|[AI Can Best Google‚Äôs Bot Detection System, Swiss Researchers Find.](https://decrypt.co/251107/ai-can-best-googles-bot-detection-system-swiss-researchers-find) |Researchers from ETH Zurich used advanced machine learning to solve 100% of Google's reCAPTCHAv2, designed to distinguish humans from bots. |
|[OpenAI Training Data to Be Inspected in Authors‚Äô Copyright Cases.](https://www.hollywoodreporter.com/business/business-news/openai-training-data-inspected-authors-copyright-case-1236011291/) |At a secure room in its San Francisco office, representatives for authors suing OpenAI will examine materials that were used to train its AI system. They allege copyrighted works were utilized without their consent or compensation. |
|[ByteDance will reportedly use Huawei chips to train a new AI model.](https://www.engadget.com/ai/bytedance-will-reportedly-use-huawei-chips-to-train-a-new-ai-model-154846749.html) |US export restrictions are preventing ByteDance from using NVIDIA chips. |
|[Announcing FLUX1.1 [pro] and the BFL API.](https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/) |FLUX1.1 [pro] has been released, offering six times faster generation speeds compared to its predecessor, alongside enhanced image quality and overall performance. The new beta BFL API introduces advanced customization options and competitive pricing, making it easier for developers to integrate FLUX‚Äôs capabilities. FLUX1.1 [pro] will be available across multiple platforms, providing greater scalability and efficiency for users and developers alike. |
|[OpenAI launches new ‚ÄòCanvas‚Äô ChatGPT interface tailored to writing and coding projects.](https://techcrunch.com/2024/10/03/openai-launches-new-canvas-chatgpt-interface-tailored-to-writing-and-coding-projects) |OpenAI introduced a new way to interact with ChatGPT on Thursday: an interface it calls ‚Äúcanvas.‚Äù The product opens a separate window, beside the normal chat window, with a workspace for writing and coding projects. Users can generate writing or code directly in the canvas, then highlight sections of the work to have the model edit. Canvas is rolling out in beta to ChatGPT Plus and Teams users on Thursday, and Enterprise and Edu users next week. |
|[Anthropic hires OpenAI co-founder Durk Kingma.](https://techcrunch.com/2024/10/01/anthropic-hires-openai-co-founder-durk-kingma/) |Durk Kingma, one of the lesser-known co-founders of OpenAI, today announced that he‚Äôll be joining Anthropic. |
|[OpenAI unveils easy voice assistant creation at 2024 developer event.](https://arstechnica.com/information-technology/2024/10/openai-unveils-easy-voice-assistant-creation-at-2024-developer-event/) | Altman steps back from the keynote limelight and lets four major API additions do the talking.|


## Resources
|Link|description|
|---|---|
|[üöÄ FlowTurbo.](https://github.com/shiml20/flowturbo) |FlowTurbo is a method developed to accelerate the sampling process in flow-based models while maintaining high-quality outputs. It achieves faster results without compromising the precision or performance of the model. |
|[Transformer4SED.](https://github.com/cai525/transformer4sed) |This repository presents the Prototype-based Masked Audio Model, which enhances sound event detection by leveraging unlabeled data more effectively. The method generates pseudo labels through a Gaussian mixture model, which directs the training of a Transformer-based audio model for improved performance. |
|[VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models.](https://github.com/microsoft/vptq) |Vector Post-Training Quantization is a technique aimed at enabling ultra-low-bit quantization for large language models, optimizing memory and storage efficiency during deployment without significantly compromising performance. |
|[LightAvatar: Efficient Head Avatar as Dynamic NeLF.](https://github.com/mingsun-tse/lightavatar-tensorflow) |LightAvatar is a head avatar model that improves rendering speed and efficiency using neural light fields (NeLFs). |
|[Separating code reasoning and editing.](https://aider.chat/2024/09/26/architect.html) |Aider has significantly enhanced the performance of general-purpose code editing by employing o1 as the architect and DeepSeek as the writer. This collaboration streamlines the process, leading to more efficient and accurate code generation. |
|[Heralax/Mistrilitary-7b.](https://huggingface.co/Heralax/Mistrilitary-7b) |This model was trained using army handbooks and incorporates deep, specialized knowledge that is uncommon in fine-tuned models. This unique training approach allows it to possess a rare level of expertise in military-related tasks and information. |
|[Developing a go bot embedding ichiban Prolog.](https://rogersm.net/posts/developing-a-go-bot-embedding-ichiban-prolog/) |Ichiban Prolog was integrated into Hellabot, a Go-based IRC bot, to eliminate the need for recompiling when adding new triggers. This integration enables dynamic Prolog code execution, allowing users to adjust the bot's logic in real time. Future enhancements could focus on minimizing interpreter setup overhead and shifting more of the bot's logic into Prolog for greater flexibility and efficiency. |
|[Emu 3 open early fusion multimodal model.](https://emu.baai.ac.cn/about) | Emu 3 is a next-token prediction model that surpasses SDXL in image synthesis, LlaVa-1.6 in image understanding, and OpenSora 2 in video generation. With 9 billion parameters, Emu 3 is trained on these tasks in an interleaved manner, similar to Gemini, making it highly versatile and effective across multiple domains.|
|[LOTUS: Diffusion-based Visual Foundation Model for High-quality Dense Prediction.](https://lotus3d.github.io/) |Using pretrained diffusion models for tasks like depth estimation has become highly popular and effective. This work demonstrates how certain previous methods contained minor inaccuracies and presents improvements that not only boost performance but also significantly simplify the overall modeling process. |
|[Revisit Anything: Visual Place Recognition via Image Segment Retrieval.](https://arxiv.org/abs/2409.18049v1) | SegVLAD is a method for visual place recognition that emphasizes the analysis of image segments instead of relying on entire images. This approach enhances recognition accuracy by focusing on distinctive parts of the scene, making it more robust in various environments.|
|[LeanRL - Turbo-implementations of CleanRL scripts.](https://github.com/pytorch-labs/leanrl) |LeanRL is a lightweight library consisting of single-file, pytorch-based implementations of popular Reinforcement Learning (RL) algorithms. The primary goal of this library is to inform the RL PyTorch user base of optimization tricks to cut training time by half or more. |
|[E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding.](https://polyu-chenlab.github.io/etbench/) | E.T. Bench is a newly developed benchmark created to assess the performance of video language models on fine-grained, event-level tasks. Unlike earlier benchmarks that emphasize video-level questions, E.T. Bench spans a variety of time-sensitive tasks across multiple domains, providing a more detailed evaluation of model capabilities.|
|[MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning.](https://arxiv.org/abs/2409.20566) | Apple is continuing to strengthen its in-house AI capabilities by developing a robust multimodal foundation model. This initiative is part of Apple's broader efforts to integrate advanced AI technologies across its ecosystem, supporting tasks that span text, image, and other data modalities for enhanced user experiences.|
|[The Perfect Blend: Redefining RLHF with Mixture of Judges.](https://arxiv.org/abs/2409.20370) | Meta has introduced an impressive new paper detailing the use of a mixture of judges models to effectively conduct multi-task reinforcement learning with human feedback (RLHF) during post-training. This approach significantly enhances the final performance of models across various benchmarks, demonstrating superior results compared to previous methods.|
|[A Survey on the Honesty of Large Language Models.](https://arxiv.org/abs/2409.18786v1) |This survey explores the honesty of large language models (LLMs), a crucial aspect in aligning AI with human values. It addresses challenges such as models confidently providing incorrect answers and the difficulty in distinguishing between what the model knows and what it doesn't. The review highlights these obstacles as key areas for improving the reliability and trustworthiness of LLMs. |
|[LexEval: A Comprehensive Benchmark for Evaluating Large Language Models in Legal Domain.](https://github.com/cshaitao/lexeval) |LexEval is a benchmark created to evaluate large language models (LLMs) specifically in the legal domain. Recognizing the critical need for accuracy, reliability, and fairness in legal applications, LexEval provides a framework for assessing the strengths and limitations of LLMs when applied to legal tasks, ensuring they meet the rigorous demands of the field. |
|[Perceptual Compression (PerCo).](https://github.com/Nikolai10/PerCo) | PerCo (SD) is a novel perceptual image compression technique built on Stable Diffusion v2.1, specifically designed for ultra-low bit ranges. This method leverages the power of diffusion models to achieve high-quality image compression at significantly reduced bitrates, optimizing storage and transmission without sacrificing visual fidelity.|
|[nvidia/NVLM-D-72B.](https://huggingface.co/nvidia/NVLM-D-72B) |Nvidia conducted a thorough ablation study on various methods of incorporating images into a language model. The results showed that the LlaVa concatenation approach outperformed the other methods, proving to be the most effective for integrating visual information into language models. |
|[ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification.](https://arxiv.org/abs/2409.20081v1) |This paper introduces a new method called Prompt-guided Feature Disentangling (ProFD) to tackle occlusion challenges in person Re-Identification (ReID) tasks. ProFD helps separate relevant features from occluded or irrelevant ones, improving the accuracy and robustness of ReID models when identifying individuals in complex or obstructed environments. |
|[Local File Organizer: AI File Management Run Entirely on Your Device, Privacy Assured.](https://github.com/NexaAI/nexa-sdk/tree/main/examples/local_file_organization) | This tool utilizes Llama 3.2 3B and Llava-1.6 to intelligently organize files on your computer into logical sections based on their content. By analyzing the data within the files, it categorizes and arranges them for easier navigation and more efficient file management.|
|[Posterior-Mean Rectified Flow:
Towards Minimum MSE Photo-Realistic Image Restoration.](https://pmrf-ml.github.io/) |Posterior-Mean Rectified Flow (PMRF) is a cutting-edge algorithm designed for photo-realistic image restoration. It improves the quality of restored images by refining the flow of information, resulting in highly accurate and visually appealing reconstructions. |
|[RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models.](https://github.com/shuhao02/routerdc) | RouterDC is an innovative method designed to enhance collaboration between multiple large language models (LLMs) through query-based routing. It utilizes contrastive learning to determine the most suitable model for each query, leading to improved performance compared to existing routing techniques. This approach optimizes model selection, ensuring more accurate and efficient responses.|
|[Distributed Training of Deep Learning models .](https://vaibhawvipul.github.io/2024/09/29/Distributed-Training-of-Deep-Learning-models-Part-~-1.html) |This post provides an excellent introduction to the challenges and algorithms involved in distributed training for modern deep learning models. It explores the difficulties and bottlenecks of training models that are too large for a single GPU, including issues like communication overhead, synchronization, and memory limitations, while also discussing key techniques to overcome these obstacles. |
|[ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation.](https://comfygen-paper.github.io/) | Instead of directly generating an image from a prompt, the authors created a workflow using a comfy UI node-based system to guide the image generation process. This approach significantly enhanced the final output quality, allowing for greater control and precision in the generation pipeline.|
|[KnobGen.](https://github.com/aminK8/KnobGen) |KnobGen is a new framework developed to make sketch-based image generation more accessible to users of varying skill levels. By offering intuitive controls and simplified tools, KnobGen allows users to generate high-quality images from sketches, regardless of their artistic expertise. |
|[Tiny Test Models.](https://huggingface.co/blog/rwightman/timm-tiny-test) |AI researcher Ross Wightman has released a collection of models trained on ImageNet-1k that are remarkably small, with fewer than 1 million parameters. Despite their compact size, these models perform reasonably well and are designed to be easy to fine-tune, making them highly accessible for various applications where model efficiency is critical. |
|[entropix.](https://github.com/xjdr-alt/entropix) |Entropy-based sampling and parallel Chain of Thought (CoT) decoding are promising strategies for advancing reasoning models to match |
|[Concordia.](https://github.com/google-deepmind/concordia) | DeepMind's Concordia repository enables the simulation of social interactions between individuals and groups at a reasonable scale. This platform allows researchers to model complex social behaviors, study group dynamics, and explore various interaction scenarios in a controlled, scalable environment.|

## Perspectives
|Link|description|
|---|---|
|[The Intelligence Age.](https://ia.samaltman.com/) |AI is set to enhance human abilities, empowering us to accomplish tasks that are currently beyond imagination. With the help of deep learning and more powerful computational tools, AI will drive innovations such as personalized assistants, learning tutors, and healthcare advisors. The emphasis should be on ensuring AI is widely accessible while addressing its potential risks, creating a path toward shared prosperity in the era of intelligent systems. |
|[How AlphaChip transformed computer chip design.](https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design) |AlphaChip is a reinforcement learning model that dramatically speeds up and improves chip design, creating layouts that surpass human capabilities. It produces optimized chip designs, such as those used in Google's TPUs, in just hours instead of weeks. This AI-powered approach has wide-ranging applications, benefiting not only Google's hardware but also external companies like MediaTek. |
|[AI pareidolia: Can machines spot faces in inanimate objects?](https://news.mit.edu/2024/ai-pareidolia-can-machines-spot-faces-in-inanimate-objects-0930) | New dataset of ‚Äúillusory‚Äù faces reveals differences between human and algorithmic face detection, links to animal face recognition, and a formula predicting where people most often perceive faces.|
|[Table Extraction using LLMs: Unlocking Structured Data from Documents.](https://nanonets.com/blog/table-extraction-using-llms-unlocking-structured-data-from-documents/) | This article discusses how large language models (LLMs) are transforming table extraction from complex documents, surpassing the limitations of traditional methods such as OCR, rule-based systems, and machine learning. LLMs offer greater flexibility and contextual comprehension, significantly improving accuracy in handling varied and intricate table structures. While challenges like hallucination and high computational demands remain, the integration of traditional techniques with LLMs currently provides the most effective solution for automated table extraction.|
|[The Other Bubble.](https://www.wheresyoured.at/saaspocalypse-now/) | Microsoft considered diverting its US-based server power to GPUs for AI purposes but ultimately abandoned the idea. Major tech companies like Microsoft, Google, and Amazon are making significant investments in AI, yet they continue to see underwhelming returns from generative AI applications. The industry's reliance on SaaS and the integration of AI tools, which frequently offer limited practical value while incurring substantial costs, underscores an increasing urgency to sustain growth in a slowing market.|
|[AI's Privilege Expansion.](https://www.digitalnative.tech/p/ais-privilege-expansion) |AI is quickly broadening access to services that were once expensive and difficult to obtain, such as education, healthcare, and personal styling. Generative AI models like ChatGPT offer affordable, personalized support by acting as tutors, healthcare advisors, and stylists, reducing the need for costly human professionals. This transformation democratizes access to high-end services, making them more widely available to the general public at a significantly lower cost. |
|[Behind OpenAI‚Äôs Audacious Plan to Make A.I. Flow Like Electricity.](https://www.nytimes.com/2024/09/25/business/openai-plan-electricity.html) | OpenAI CEO Sam Altman has proposed a global initiative to construct data centers and chip factories to drive advanced AI development. While Altman initially aimed for trillions in funding, he has now scaled back to targeting hundreds of billions. The plan envisions partnerships with global tech giants and governments, though it faces significant regulatory and logistical hurdles. Despite early skepticism, ongoing discussions suggest potential expansions across the US, Europe, and Asia to significantly increase computing power for AI advancements.|
|[Devs gaining little (if anything) from AI coding assistants.](https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html) |Code analysis firm sees no major benefits from AI dev tool when measuring key programming metrics, though others report incremental gains from coding copilots with emphasis on code review. |
|[Negligence Liability for AI Developers.](https://www.lawfaremedia.org/article/negligence-liability-for-ai-developers) |This article advocates for a negligence-based approach to AI accountability, emphasizing the human factors and responsibilities behind AI systems. It critiques existing regulatory frameworks for neglecting the role of AI developers and highlights California's AI safety bill as a promising example. The article also delves into the complexities of defining "reasonable care" in AI development and the potential consequences of classifying AI developers as professionals, raising important questions about the standards and obligations they should meet. |
|[I am tired of AI.](https://www.ontestautomation.com/i-am-tired-of-ai/) | The author expresses frustration with the widespread marketing and overuse of AI, especially in fields like software testing and conference proposals. They argue that AI tools often prioritize speed at the expense of quality and fail to offer the unique insights that come from human-generated work. While acknowledging some useful applications of AI, the author criticizes the increasing amount of mediocre AI-produced content, seeing it as a detriment to innovation and depth in these areas.|
|[The Four Short Term Winners of AI.](https://www.whitenoise.email/p/the-four-short-term-winners-of-ai) |The global AI arms race is primarily driven by Big Tech companies, chipmakers such as NVIDIA, intellectual property lawyers, and the Big 4 consulting firms. These key players are competing to secure technological dominance, resources, and expertise in AI development, shaping the future of the industry through their influence and innovations. |
|[The Art of the OpenAI Deal.](https://spyglass.org/the-art-of-the-openai-deal/) | OpenAI's revenue soared to $300 million in August, with the company forecasting $3.7 billion in annual sales for this year and $11.6 billion for next year. However, it is facing a $5 billion annual loss. This rapid growth has been driven primarily by the widespread success of ChatGPT, which generates the majority of its revenue. Despite this momentum, OpenAI is actively seeking additional investors to cover its high operational costs and work towards becoming a profitable enterprise.|
|[What comes after?](https://www.strangeloopcanon.com/p/ai-bill-vetoed-whats-next) |California Governor Gavin Newsom has vetoed SB 1047, a bill aimed at regulating large AI models. He stressed the importance of creating evidence-based regulations and cautioned that overly restrictive rules could hinder innovation. Instead, Newsom plans to collaborate with experts, including Dr. Fei-Fei Li, to develop empirical, science-driven guidelines that balance safety and progress in AI development. |
|[Sorry, GenAI is NOT going to 10x computer programming.](https://garymarcus.substack.com/p/sorry-genai-is-not-going-to-10x-computer) |Recent studies indicate that generative AI has not yet delivered the expected 10x improvement in coding productivity. While AI tools can assist with code generation and streamline certain tasks, the overall productivity gains have been more modest than initially projected, with challenges such as integration, context understanding, and debugging limiting the full potential of these technologies in real-world coding environments. |


# ML news: Week 23 - 29 September

## Research
|Link|description|
|---|---|
|[Moshi: a speech-text foundation model for real-time dialogue.](https://kyutai.org/Moshi.pdf) |presents a full-duplex spoken dialogue framework and a speech-text basis paradigm; they also present several system components; Helium is a 7B parameter text LLM; Mimi is a semantic-acoustic neural audio code that achieves cutting-edge audio quality performance; and a hierarchical multi-stream architecture that can produce speech-to-speech from any given dialog.|
|[Training Language Models to Self-Correct via Reinforcement Learning.](https://arxiv.org/abs/2409.12917) |creates a multi-turn online reinforcement learning system that is fully based on self-generated data in order to enhance an LLM's ability to self-correct; It is demonstrated that SFT has a distribution mismatch between training data and model responses and is inefficient at learning self-correction; suggests a two-stage method that, when applied to the Gemini 1.0 Pro and 1.5 Flash models, achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1%, respectively, on the MATH and HumanEval benchmarks. The first stage of the method optimizes correction behavior, and the second uses a reward bonus to amplify self-correction during training. |
|[On the Diagram of Thought.](https://arxiv.org/abs/2409.10038) |strengthens LLMs' capacity for reasoning through rigorous mathematics; DAT represents iterative reasoning in LLM as the building of a directed acyclic graph; it combines propositions, criticisms, refinement, and verification into a single DAG structure; this enables DoT to capture sophisticated logical deduction that is beyond the scope of linear or tree-based methods |
|[To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning.](https://arxiv.org/abs/2409.12183) | examines which tasks benefit most from chain-of-thought (CoT) prompting; following a meta-analysis of over 100 papers and multiple evaluations, it concludes that CoT leads to significant performance gains, mostly on math and logic tasks; the majority of the CoT gain is derived from improving symbolic execution, although a symbolic solver performs better than it.|
|[A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B.](https://arxiv.org/abs/2409.11055) | examines how instruction-tuned LLMs perform on models ranging from 7B to 405B using different quantization techniques. The main conclusions are that: 1) one should quantize a larger LLM to a similar size because a smaller FP16 LLM typically performs better across most benchmarks; 2) performance varies significantly with different quantization techniques, model size, and bit-width, with weight-only methods frequently producing better results in larger models; and 3) task difficulty does not significantly impact accuracy degradation due to quantization.|
|[Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning.](https://arxiv.org/abs/2409.12618) | uses an inner dialogue agent to act as a guide to dynamically adjust reasoning paths, allowing adaptive cross-path exploration and improving response accuracy. This makes it different from CoT and ToT, which are both rigid processes, in that its prompt generation is a dynamic process that allows it to adapt. suggests the Iteration of Thought (IoT) framework to improve the LLM responses and reasoning capabilities with adaptive reasoning paths.|
|[Schrodinger's Memory: Large Language Models.](https://arxiv.org/abs/2409.10482) | utilizes the Universal Approximation Theorem to describe how LLMs store memory. Additionally, it suggests a novel method for assessing LLM performance by contrasting the memory capacities of various models; the Transformer architecture serves as a dynamic fitting UAT model with a high degree of adaptability in fitting inputs, allowing LLMs to recall the entirety of the content with the least amount of input data.|
|[Jailbreaking Large Language Models with Symbolic Mathematics.](https://arxiv.org/abs/2409.11445) |generates mathematically encoded prompts using GPT-4o, which is a useful jailbreaking strategy; the average attack success rate over 13 state-of-the-art is 73.6%. This indicates that current safety training systems are not able to generalize to mathematically encoded inputs. |
|[Iterative Object Count Optimization for Text-to-image Diffusion Models.](https://ozzafar.github.io/count_token/) |Generating a specific number of objects with a diffusion model is often a difficult task. This work introduces a counting token that enables the model to more accurately produce either a few or many instances of a given object. While it's not flawless and is based on the original stable diffusion model, it significantly outperforms existing methods. |
|[A Controlled Study on Long Context Extension and Generalization in LLMs.](https://arxiv.org/abs/2409.12181v2) |Researchers have created a standardized evaluation protocol designed to compare different methods for extending language models to effectively handle long document contexts. |
|[MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning.](https://arxiv.org/abs/2409.12147v1) |MAgICoRe is a novel strategy designed to enhance reasoning in large language models by tackling challenges in refinement processes. It classifies problems based on difficulty, applying straightforward strategies to simpler tasks and employing multi-agent iterative refinement for more complex ones. |
|[The Impact of Element Ordering on LM Agent Performance.](https://arxiv.org/abs/2409.12089v2) |The sequence in which UI elements are displayed greatly affects agent performance in virtual environments. Randomizing the order of elements can decrease performance as much as completely removing all visible text.|
|[Larger and more instructable language models become less reliable.](https://www.nature.com/articles/s41586-024-07930-y) | Scaling up and shaping up large language models increased their tendency to provide sensible yet incorrect answers at difficulty levels humans cannot supervise, highlighting the need for a fundamental shift in artificial intelligence design towards reliability.|
|[SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs and Agents.](https://arxiv.org/abs/2409.15817) |This work addresses the limitations of LLMs in drug discovery by integrating an advanced Retrieval-Augmented Generation (RAG) system for more accurate answers and combining LLMs with external tools to create an automatic target dossier. The result is a production-ready dossier with comprehensive data, summarized into a PDF and PowerPoint presentation. |
|[Self-Explainable AI.](https://arxiv.org/abs/2409.16693v1) |In the field of explainable AI, there is a strong focus on developing self-explainable models, which offer a more principled approach compared to post-hoc methods that attempt to interpret decisions after they have been made by opaque models. Despite its potential, this line of research often faces challenges such as lack of reproducibility, difficulties in comparison, and inconsistent standards. To address these issues, we introduce CaBRNet, an open-source, modular, and backward-compatible framework for Case-Based Reasoning Networks |



























































