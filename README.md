# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

# ON WORKING

# ML news: 

## Research
|Link|description|
|---|---|
|[Prover-Verifier Games improve legibility of LLM outputs.](https://arxiv.org/abs/2407.13692) | Iteratively trains helpful provers to produce correct solutions accepted by the verifier, sneaky provers to produce incorrect solutions that trick the verifier, and small verifiers to predict the correctness of solutions; this process helps train models that can produce text that is clear and accurate for both AI and human readers, which results in more reliable systems.|
|[SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.](https://arxiv.org/abs/2407.09025) |outlines a method for efficiently encoding spreadsheets to maximize an LLM's comprehension and reasoning skills; creates a sheet compressor that efficiently compresses and encodes spreadsheets using inverse index translation, structural anchor-based compression, and data-format-aware aggregation modules; in GPT-4's in-context learning, it improves performance in spreadsheet table detection by 25.6%. |
|[Context Embeddings for Efficient Answer Generation in RAG.](https://arxiv.org/abs/2407.09252) |presents a useful context compression technique that shortens long contexts and accelerates generation times in RAG systems. Long contexts are condensed into a limited number of context embeddings, allowing for varying compression rates that balance generation quality against decoding time. This technique maintains high performance while reducing inference times by up to 5.69 x and GFLOPs by up to 22x. |
|[Weak-to-Strong Reasoning.](https://arxiv.org/abs/2407.13647) | reports that strong models can automatically refine their training data without explicitly being trained to do so; shows how to use weak supervision to elicit strong reasoning capabilities in LLMs without relying on human annotations or advanced models; permits extending a model's learning scope and scaling performance on reasoning. |
|[Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969) | concludes that many state-of-the-art LLMs can be jailbroken by simply rephrasing an LLM request into the past tense. For instance, "How to make a Molotov cocktail?" can be rephrased as "How did people make a Molotov cocktail?" The success rate of such requests can increase from 1% to 88% when using direct requests on GPT-4o.|
|[NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?](https://arxiv.org/abs/2407.11963) |presents the Ancestral Trace Challenge, which raises the bar for complex logical reasoning and is typical of real-world long-context tasks. Their findings imply that current LLMs struggle to handle reasoning tasks with complex logical relationships, even with texts shorter than 2K tokens. They also propose a framework (NeedleBench) of progressively challenging tasks to assess the long-context retrieval and reasoning capabilities of LLMs. |
|[Distilling System 2 into System 1.](https://arxiv.org/abs/2407.06023v2) | explores self-supervised ways for extracting high-quality outputs from System 2 methods and then refines System 1 to fit the System 2 method's predictions without creating intermediate steps; extracting reasoning from System 1 reduces the cost of inference.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[GPs use AI to boost cancer detection rates in England by 8%.](https://www.theguardian.com/society/article/2024/jul/21/gps-use-ai-to-boost-cancer-detection-rates-in-england-by-8) |‘C the Signs’ artificial intelligence program scans medical records to increase likelihood of spotting cancers |
|[Artificial Agency raises $16M to use AI to make NPCs feel more realistic in video games.](https://techcrunch.com/2024/07/18/artificial-agency-raises-video-game-npcs-ai/) | A group of former Google DeepMind researchers has created an AI behavior engine that aims to transform traditional video games into a more dynamic experience by improving how non-playable characters (NPCs) behave and interact with gamers.|
|[Inside the United Nations’ AI policy grab.](https://www.politico.eu/article/united-nations-artificial-intelligence-policy-report-carme-artigas-paolo-benanti-mira-murati/) |The United Nations wants to create an artificial intelligence forum to rule them all.  |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks.](https://arxiv.org/abs/2407.12994) | a set of quick engineering techniques for various NLP applications.|
|[Exploring Advanced Large Language Models with LLMsuite.](https://arxiv.org/abs/2407.12036) |provides helpful advice for using and assessing LLMs in development; approaches discussed include parameter-efficient techniques, RAG, and ReAct. |
|[Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures.](https://www.arxiv.org/abs/2407.09468) |offers a graphical taxonomy and detailed tour to the most recent developments in non-Euclidean machine learning. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[‘Google says I’m a dead physicist’: is the world’s biggest search engine broken?](https://www.theguardian.com/technology/article/2024/jul/20/google-is-the-worlds-biggest-search-engine-broken) |For decades now, anyone who’s wanted to know everything about anything has asked Google. But is the platform losing its edge – and can we still trust it to tell us the truth? |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

# ML news: ML news: Week 15 - 21 July

## Research
|Link|description|
|---|---|
|[RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs.](https://arxiv.org/abs/2407.02485v1) |demonstrates how a Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. It also introduces a new instruction fine-tuning framework to perform effective context ranking and answering generation to enhance an LLM's RAG capabilities. This framework makes use of a small ranking dataset to outperform existing expert ranking models. |
|[Mixture of A Million Experts.](https://arxiv.org/abs/2407.04153) |aims to decouple computational cost from parameter count by efficiently routing to a large number of tiny experts through a learned index structure used for routing. It shows superior efficiency compared to dense FFW, coarse-grained MoEs, and Product Key Memory (PKM) layers. introduces a parameter-efficient expert retrieval mechanism that uses the product key technique for sparse retrieval from a million tiny experts. |
|[Reasoning in Large Language Models: A Geometric Perspective.](https://arxiv.org/abs/2407.02678) | establishes a relationship between the expressive power of LLMs and the density of their self-attention graphs; their analysis shows that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. investigates the reasoning of LLMs from a geometrical perspective; reports that a higher intrinsic dimension implies greater expressive capacity of the LLM.|
|[Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps.](https://arxiv.org/abs/2407.07071) |Contextual Hallucinations Mitigation in LLMs: This paper presents a novel approach that both detects and reduces contextual hallucinations in LLMs (e.g., reduces by 10% in the XSum summarization task). It does this by building a hallucination detection model based on input features provided by the ratio of attention weights on the context vs. newly generated tokens (for each attention head). The theory behind this approach is that contextual hallucinations are related to the degree to which an LLM attends to the contextual information provided. Additionally, they suggest a decoding strategy that mitigates contextual hallucinations based on their detection method, and this can be applied to other models without requiring retraining. |
|[RouteLLM.](https://arxiv.org/abs/2406.18665v2) |uses human preference data and data augmentation techniques in its training framework to improve performance and reduce costs by over two times in some cases, all while maintaining response quality. It suggests effective router models to dynamically choose between stronger and weaker LLMs during inference to achieve a balance between cost and performance. |
|[Learning to (Learn at Test Time): RNNs with Expressive Hidden States.](https://arxiv.org/abs/2407.04620) | suggests new layers for sequence modeling that have linear complexity and an expressive hidden state; defines a hidden state as an ML model that can update even when tested; a two-layer MLP-based hidden state combined with a linear model is found to match or outperform baseline models such as Mamba, Transformers, and contemporary RNNs; the linear model is faster than Mamba in wall-clock time and matches Transformer at 8k context. |
|[Physicochemical graph neural network for learning protein–ligand interaction fingerprints from sequence data.](https://www.nature.com/articles/s42256-024-00847-1) | Predicting the binding affinity between small-molecule ligands and proteins is a key task in drug discovery; however, sequence-based methods are often less accurate than structure-based ones. Koh et al. develop a graph neural network using physicochemical constraints that discovers interactions between small molecules and proteins directly from sequence data and that can achieve state-of-the-art performance without the need for costly, experimental 3D structures.|
|[Generic protein–ligand interaction scoring by integrating physical prior knowledge and data augmentation modelling.](https://www.nature.com/articles/s42256-024-00849-z) |Machine learning can improve scoring methods to evaluate protein–ligand interactions, but achieving good generalization is an outstanding challenge. Cao et al. introduce EquiScore, which is based on a graph neural network that integrates physical knowledge and is shown to have robust capabilities when applied to unseen protein targets. |
|[MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis.](https://arxiv.org/abs/2407.07614v1) | Semantic Vision-Language Integration Expert (SemVIE) is a feature of MARS, a novel text-to-image (T2I) generation system.|
|[OpenDiLoCo.](https://www.primeintellect.ai/blog/opendiloco) |Prime Intellect duplicated the DeepMind technique known as Distributed Low-Communication (DiLoCo). It preserves GPU consumption while enabling cross-datacenter training. |
|[gpu.cpp.](https://github.com/AnswerDotAI/gpu.cpp) | A new lightweight and portable library for WebGPU-based low-level GPU computations has been launched by Answer AI. Writing cross-GPU kernels is possible with it, and portable instructions are provided.|
|[ViTime: A Visual Intelligence-based Foundation Model for Time Series Forecasting.](https://github.com/ikeyang/vitime) |Rather than using conventional numerical data fitting, the foundation model for time series forecasting (TSF) called ViTime makes use of visual intelligence. |
|[Gradient Boosting Reinforcement Learning.](https://arxiv.org/abs/2407.08250v1) |The benefits of Gradient Boosting Trees (GBT) are applied to reinforcement learning using Gradient-Boosting RL (GBRL). |
|[SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.](https://arxiv.org/abs/2407.09025) | An excellent study explaining how to convert a spreadsheet into a suitable representation for a contemporary LLM. Q/A, formatting, and other data operations can be done using this.|
|[LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models.](https://arxiv.org/abs/2407.08966v1) |Label-focused A novel technique for out-of-distribution (OOD) detection in Vision-Language Models such as CLIP is Automated Prompt Tuning (LAPT). |
|[Prover-Verifier Games improve legibility of language model outputs.](https://openai.com/index/prover-verifier-games-improve-legibility/) |In order to enable a weak model to grade content reliably, OpenAI trained a strong model to produce more legible text. The company discovered that this improved overall readability generally. |
|[Temporally Consistent Stereo Matching.](https://arxiv.org/abs/2407.11950v1) | By guaranteeing temporal consistency, researchers present a novel technique for video stereo matching that improves depth estimation.|
|[Patch-Level Training for Large Language Models.](https://arxiv.org/abs/2407.12665v1) |To increase training efficiency for big language models, researchers suggest patch-level training. |


## News
|Link|description|
|---|---|
|[Elon Musk promises ‘battle in court’ over EU’s crackdown on X’s blue checks.](https://www.theguardian.com/technology/article/2024/jul/12/eu-regulators-warns-x-may-face-fines-for-deceptive-blue-tick-system) | Regulators’ findings suggest social network breached Digital Services Act and could be fined 6% of global turnover|
|[AI prompts can boost writers’ creativity but result in similar stories, study finds.](https://www.theguardian.com/technology/article/2024/jul/12/ai-prompts-can-boost-writers-creativity-but-result-in-similar-stories-study-finds) | Ideas generated by ChatGPT can help writers who lack inherent flair but may mean there are fewer unique ideas|
|[OpenAI is reportedly working on more advanced AI models capable of reasoning and ‘deep research’.](https://www.engadget.com/openai-is-reportedly-working-on-more-advanced-ai-models-capable-of-reasoning-and-deep-research-202419228.html) |The secret project is code-named ‘Strawberry,’ according to a Reuters report. |
|[Meet the AI Agent Engineer.](https://sierra.ai/blog/meet-the-ai-agent-engineer) |At his company, Sierra, Bret Taylor, the Chairman of the Board of OpenAI, has created a new position called Agent Engineer. One of the first people in the role recently wrote a blog post describing the Sierra team's view of agent engineering as a new field inside AI engineering. |
|[OpenAI Revenue.](https://futuresearch.ai/openai-revenue-report) |An estimated $3.4 billion in revenue for OpenAI comes from its ChatGPT services. |
|[Taming the tail utilization of ads inference at Meta scale.](https://engineering.fb.com/2024/07/10/production-engineering/tail-utilization-ads-inference-meta) |Meta's machine learning inference services saw a two-thirds decrease in failure rates, a 35% increase in compute efficiency, and a halving of p99 latency because to changes made in the tail utilization. With these improvements, Meta's ad delivery systems are guaranteed to be able to manage growing workloads without requiring more resources and to uphold service level agreements. Predictive scaling and managing the machine learning model lifetime with Meta's unified platform, IPnext, are examples of continuous improvement techniques. |
|[Meta to reportedly launch largest Llama 3 model on July 23.](https://breakingthenews.net/Article/Meta-to-reportedly-launch-largest-Llama-3-model-on-July-23/62364570) |Meta Platforms will release its largest Llama 3 model on July 23, The Information reported on Friday, citing an employee of the company. The new model, boasting 405 billion parameters, will be multimodal and capable of understanding and generating both images and text. |
|[Quora’s Poe now lets users create and share web apps.](https://techcrunch.com/2024/07/08/quoras-poe-now-lets-users-create-and-share-web-apps/) |Poe, Quora’s subscription-based, cross-platform aggregator for AI-powered chatbots like Anthropic’s Claude and OpenAI’s GPT-4o, has launched a feature called Previews that lets people create interactive apps directly in chats with chatbots.|
|[Microsoft CTO Kevin Scott thinks LLM “scaling laws” will hold despite criticism.](https://arstechnica.com/information-technology/2024/07/microsoft-cto-defies-critics-ai-progress-not-slowing-down-its-just-warming-up/) |Will LLMs keep improving if we throw more compute at them? OpenAI dealmaker thinks so. |
|[OpenAI says there are 5 'levels' for AI to reach human intelligence — it's already almost at level 2.](https://qz.com/openai-five-level-system-human-intelligence-ai-1851588122) |The company shared a five-level system it developed to track its artificial general intelligence, or AGI, progress with employees this week, an OpenAI spokesperson told Bloomberg. The levels go from the currently available conversational AI to AI that can perform the same amount of work as an organization.|
|[AI startup Hebbia raised $130M at a $700M valuation on $13 million of profitable revenue.](https://techcrunch.com/2024/07/09/ai-startup-hebbia-rased-130m-at-a-700m-valuation-on-13-million-of-profitable-revenue) | Hebbia, a startup that uses generative AI to search large documents and respond to large questions, has raised a $130 million Series B at a roughly $700 million valuation led by Andreessen Horowitz, with participation from Index Ventures, Google Ventures and Peter Thiel.|
|[Pixel 9 Pro might come with 1-year of Gemini Advanced.](https://9to5google.com/2024/07/15/pixel-9-pro-might-gemini-advanced/) |With less than a month until Made by Google 2024, the latest leak suggests that the Pixel 9 Pro will come with 1-year of Gemini Advanced. |
|[Company Abandons Plans to Give AI Workers "Rights" and Add Them to Org Chart After Outcry From Human Employees.](https://futurism.com/startup-ai-rights-org-chart) |Following its announcement that it would give AI algorithms "rights" and integrate them as "digital workers" with managers and performance evaluations in its product, the HR software provider Lattice encountered criticism. |
|[Want to know how AI will affect government and politics? The bots have the answers.](https://www.theguardian.com/technology/article/2024/jul/16/want-to-know-how-ai-will-affect-government-and-politics-the-bots-have-the-answers) |Tony Blair’s powerful thinktank asked ChatGPT how AI might affect public sector jobs. Critics say the results were … wonky |
|[Andrej Karpathy's new company.](https://eurekalabs.ai/) |A new AI startup with an emphasis on education, Eureka Labs aims to transform the way we acquire new knowledge. |
|[Whistleblowers accuse OpenAI of ‘illegally restrictive’ NDAs.](https://techcrunch.com/2024/07/13/whistleblowers-accuse-openai-of-illegally-restrictive-ndas/) | Whistleblowers have accused OpenAI of placing illegal restrictions on how employees can communicate with government regulators, according to a letter obtained by The Washington Post.|
|[Apple, Nvidia, Anthropic Used Thousands of Swiped YouTube Videos to Train AI.](https://www.proofnews.org/apple-nvidia-anthropic-used-thousands-of-swiped-youtube-videos-to-train-ai/) | AI companies are generally secretive about their sources of training data, but an investigation by Proof News found some of the wealthiest AI companies in the world have used material from  thousands of  YouTube videos to train AI. Companies did so despite YouTube’s rules against harvesting materials from the platform without permission.|
|[SciCode: A Research Coding Benchmark Curated by Scientists.](https://scicode-bench.github.io/) |The objective of coding models has always been HumanEval. It is essentially solved now. This benchmark is the next step forward in solving difficult science programming puzzles. |
|[SmolLM - blazingly fast and remarkably powerful.](https://huggingface.co/blog/smollm) |This blog post introduces SmolLM, a family of state-of-the-art small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset. It covers data curation, model evaluation, and usage. |
|[Benchmarking results for vector databases.](https://redis.io/blog/benchmarking-results-for-vector-databases/) |Redis has released updated information on the best vector databases, measuring throughput and latency with the help of the industry-recognized Qdrant framework. Key findings include Redis achieving much higher queries per second and lower latency than Qdrant, Milvus, and Weaviate, and outperforming competitors by 62% for low complexity datasets and by 21% for high-dimensional datasets. |
|[Announcing the launch of Gray Swan.](https://www.grayswan.ai/news/gray-swan-launch) |A company specializing in creating tools to assist businesses in evaluating the risks associated with their AI systems and protecting their AI installations from inappropriate use is called Gray Swan AI. |
|[Anthropic releases Claude app for Android.](https://techcrunch.com/2024/07/16/anthropic-releases-claude-app-for-android/) |Anthropic launched its Claude Android app on Tuesday to bring its AI chatbot to more users. This is Anthropic’s latest effort to convince users to ditch ChatGPT by making Claude available in more places. |
|[AI tool can pinpoint dementia’s cause — from stroke to Alzheimer’s.](https://www.nature.com/articles/d41586-024-02202-1) |Algorithm that distinguishes among a host of underlying causes of dementia could be used for diagnosis in hospitals and clinics. |
|[Portal needed for victims to report AI deepfakes, federal police union says.](https://www.theguardian.com/technology/article/2024/jul/18/ai-deepfakes-revenge-porn-reporting-portal-australia-laws) |Parliamentary inquiry told police forced to ‘cobble together’ laws to prosecute man who allegedly spread deepfake images of women |
|[Meta Won't Offer Future Multimodal AI Models In The EU.](https://www.axios.com/2024/07/17/meta-future-multimodal-ai-models-eu) |Due to legislative uncertainties, Meta will not be able to provide future multimodal AI models to consumers in the EU; however, Llama 3 will still be offered in text only. |
|[Anthropic teams up with venture capital firm to kickstart $100M AI startup fund.](https://www.theregister.com/2024/07/17/anthropic_teams_up_with_vc/) |Recipients of six-digit investments aren’t required to use Claude |
|[Anthropic doubles output token limit.](https://threadreaderapp.com/thread/1812921642143900036.html) | Anthropic has doubled the max output token limit for Claude 3.5 Sonnet from 4096 to 8192 in the Anthropic API.|
|[AI-powered video creation for work.](https://workspace.google.com/products/vids/) |An AI-powered video creation tool for the workplace, Google Vids is tightly integrated with the Workspace suite. |
|[aiXplain Secures $6.5M pre-Series A to Universalize AI Agent Development.](https://www.einnews.com/pr_news/728139645/aixplain-secures-6-5m-pre-series-a-to-universalize-ai-agent-development) | Saudi Aramco's venture arm, Wa'ed Ventures, has announced a $6.5 million pre-series A fundraising round for aiXplain (a global top 10 firm by market cap).|
|[Meta pulls plug on release of advanced AI model in EU.](https://www.theguardian.com/technology/article/2024/jul/18/meta-release-advanced-ai-multimodal-llama-model-eu-facebook-owner) |‘Unpredictable’ privacy regulations prompt Facebook owner to scrap regional plans for multimodal Llama |
|[Mistral NeMo.](https://mistral.ai/news/mistral-nemo/) | A novel tokenizer was used to train the multilingual Mistral Nemo 12B model, which exhibits strong multilingual and English performance. Also supported are 128k contexts.|
|[OpenAI is releasing a cheaper, smarter model.](https://www.theverge.com/2024/7/18/24200714/openai-new-cheaper-smarter-model-gpt-4o-mini) |OpenAI is releasing a lighter, cheaper model for developers to tinker with called GPT-4o Mini. It costs significantly less than full-sized models and is said to be more capable than GPT-3.5. |
|[Cohere and Fujitsu Announce Strategic Partnership To Provide Japanese Enterprise AI Services.](https://cohere.com/blog/fujitsu-partnership) |Cohere and Fujitsu have partnered strategically to create and offer enterprise AI services that have the best Japanese language capabilities in the market. These services, which will provide private cloud deployments to businesses in highly regulated sectors including financial institutions, the public sector, and research and development units, will be developed with security and data privacy as their primary goals. |
|[OpenAI And Broadcom Held Discussions About Producing An AI Chip.](https://seekingalpha.com/news/4125638-broadcom-held-discussions-with-openai-about-producing-ai-chip-report) | OpenAI and Broadcom have discussed developing a new artificial intelligence server processor.|
|[Flow Studio.](https://www.producthunt.com/posts/flow-studio) |Flow Studio creates 3-minute films that are completely produced, with a believable story, dependable characters, and automatically synced sound effects and background music. |
|[Slow recovery from IT outage begins as experts warn of future risks.](https://www.theguardian.com/australia-news/article/2024/jul/19/microsoft-windows-pcs-outage-blue-screen-of-death) |Fault in CrowdStrike caused airports, businesses and healthcare services to languish in ‘largest outage in history’ |

## Resources
|Link|description|
|---|---|
|[A Survey on Mixture of Experts.](https://arxiv.org/abs/2407.06204) | a survey study on the Mixture of Experts (MoE), covering its technical specifications, open-source implementations, assessment methods, and practical uses. |
|[Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence.](https://arxiv.org/abs/2407.07061v2) |a new framework to address several limitations in multi-agent frameworks such as integrating diverse third-party agents and adaptability to dynamic task requirements; introduces an agent integration protocol, instant messaging architecture design, and dynamic mechanisms for effective collaboration among heterogeneous agents. |
|[Meta 3D Gen.](https://ai.meta.com/research/publications/meta-3d-gen/) | a new pipeline that can generate 3D assets from text in less than a minute, from start to finish. It incorporates cutting-edge parts like TextureGen and AssetGen to represent objects in three dimensions: view space, volumetric space, and UV space. It also achieves a 68% win rate compared to the single-stage model.|
|[Challenges, evaluation and opportunities for open-world learning.](https://www.nature.com/articles/s42256-024-00852-4) | Here we argue that designing machine intelligence that can operate in open worlds, including detecting, characterizing and adapting to structurally unexpected environmental changes, is a critical goal on the path to building systems that can solve complex and relatively under-determined problems.  |
|[Machine learning-aided generative molecular design.](https://www.nature.com/articles/s42256-024-00843-5) |Data-driven generative methods have the potential to greatly facilitate molecular design tasks for drug design. |
|[Introducing AuraFlow v0.1, an Open Exploration of Large Rectified Flow Models.](https://blog.fal.ai/auraflow/) | Fal trained a new open model called AuraFlow. The model has 5.8B parameters and was trained with muP.|
|[Lynx: State-of-the-Art Open Source Hallucination Detection Model.](https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model) |a model for identifying language model hallucinations that performs noticeably better than the state of the art in its generations. |
|[Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph.](https://arxiv.org/abs/2403.09236v1) | Hyper-3DG enhances text-to-3D model creation by emphasizing the intricate connections between texture and geometry.|
|[LightenDiffusion.](https://github.com/jianghaiscu/lightendiffusion) |By utilizing diffusion models and Retinex theory, LightenDiffusion enhances low-light photos. |
|[ProDepth.](https://sungmin-woo.github.io/prodepth/) |A novel framework for monocular depth estimation called ProDepth addresses problems brought on by moving objects in dynamic situations. It finds and fixes discrepancies in depth estimate using a probabilistic method. |
|[Open-Canopy.](https://arxiv.org/abs/2407.09392v1) | A high-resolution (1.5 m) publicly available dataset called Open-Canopy is used to estimate canopy height over France.|
|[crawlee-python.](https://github.com/apify/crawlee-python) | Crawlee—A web scraping and browser automation library for Python to build reliable crawlers. Extract data for AI, LLMs, RAG, or GPTs. Download HTML, PDF, JPG, PNG, and other files from websites. Works with BeautifulSoup, Playwright, and raw HTTP. Both headful and headless mode. With proxy rotation.|
|[Mathstral.](https://mistral.ai/news/mathstral/) | Mistral's newest math model performs well on various benchmarks|
|[Codestral Mamba.](https://mistral.ai/news/codestral-mamba/) | Codestral Mamba, a Mamba2 language model specialised in code generation, available under an Apache 2.0 license.|
|[exo.](https://github.com/exo-explore/exo) |Run your own AI cluster at home on everyday devices. |
|[Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training.](https://github.com/robustnlp/derta) |Through addressing refusal position bias, a novel method called Decoupled Refusal Training (DeRTa) enhances safety tuning in large language models. |
|[PID: Physics-Informed Diffusion Model for Infrared Image Generation.](https://github.com/fangyuanmao/pid) | By integrating physical laws into the conversion process, researchers have created a Physics-Informed Diffusion (PID) model that enhances the translation of RGB images to infrared images.|
|[What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives.](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising) |Excellent post on encoders, prefixlm, denoising aims, and other contemporary language modeling techniques by Yi Tay of Reka and Google. |
|[LiDAR Semantic Segmentation.](https://arxiv.org/abs/2407.11569v1) |A novel technique called SFPNet is intended to be universal across various LiDAR technology types. Instead of employing window-attention as in the past, SFPNet uses sparse focus point modulation to extract and dynamically collect multi-level contexts. |
|[Praison AI.](https://github.com/MervinPraison/PraisonAI) |Using prior agent frameworks as a springboard, Praison AI is a low-code, centralized framework with customizable features and human-agent interaction that makes it easier to create and manage multi-agent systems for a range of LLM applications. |
|[Video Object Segmentation with World Knowledge.](https://github.com/cilinyan/VISA) | Reasoning Video Object Segmentation (ReasonVOS) is a new task that uses implicit text queries to generate segmentation masks. It requires complex reasoning and world knowledge.|
|[Enhancing Class Learning Without Forgetting.](https://github.com/roadonep/eccv2024_mbs) |In order to enhance Class-Incremental Semantic Segmentation (CISS), this project presents a background-class separation framework. |
|[Leapfrogging traditional vector-based RAG with language maps.](https://x.com/mutableai/status/1813815706783490055) |When developing a chat application over data, retrieval plays a major role. But frequently, systems are delicate to the format of the data being accessed. Chat-based performance is greatly enhanced by creating a language map (e.g., Wikipedia style entry) of the material and using that for retrieval. This is how code base question answering is handled by mutable AI. |
|[Removing Inappropriate Content from Diffusion Models.](https://arxiv.org/abs/2407.12383v1) | Using a revolutionary technique called Reliable and Efficient Concept Erasure (RECE), improper content may be removed from diffusion models in only three seconds without requiring additional fine-tuning.|
|[LLM2sh.](https://github.com/randombk/llm2sh) |A command-line tool called LLM2sh uses LLMs to convert requests written in plain English into shell instructions. |
|[GraphMuse.](https://github.com/manoskary/graphmuse) |GraphMuse is a Python Library for Graph Deep Learning on Symbolic Music. This library intents to address Graph Deep Learning techniques and models applied specifically to Music Scores. |
|[E5-V: Universal Embeddings with Multimodal Large Language Models.](https://github.com/kongds/e5-v) | A novel framework called E5-V modifies Multimodal Large Language Models (MLLMs) to provide multimodal embeddings that are universal. With prompts, it bridges the gap between various input formats and achieves remarkable results in multimodal activities without the need for fine-tuning.|
|[Strategizing Your Preparation for Machine Learning Interviews.](https://mlengineerinsights.substack.com/p/strategizing-your-preparation-for) | Interviews for machine learning might be difficult. You may greatly increase your chances by being aware of the range of machine learning positions and adjusting your preparation to fit particular job duties and specializations. To approach interviews with confidence, concentrate on learning the fundamentals, investigating technology unique to the organization, and regularly monitoring your progress.|
|[Uncensor Any LLM With Abliteration.](https://research.google/blog/smart-paste-for-context-aware-adjustments-to-pasted-code/) | For safety, llama models are heavily restricted, which reduces their versatility. Through the identification and elimination of the rejection mechanism, the "abliteration" technique uncensors them, enabling models to respond to all stimuli without requiring retraining.|
|[SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers.](https://arxiv.org/abs/2407.09413v1) |SPIQA is a quality assurance dataset created to assist users in rapidly locating solutions within scientific research publications by deciphering intricate figures and tables. |


## Perspectives
|Link|description|
|---|---|
|[AI’s ‘Oppenheimer moment’: autonomous weapons enter the battlefield.](https://www.theguardian.com/technology/article/2024/jul/14/ais-oppenheimer-moment-autonomous-weapons-enter-the-battlefield) |The military use of AI-enabled weapons is growing, and the industry that provides them is booming |
|[Will generative AI transform robotics?](https://www.nature.com/articles/s42256-024-00862-2) |In the current wave of excitement about applying large vision–language models and generative AI to robotics, expectations are running high, but conquering real-world complexities remains challenging for robots. |
|[Introducing: The Managed-Service-as-Software (M-SaS) Startup.](https://dannguyenhuu.substack.com/p/introducing-the-managed-service-as) |AI-driven, service-oriented firms are creating Managed-Service-as-Software (M-SaS) enterprises, which follow a new business model blueprint in building their businesses. Startups need to adopt a fundamentally different attitude in order to use AI instead of selling it. These firms start off labor-intensive with low gross margins and then use automation and artificial intelligence (AI) to progressively move to greater SaaS-like gross margins. |
|[Could AIs become conscious? Right now, we have no way to tell.](https://arstechnica.com/science/2024/07/could-ais-become-conscious-right-now-we-have-no-way-to-tell/) |With divergent opinions on whether developments in machine learning and neuromorphic computing can result in sentient computers, the discussion over artificial intelligence potentially gaining awareness is becoming more heated. The theory of Integrated Information holds that the current hardware limits make AI consciousness implausible, while computational functionalist theories such as Global Neuronal Workspace Theory and Attention Schema Theory believe that AI awareness is inevitable. Neuroscience is trying to come up with a single theory of consciousness in order to better understand how it might show up in AI. |
|[Generative AI makes for better scientific writing — but beware the pitfalls.](https://www.nature.com/articles/d41586-024-02319-3) |As researchers who have sometimes struggled with articulating intricate concepts, we find his suggestions for using ChatGPT to improve the clarity and coherence of academic papers compelling. But potential pitfalls warrant further discussion. |
|[My trip to the frontier of AI education.](https://www.gatesnotes.com/My-trip-to-the-frontier-of-AI-education) |First Avenue Elementary School in Newark is utilizing Khanmigo, an AI-powered tutor and teacher assistant created by Khan Academy, to include AI tools for education. Teachers in the classroom can customize instruction and cut down on work time by using this technology. The goal of increasing responsiveness and inclusion is a continuous endeavor. Through increased teacher-student involvement, this Gates Foundation-backed project seeks to level the playing field in education. |
|[AI-Driven Behavior Change Could Transform Health Care.](https://time.com/6994739/ai-behavior-change-health-care/) |Thrive AI Health is being funded by OpenAI and Thrive Global to create a customized AI health coach that addresses everyday health-related behaviors like nutrition and sleep. AI's hyper-personalization powers the mobile app and corporate solution by fusing individual data with peer-reviewed science. The project intends to manage chronic diseases, democratize healthy behavior modification, and show how effectively AI can be integrated into healthcare while maintaining robust privacy protections. |
|[GraphRAG Analysis, Part 1: How Indexing Elevates Knowledge Graph Performance in RAG.](https://aiencoder.substack.com/p/graphrag-analysis-part-1-how-indexing) | Analysis of Microsoft's GraphRAG research suggests that knowledge graphs like Neo4j may not significantly beat FAISS in context retrieval for RAG applications. While Neo4j without its indexing can reach a better answer relevancy, the minor advantages may not justify the cost given ROI limits. Neo4j's indexing, on the other hand, significantly improves answer faithfulness, lowering the possibility of false information.|
|[How Taiwan secured semiconductor supremacy – and why it won’t give it up.](https://www.theguardian.com/world/article/2024/jul/19/taiwan-semiconductor-industry-booming) | Trump has accused Taiwan of ‘taking’ the US chip sector, but Taipei has been at the forefront of the industry for decades, and its future could depend on it|
|[Overcoming The Limits Of Current LLMs.](https://seanpedersen.github.io/posts/overcoming-llm-limits) | Large language models (LLM) have been all the rage for quite some time now. Looking beyond the hype though, they have severe limitations: hallucinations, lack of confidence estimates and lack of citations.|

# ML news: 8 - 14  July

## Research
|Link|description|
|---|---|
|[MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases.](https://arxiv.org/abs/2402.14905) |Comprehensive and fascinating work by Meta that demonstrates how to train tiny models to maximize performance. |
|[Non-Adversarial Learning: Vector-Quantized Common Latent Space for Multi-Sequence MRI.](https://arxiv.org/abs/2407.02911v1) | Without the need for paired samples, researchers have created a new generative model to enhance MRI image translation between various sequences.|
|[Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene Reconstruction.](https://arxiv.org/abs/2407.02918v1) | A new approach to 3D reconstruction of surgical scenes that does not require SfM has been presented. It overcomes the drawbacks of earlier methods that had trouble with inconsistent photometry and sparse textures.|
|[FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs.](https://fun-audio-llm.github.io/) |Extremely powerful models for audio understanding and generation were provided by the Tongyi speech team. |
|[APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.](https://arxiv.org/abs/2406.18518) |A dataset with 60K entries is also released to aid in research on function-calling enabled agents. APIGen - presents an automated data generation pipeline to synthesize high-quality datasets for function-calling applications; demonstrates that 7B models trained on curated datasets outperform GPT-4 models and other state-of-the-art models on the Berkeley Function-Calling Benchmark.  |
|[Searching for Best Practices in Retrieval-Augmented Generation.](https://arxiv.org/abs/2407.01219) |Looking for Best Practices in RAG outlines best practices for creating efficient RAG workflows and suggests performance- and efficiency-focused tactics, such as newly developed multimodal retrieval tools. |
|[Self-Evaluation as a Defense Against Adversarial Attacks on LLMs.](https://arxiv.org/abs/2407.03234) |The article "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs" suggests using self-evaluation as a defense against adversarial attacks. It demonstrates that developing a dedicated evaluator can significantly lower the success rate of attacks and uses a pre-trained LLM to build defense that is more effective than fine-tuned models, dedicated safety LLMs, and enterprise moderation APIs. The article evaluates various settings, such as attacks on the generator alone and the generator + evaluator combined. |
|[Adaptable Logical Control for Large Language Models.](https://arxiv.org/abs/2406.13892) |The Ctrl-G framework, which combines LLMs and Hidden Markow Models to enable following logical constraints (represented as deterministic finite automata), is presented in Adaptable Logical Control for LLMs. Ctrl-G achieves over 30% higher satisfaction rate in human evaluation compared to GPT4. |
|[LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives.](https://arxiv.org/abs/2407.01490) | In LLM See, LLM Do, the effectiveness and effects of synthetic data are examined in detail, along with how they affect a model's internal biases, calibration, attributes, and preferences. It is discovered that LLMs are sensitive to certain attributes even when the prompts from the synthetic data seem neutral, indicating that it is possible to influence the generation profiles of models to reflect desirable attributes.|
|[Chinese developers scramble as OpenAI blocks access in China.](https://www.theguardian.com/world/article/2024/jul/09/chinese-developers-openai-blocks-access-in-china-artificial-intelligence) | US firm’s move, amid Beijing-Washington tensions, sparks rush to lure users to homegrown models|
|[PartCraft: Crafting Creative Objects by Parts.](https://arxiv.org/abs/2407.04604v1) |PartCraft is a novel approach in generative visual AI that goes beyond conventional text- or sketch-based methods by enabling users to choose visual concepts by parts. |
|[AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.](https://arxiv.org/abs/2407.04363v1) |AriGraph is a new technique that assists AI agents in creating a memory graph that incorporates episodic and semantic memories. |
|[Researchers leverage shadows to model 3D scenes, including objects blocked from view.](https://news.mit.edu/2024/researchers-leverage-shadows-model-3d-scenes-blocked-objects-0618) | Researchers at MIT and Meta developed PlatoNeRF, an AI method that builds 3D representations of scenes, including blocked areas, using single-photon lidar and shadows. This technique could improve AR/VR experiences and increase the safety of autonomous vehicles. With lower-resolution sensors, PlatoNeRF performs better than conventional techniques and shows promise for real-world applications.|
|[Distilling System 2 into System 1.](https://arxiv.org/abs/2407.06023) | Models classified as System 2 employ techniques similar to Chain of Thought in order to increase test time, compute, and enhance thinking. It turns out that this behavior can be reduced to a speedier, similarly accurate System 1 model.|
|[Learning to (Learn at Test Time): RNNs with Expressive Hidden States.](https://arxiv.org/abs/2407.04620) | a recently developed RNN variation that beats Mamba in several tasks. Significantly, extended contexts and in-context learning are made possible by the update function, which is an ML model in and of itself.|
|[NuminaMath 7B TIR: Open Math Olympiad Model Released.](https://huggingface.co/AI-MO/NuminaMath-7B-TIR) |NuminaMath is a series of language models that are trained to solve math problems using tool-integrated reasoning (TIR). |
|[4D Contrastive Superflows are Dense 3D Representation Learners.](https://arxiv.org/abs/2407.06190v1) | SuperFlow is a novel system that uses successive LiDAR-camera pairs for spatiotemporal pretraining to improve 3D vision in autonomous driving.|
|[PaliGemma: A versatile 3B VLM for transfer.](https://arxiv.org/abs/2407.07726) | Based on Gemma 2B and SigLIP, PaliGemma is a powerful vision language model. Many of the choices taken in terms of architecture and data collecting are displayed in this technical paper.|
|[ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction.](https://haoosz.github.io/ConceptExpress/) |A novel job called Unsupervised Concept Extraction (UCE) collects and reconstructs many concepts from a single image without the need for human annotations. |
|[Lookback Lens.](https://github.com/voidism/lookback-lens) | A simple model called Lookback Lens can be used to identify contextual hallucinations in large language models.|


## News
|Link|description|
|---|---|
|[A Hacker Stole OpenAI Secrets, Raising Fears That China Could, Too.](https://www.nytimes.com/2024/07/04/technology/openai-hack.html?unlocked_article_code=1.4k0.8GOs.WFzxVAjkpQLt&smid=url-share) |A security breach at the maker of ChatGPT last year revealed internal discussions among researchers and other employees, but not the code behind OpenAI’s systems. |
|[Figma pulls AI tool after criticism that it ripped off Apple’s design.](https://www.theverge.com/2024/7/2/24190823/figma-ai-tool-apple-weather-app-copy) |Figma says it didn’t train the generative AI models it used and blames a ‘bespoke design system.’ |
|[Hollywood stars’ estates agree to the use of their voices with AI.](https://edition.cnn.com/2024/07/03/tech/elevenlabs-ai-celebrity-voices) | Earlier this week, AI company ElevenLabs said it is bringing digitally produced celebrity voice-overs of deceased actors, including Garland, James Dean and Burt Reynolds, to its newly launched Reader app. The company said the app takes articles, PDF, ePub, newsletters, e-books or any other text on your phone and turns it into voice-overs.|
|[Smart Paste for context-aware adjustments to pasted code.](https://research.google/blog/smart-paste-for-context-aware-adjustments-to-pasted-code/) | We present Smart Paste, an internal tool that streamlines the code authoring workflow by automating adjustments to pasted code. We describe key insights from our UX and model preparation efforts, which have led to high performance and successful adoption among Google developers.|
|[Apple M5 Chip's Dual-Use Design Will Power Future Macs and AI Servers.](https://www.macrumors.com/2024/07/04/apple-m5-chips-advanced-packaging-tsmc/) | Apple will reportedly use a more advanced SoIC packaging technology for its M5 chips, as part of a two-pronged strategy to meet its growing need for silicon that can power consumer Macs and enhance the performance of its data centers and future AI tools that rely on the cloud.|
|[Apple Intelligence and a better Siri may be coming to iPhones this spring.](https://www.theverge.com/2024/7/7/24193619/apple-intelligence-better-siri-ios-18-4-spring-public-launch) | Expect Apple’s AI system in iOS 18.4, says a new Bloomberg rumor.|
|[Meta claims news is not an antidote to misinformation on its platforms.](https://www.theguardian.com/media/article/2024/jul/09/meta-facebook-australia-news-ban-misinformation) |Company says it has ‘never thought about news’ as a way to counter misleading content on Facebook and Instagram despite evidence to the contrary |
|[Meta drops AI bombshell: Multi-token prediction models now open for research.](https://venturebeat.com/ai/meta-drops-ai-bombshell-multi-token-prediction-models-now-open-for-research/) | Meta has thrown down the gauntlet in the race for more efficient artificial intelligence. The tech giant released pre-trained models on Wednesday that leverage a novel multi-token prediction approach, potentially changing how large language models (LLMs) are developed and deployed.|
|[Google DeepMind’s AI Rat Brains Could Make Robots Scurry Like the Real Thing.](https://singularityhub.com/2024/07/03/google-deepminds-ai-rat-brains-could-make-robots-scurry-like-the-real-thing/) |In order to investigate the brain circuits underlying complicated motor skills, DeepMind and Harvard University created a virtual rat using artificial intelligence (AI) neural networks trained on real rat motions and neural patterns. With its ability to transfer acquired movement skills to other settings, this bio-inspired AI could advance robotics and provide new insights into brain function. The study shows that brain activity associated with various behaviors may be accurately mimicked and decoded by digital simulations. |
|[Microsoft drops observer seat on OpenAI board amid regulator scrutiny.](https://www.theguardian.com/technology/article/2024/jul/10/microsoft-drops-observer-seat-on-openai-board-amid-regulator-scrutiny) |Startup’s new approach means Apple will no longer be able to appoint executive to similar role |
|[xAI ends deal with Oracle, builds own AI datacente.](https://threadreaderapp.com/thread/1810723880937607564.html) |Oracle has terminated xAI's agreement. After Grok 2 training is completed, it will construct its own datacenter. Originally, the corporation had a deal with Oracle for 24k H100s. |
|[a16z is trying to keep AI alive with Oxygen initiative.](https://www.theverge.com/2024/7/9/24195082/a16z-trying-to-keep-ai-alive-with-oxygen-intiative) | |
|[Quora’s Poe now lets users create and share web apps.](https://techcrunch.com/2024/07/08/quoras-poe-now-lets-users-create-and-share-web-apps/) | Poe, Quora’s subscription-based, cross-platform aggregator for AI-powered chatbots like Anthropic’s Claude and OpenAI’s GPT-4o, has launched a feature called Previews that lets people create interactive apps directly in chats with chatbots.|
|[Ex-Meta scientists debut gigantic AI protein design model.](https://www.nature.com/articles/d41586-024-02214-x) | EvolutionaryScale’s protein language model — among the largest AI models in biology — has created new fluorescent proteins and won big investment.|
|[Anthropic’s Claude adds a prompt playground to quickly improve your AI apps.](https://techcrunch.com/2024/07/09/anthropics-claude-adds-a-prompt-playground-to-quickly-improve-your-ai-apps/) | Prompt engineering became a hot job last year in the AI industry, but it seems Anthropic is now developing tools to at least partially automate it.|
|[OpenAI and Los Alamos National Laboratory announce bioscience research partnership.](https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/) |OpenAI and Los Alamos National Laboratory are developing evaluations to understand how multimodal AI models can be used safely by scientists in laboratory settings. |
|[‘I am happy to see how my baby is bouncing’: the AI transforming pregnancy scans in Africa.](https://www.theguardian.com/global-development/article/2024/jul/12/i-am-happy-to-see-how-my-baby-is-bouncing-the-ai-transforming-pregnancy-scans-in-africa) |While ultrasound services are normal practice in many countries, software being tested in Uganda will allow a scan without the need for specialists, providing an incentive for pregnant women to visit health services early on |
|[Samsung to launch upgraded voice assistant Bixby this year with its own AI.](https://www.cnbc.com/2024/07/11/samsung-to-launch-upgraded-bixby-this-year-with-its-own-ai.html) |Samsung will launch an upgraded version of its voice assistant Bixby this year based on its own artificial intelligence models, mobile chief TM Roh told CNBC. |
|[Google says Gemini AI is making its robots smarter.](https://www.theverge.com/2024/7/11/24196402/google-deepmind-gemini-1-5-pro-robot-navigation) | DeepMind is using video tours and Gemini 1.5 Pro to train robots to navigate and complete tasks.|
|[Here’s how Qualcomm’s new laptop chips really stack up to Apple, Intel, and AMD.](https://www.theverge.com/24191671/copilot-plus-pcs-laptops-qualcomm-intel-amd-apple) |The Snapdragon X Elite and X Plus chips from Qualcomm are making Windows on Arm a competitive platform, roughly matching the performance and battery life of AMD Ryzen, Apple's M3 chip, and Intel Core Ultra. The Snapdragon chips are excellent in multi-core scores and power economy, even though they don't lead in GPU performance. The latest generation of laptops with Snapdragon processors is a more affordable option than MacBooks and conventional Intel or AMD-based devices. |
|[China's Laws of Robotics: Shanghai publishes first humanoid robot guidelines.](https://finance.yahoo.com/news/chinas-laws-robotics-shanghai-publishes-093000734.html) | Shanghai has published China's first governance guidelines for humanoid robots, calling for risk controls and international collaboration, as tech giants like Tesla showed off their own automatons at the country's largest artificial intelligence (AI) conference.|
|[Crowdsourced Decentralized AI Market Map.](https://threadreaderapp.com/thread/1810703553901563923.html) | Open sourcing a community-led market map of Decentralized AI|


## Resources
|Link|description|
|---|---|
|[CapPa: Training vision models as captioners.](https://wandb.ai/craiyon/cappa-jax/reports/CapPa-Training-vision-models-as-captioners--Vmlldzo4NDUyNDUz) |Craiyon's trained CapPa vision model achieves state-of-the-art results on several difficult vision benchmarks. |
|[Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis.](https://huggingface.co/Kwai-Kolors/Kolors) |Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters.  |
|[EGIInet: Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion.](https://github.com/whu-usi3dv/egiinet) | By means of geometric task guiding, EGIInet successfully combines two modalities to present a novel way to point cloud completion.|
|[Quality Prompts.](https://github.com/sarthakrastogi/quality-prompts) |QualityPrompts implements 58 prompting techniques explained in this survey from OpenAI, Microsoft, et al. |
|[Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems.](https://arxiv.org/abs/2407.01370) |Describes a new job, SummHay, to evaluate a model's capacity to process a Haystack and produce a summary that highlights the key insights and references the original documents; finds that RAG components are found to improve performance on the benchmark, making it a feasible choice for holistic RAG evaluation. Long-context LLMs score 20% on the benchmark, which lags the human performance estimate of 56%. |
|[AI Agents That Matter.](https://arxiv.org/abs/2407.01502) |AI Agents That Matter examines existing agent evaluation procedures and identifies flaws that could prevent practical deployment; it also suggests a framework to prevent overfitting agents and an implementation that simultaneously maximizes accuracy and cost. |
|[An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2.](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1 |A post by Neel Nanda, a Research Engineer at Google DeepMind, about his favorite papers to read in Mechanistic Interpretability. |
|[SAE.](https://github.com/EleutherAI/sae) |This library trains k-sparse autoencoders (SAEs) on the residual stream activations of HuggingFace language models, roughly following the recipe detailed in Scaling and evaluating sparse autoencoders (Gao et al. 2024) |
|[MInference.](https://github.com/microsoft/MInference) | To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.|
|[micro-agent.](https://github.com/BuilderIO/micro-agent) | An AI agent that writes and fixes code for you.|
|[AnySR.](https://github.com/crispyfeso4/anysr) | A novel method for improving efficiency and scalability in single-image super-resolution (SISR) is called AnySR. The 'Any-Scale, Any-Resource' implementation is supported by AnySR, in contrast to previous techniques, which reduces resource requirements at smaller scales without the need for extra parameters.|
|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos.](https://arxiv.org/abs/2407.04384v1) | Without human supervision, researchers have created a novel method for estimating category-level 3D poses from informal, object-centric films.|
|[SenseVoice .](https://github.com/FunAudioLLM/SenseVoice) |a speech foundation model that possesses a variety of speech understanding functions, such as auditory event detection, spoken language identification, automatic speech recognition, and speech emotion recognition. |
|[Boosting Large Vision Language Models with Self-Training.](https://github.com/orrzohar/Video-STaR) | A novel method called Video Self-Training with Augmented Reasoning (Video-STaR) aims to enhance Large Vision Language Models (LVLMs).|
|[GraphRAG.](https://github.com/microsoft/graphrag) |With GraphRAG, you may use language models to analyze unstructured text. The quick start is simple to spin up because it operates on Azure. |
|[iLLM-TSC.](https://github.com/traffic-alpha/illm-tsc) |To enhance traffic signal control systems, researchers have created a novel framework that blends reinforcement learning with a sizable language model. |
|[Tutorials on Tinygrad.](https://mesozoic-egg.github.io/tinygrad-notes/) | A set of tools called Tinygrad is used to train deep learning models. An in-depth look at Tinygrad internals is made possible by this set of notes, which serves as an excellent introduction for AI compilers.|
|[OccSora: 4D Occupancy Generation Models as
World Simulators for Autonomous Driving.](https://wzzheng.net/OccSora/) |A 4D occupancy generation model based on diffusion called OccSora is intended to enhance long-term temporal evolutions. |
|[Awesome AGI Survey.](https://github.com/ulab-uiuc/agi-survey) | The goal of Artificial General Intelligence (AGI) is to execute a variety of real-world jobs with human-like efficiency. This project explores the path towards AGI.|
|[ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation.](https://arxiv.org/abs/2407.06135) | Developed from Meta's Chameleon model, Anole is an open autoregressive multimodal model. With focused fine-tuning, this effort restores the model's ability to generate images.|
|[Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning.](https://arxiv.org/abs/2407.06642v1) | A novel reinforcement learning framework is presented by researchers to enhance customized text-to-image generation.|
|[PerlDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Models.](https://arxiv.org/abs/2407.06109v1) |PerlDiff is a technique that incorporates 3D geometric information to increase the accuracy of street view image production. |
|[Paints-Undo.](https://github.com/lllyasviel/Paints-UNDO) |Paints UNDO is a system where a model generates strokes that are used to reconstruct an image. It comes from the same creators as ControlNet, IC-Light, and many other image production systems. Remarkably, in contrast to earlier stroke systems, this model is able to cancel strokes and frequently completely reevaluates its strategy halfway through—quite like a human artist would. |
|[minRF.](https://github.com/cloneofsimo/minRF) |For Stable Diffusion 3, scalable rectified flow transformers are partially utilized. This repository contains sweeps of the muP hyperparameters along with a rudimentary implementation of them. |
|[RouteLLM.](https://github.com/lm-sys/RouteLLM) | RouteLLM is a framework for serving and evaluating LLM routers|
|[30x speedup in model init for HF Transformers.](https://github.com/huggingface/transformers/pull/31771) | If you move some lazy loading to the model on the first pass, you can significantly reduce the amount of tokens lost every second during model initialization.|
|[FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision.](https://tridao.me/blog/2024/flash3/) | The basis for contemporary fast language models is FlashAttention. Up from 35% previously, this new variant takes 75% of the H100 capacity. This capability gain is the result of several significant system enhancements.|
|[OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion.](OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion) |A novel approach to open-vocabulary detection called OV-DINO addresses the difficulties of combining various data sources and making use of language-aware capabilities. |
|[Open-Vocabulary Video Instance Segmentation.](https://github.com/fanghaook/ovformer) |A innovative approach to Open-Vocabulary Video Instance Segmentation (VIS), OVFormer tackles important problems in the field. It uses video-based training to increase temporal consistency and align embeddings better. |
|[Satellite Image Time Series Semantic Change Detection: Novel Architecture and Analysis of Domain Shift.](https://imagine.enpc.fr/~elliot.vincent/sitsscd) |This work integrates semantic segmentation and change detection to address semantic change detection using satellite image time series (SITS-SCD). |
|[PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer.](https://arxiv.org/abs/2407.07764v1) |The PosFormer model overcomes the drawbacks of sequence-based methods to greatly enhance Handwritten Mathematical Expression Recognition (HMER). |



## Perspectives
|Link|description|
|---|---|
|[Real criminals, fake victims: how chatbots are being deployed in the global fight against phone scammers.](https://www.theguardian.com/technology/article/2024/jul/07/ai-chatbots-phone-scams) |New scambaiting AI technology Apate aims to keep scammers on the line while collecting data that could help disrupt their business model |
|[James Muldoon, Mark Graham and Callum Cant: ‘AI feeds off the work of human beings’.](https://www.theguardian.com/technology/article/2024/jul/06/james-muldoon-mark-graham-callum-cant-ai-artificial-intelligence-human-work-exploitation-fairwork-feeding-machine) | The Fairwork trio talk about their new book on the ‘extraction machine’, exposing the repetitive labour, often in terrible conditions, that big tech is using to create artificial intelligence|
|[Superintelligence—10 years later.](https://www.humanityredefined.com/p/superintelligence10-years-later) |Ten years after the publication of Nick Bostrom's seminal book "Superintelligence," advances in AI have raised awareness of the potential for AGI and its associated concerns. With 2024 being a turning point toward guaranteeing control and alignment with human values, the AI research community is now giving AI safety serious attention. With AI technologies advancing so quickly, the sector faces concerns related to safety and ethics that were previously thought to be theoretical. |
|[How Good Is ChatGPT at Coding, Really?](https://spectrum.ieee.org/chatgpt-for-coding) |Depending on the task difficulty and programming language, OpenAI's ChatGPT may generate code with success rates anywhere from less than 1% to 89%. |
|[TechScape: Can AI really help fix a healthcare system in crisis?](https://www.theguardian.com/technology/article/2024/jul/09/techscape-ai-nhs-healthcare-artificial-intelligence-cancer-care) | Artificial intelligence is heralded as helping the NHS fight cancer. But some warn it’s a distraction from more urgent challenges|
|[Pop Culture.](https://www.wheresyoured.at/pop-culture/) | In a critical 31-page analysis titled "Gen AI: Too Much Spend, Too Little Benefit?", Goldman Sachs makes the case that utility spending would rise sharply due to generative AI's power consumption and very little productivity advantages and returns. The study raises concerns about AI's potential to completely change industries by highlighting its high price, problems with the electrical infrastructure, and inability to produce appreciable increases in productivity or revenue. If significant advancements in technology are not made, it could portend a dismal future for the field.|
|[The AI summer.](https://www.ben-evans.com/benedictevans/2024/7/9/the-ai-summer) |Compared to other tech innovations like the iPhone and e-commerce, which took years to acquire hold, ChatGPT's quick adoption—it hit 100 million users in just two months—is noteworthy. Even with the initial excitement, not many users have found ChatGPT to be useful in the long run, and business adoption of big language models is still few. This suggests that more work is necessary to establish substantial product-market fit and long-term value. |
|[A Deep Dive on AI Inference Startups.](https://eastwind.substack.com/p/a-deep-dive-on-ai-inference-startups) |The development of AI's "picks and shovels," such as model fine-tuning, observability, and inference, is a well-liked field for venture capital investment. VCs are placing bets that when businesses integrate AI into their products, they won't want to develop things themselves. For AI inference, the TAM is highly limited. For VCs' investments to be profitable, they must have faith in significant TAM expansion. Although platforms for AI inference benefit startups in the short run, over the long run, they hurt them. |
|[Cyclists can't decide whether to fear or love self-driving cars.](https://www.yahoo.com/news/cyclists-t-decide-whether-fear-220824589.html) | San Francisco cyclists have reported near misses and safety concerns with self-driving cars from Waymo and Cruise. Almost 200 complaints about these self-driving cars' unpredictable behavior and near-misses have been filed with the California DMV. Despite the manufacturers' claims that their cars had improved safety features, the events cast doubt on the vehicles' suitability for widespread use in the face of heightened regulatory scrutiny.|
|[Augmenting Intelligence.](https://www.polymathicbeing.com/p/augmenting-intelligence) |This essay promotes a practical approach to employing AI as an enhancement to human intelligence and explores bridging the divide between techno-optimists and pessimists on the subject. It discusses AI's role in education, its effects on creativity and the arts, and its ethical application. The paper highlights that artificial intelligence (AI) is a tool that augments human capabilities rather than posing a threat, suggesting that the term "augmented intelligence" is a more realistic description.|


# ML news: Week 1 - 7 July

## Research
|Link|description|
|---|---|
|[LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs.](https://arxiv.org/abs/2406.15319) | claims to achieve 64.3% on HotpotQA (full-wiki), which is on par with the state-of-the-art model. proposes LongRAG, which combines RAG with long-context LLMs to enhance performance; uses a long retriever to significantly reduce the number of extracted units by operating on longer retrieval units; the long reader takes in the long retrieval units and leverages the zero-shot answer extraction capability of long-context LLMs to improve performance of the overall system. |
|[From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data.](https://arxiv.org/abs/2406.19292) |suggests a fine-tuning strategy to increase the precision of information retrieval in LLMs while preserving reasoning abilities over long-context inputs; the fine-tuning dataset consists of 350 sample numerical dictionary key-value retrieval tasks; results show that this strategy reduces the "lost-in-the-middle" effect and enhances performance on both long-context reasoning and information retrieval. |
|[GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models.](https://arxiv.org/abs/2406.14550v1) |enhances the long-context capabilities of LLMs by proposing a graph-based agent system that organizes long text into a graph and uses an agent to explore the graph (using predefined functions guided by a step-by-step rational plan) to efficiently generate answers to questions; consistently outperforms GPT-4-128k across context lengths ranging from 16k to 256k. |
|[Following Length Constraints in Instructions.](https://arxiv.org/abs/2406.17744) |explains a method for addressing length bias and training language models that adhere to length constraints more closely; it refines a model using DPO using a dataset that has been augmented with length instructions and demonstrates fewer length constraint violations while maintaining a high response quality. |
|[Adam-mini: Use Fewer Learning Rates To Gain More.](https://arxiv.org/abs/2406.16793) | a new optimizer that carefully divides parameters into blocks and assigns a single high-quality learning that outperforms Adam; it achieves consistent results on language models sized from 125M -7B for pre-training, SFT, and RLHF. It uses fewer learning rates, which results in a 45%–50% reduction in memory footprint while still performing on par or even better than AdamW.|
|[MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data.](https://arxiv.org/abs/2406.18790) |generative image model with better performance than pure text conditioned models due to its ability to interleave text and images. |
|[Scaling Synthetic Data Creation with 1,000,000,000 Personas.](https://arxiv.org/abs/2406.20094) |By treating web text as originating from a persona, this approach can significantly enhance job performance downstream by conditioning on that persona. The researchers find a jump of 20% points on MATH. |
|[Odd-One-Out: Anomaly Detection by Comparing with Neighbors.](https://arxiv.org/abs/2406.20099v1) | A novel anomaly detection challenge has been presented by researchers that focuses on things that appear unusual in comparison to other objects in the scene. In contrast to conventional techniques, anomalies in this case are distinctive to the scene and can be determined from several angles.|
|[Adaptable Logical Control for Large Language Models.](https://arxiv.org/abs/2406.13892) | This approach enables the control of model generation at inference time, as well as interactive text editing. It achieves strong performance with tiny models and permits logical limitations in the generating process.|
|[Pairwise Difference Learning for Classification.](https://arxiv.org/abs/2406.20031v1) | Scholars have expanded Pairwise Difference Learning (PDL), which was first developed as a regression method, to include classification tasks. PDL makes predictions about the differences between pairs of instances rather than the outcomes themselves.|
|[AXIAL.](https://github.com/GabrieleLozupone/AXIAL) |This research improves the explainability of model decisions by putting forth a novel technique for identifying Alzheimer's disease using 3D MRI scans. |
|[Multi-Session SLAM with Differentiable Wide-Baseline Pose Optimization.](https://arxiv.org/abs/2404.15263v1) |A novel technique called Multi-Session SLAM creatively records camera movements throughout multiple disconnected video sequences using a single global frame of reference. |

## News
|Link|description|
|---|---|
|[An Update to Adept.](https://www.adept.ai/blog/adept-update) |The founders of Adept are heading to Amazon to license some of their technology. |
|[Time strikes a deal to funnel 101 years of journalism into OpenAI's gaping maw.](https://www.engadget.com/time-strikes-a-deal-to-funnel-101-years-of-journalism-into-openais-gaping-maw-144058426.html) | Time has joined a growing number of publications to sign a licensing deal with OpenAI. The ChatGPT creator will legally be able to train its large language models on 101 years worth of the storied publication's journalism, as Axios first reported.|
|[Amazon Investigates Perplexity AI Over Potential Data-Scraping Violations.](https://www.pcmag.com/news/amazon-investigates-perplexity-ai-over-potential-data-scraping-violations) | Amazon Web Services is looking into whether Perplexity is breaking its rules after Wired said the AI startup is swiping its web archives without consent. Perplexity, however, says it's following the rules.|
|[Apple could announce a Google Gemini deal this fall .](https://www.theverge.com/2024/6/30/24189262/apple-intelligence-google-gemini-deal-iphone-mac-ipad-openai-chatgpt) | If you’re disappointed that the only AI model that will integrate with Apple devices so far will be ChatGPT, it sounds like you won’t have to wait long for that to change. Apple will announce “at least” one other deal — to add Google Gemini, too — this fall.|
|[Meta accused of breaking EU digital law by charging for ad-free social networks.](https://www.theguardian.com/technology/article/2024/jul/01/meta-facebook-instagram-eu-digital-markets-act) |European Commission objects to ‘pay or consent’ model for users of Facebook and Instagram |
|[Microsoft’s Mustafa Suleyman says he loves Sam Altman, believes he’s sincere about AI safety.](https://techcrunch.com/2024/06/25/microsofts-mustafa-suleyman-says-he-loves-sam-altman-believes-hes-sincere-about-ai-safety/) |In an interview at the Aspen Ideas Festival on Tuesday, Mustafa Suleyman, CEO of Microsoft AI, made it very clear that he admires OpenAI CEO Sam Altman. |
|[When the Terms of Service Change to Make Way for A.I. Training.](https://www.nytimes.com/2024/06/26/technology/terms-service-ai-training.html) | As they negotiate a complicated web of privacy regulations and user consent, tech giants like Google and Meta are revising their privacy rules to allow the use of public and potentially private user data to train AI systems. There have been backlash since consumers and content creators are afraid that their work will be used to train AI that may eventually replace them. The conflicts draw attention to new issues in data privacy, AI development, and striking a balance between innovation and morality in the IT sector.|
|[Meet Figma AI.](https://www.figma.com/blog/introducing-figma-ai) |Designers may get assistance with tasks like visual search, asset search, text editing, image editing, prototyping, layer renaming, and design generation with Figma AI, a new suite of AI-powered capabilities for Figma. During the beta phase, these features—which are driven by AI models from third parties—are free to use. |
|[Google’s emissions climb nearly 50% in five years due to AI energy demand.](https://www.theguardian.com/technology/article/2024/jul/02/google-ai-emissions) |Tech giant’s goal of reducing climate footprint at risk as it grows increasingly reliant on energy-hungry data centres |
|[Amazon beefs up AI development, hiring execs from startup Adept and licensing its technology.](https://www.cnbc.com/2024/06/28/amazon-hires-execs-from-ai-startup-adept-and-licenses-its-technology.html) |Amazon has hired top executives from AI agent startup Adept, the company confirmed. As part of the deal, Amazon will license technology from Adept, including some of its AI models and datasets. Amazon has been trying to keep pace with competitors in AI by developing services and through its investment in OpenAI competitor Anthropic.|
|[YouTube now lets you request removal of AI-generated content that simulates your face or voice.](https://techcrunch.com/2024/07/01/youtube-now-lets-you-request-removal-of-ai-generated-content-that-simulates-your-face-or-voice/) |YouTube also quietly rolled out a policy change in June that will allow people to request the takedown of AI-generated or other synthetic content that simulates their face or voice. The change allows people to request the removal of this type of AI content under YouTube’s privacy request process.  |
|[Phil Schiller to join OpenAI board in ‘observer’ role following Apple’s ChatGPT deal.](https://9to5mac.com/2024/07/02/apple-phil-schiller-openai-board-observer/) | At WWDC last month, Apple announced its partnership with OpenAI to integrate ChatGPT into iOS 18. While no money is changing hands between Apple and OpenAI, a new report today reveals that Apple will get an “observer role” on OpenAI’s board of directors as part of the arrangement.|
|[Japan introduces enormous humanoid robot to maintain train lines.](https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines) | The 12-metre high machine has coke bottle eyes and a crude Wall-E-like head, as well as large arms that can be fitted with blades or paint brushes|
|[Elon Musk: Grok 2 AI Arrives in August.](https://www.pcmag.com/news/elon-musk-grok-2-ai-arrives-in-august) |Musk says Grok 2 'should exceed current AI on all metrics,' though Grok 3 is waiting in the wings. |
|[Nvidia CEO Jensen Huang addresses rising competition at shareholder meeting after historic stock surge.](https://www.cnbc.com/2024/06/26/nvidia-ceo-jensen-huang-speaks-at-first-shareholder-meeting-since-stock-surge.html) |Nvidia CEO Jensen Huang answered questions at the company’s annual shareholder meeting after a more than 200% surge in the stock over the past year. The company passed a $3 trillion valuation and was briefly the most valuable public company. Without naming competitors, Huang laid out the company’s overall strategy to maintain its position.|
|[Persona’s founders are certain the world can use another humanoid robot.](https://techcrunch.com/2024/06/26/personas-founders-are-certain-the-world-can-use-another-humanoid-robot/) |MIT research scientist Jerry Pratt is back at it. In 2022, he left Boardwalk Robotics, a humanoid startup he founded and led, and joined the well-funded ranks of the Bay Area-based robotics firm Figure as its CTO months before it exited stealth. But he and Figure quietly parted ways last month. |
|[Kyutai unveils today the very first voice-enabled AI openly accessible to all.](https://kyutai.org/cp_moshi.pdf) | A pure audio LLM with low latency has been trained by Kyutai, an open research lab in France. In the upcoming months, the very amazing demo that it has managed to produce will be made available for public use.|
|[Face screening tool detects stroke in seconds.](https://www.rmit.edu.au/news/all-news/2024/june/stroke-face-screening) |A new smartphone face-screening tool could help paramedics to identify stroke in seconds – much sooner and more accurately than is possible with current technologies. |
|[This is Big Tech’s playbook for swallowing the AI industry.](https://www.theverge.com/2024/7/1/24190060/amazon-adept-ai-acquisition-playbook-microsoft-inflection) |With Amazon’s hiring of the team behind a buzzy AI startup, a pattern is emerging: the reverse acquihire. |
|[Intel shows off first fully integrated optical compute interconnect, designed to scale up AI workloads.](https://siliconangle.com/2024/06/26/intel-shows-off-first-fully-integrated-optical-compute-interconnect-designed-scale-ai-workloads/) |Intel Corp. said today it has achieved another key milestone as it strives to make integrated photonics technology for high-speed data transfers a reality. |
|[OpenAI’s ChatGPT Mac app was storing conversations in plain text.](https://www.theverge.com/2024/7/3/24191636/openai-chatgpt-mac-app-conversations-plain-text?utm_source=tldrai) | After the security flaw was spotted, OpenAI updated its desktop ChatGPT app to encrypt the locally stored records.|
|[Jeff Bezos to sell $5bn of Amazon shares after stock hits record high.](https://www.theguardian.com/technology/article/2024/jul/03/jeff-bezos-sell-amazon-shares) | Proposed sale of 25m shares disclosed in notice on Tuesday after stock hit all-time high of $200.43 during session|
|[Wimbledon employs AI to protect players from online abuse.](https://www.theguardian.com/sport/article/2024/jul/05/wimbledon-tennis-ai-artificial-intelligence-players-online-abuse) |Threat Matrix service monitors social media profiles and flags up death threats, racism and sexist comments |


## Resources
|Link|description|
|---|---|
|[EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees.](https://arxiv.org/abs/2406.16858) |improves the long-context capabilities of LLMs by putting forth a graph-based agent system that efficiently generates answers to questions by organizing long text into a graph and employing an agent to explore the graph (using predefined functions guided by a step-by-step reasonable plan); surpasses GPT-4-128k with consistency in context lengths between 16k and 256k. |
|[On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey.](https://arxiv.org/abs/2406.15126) |survey on LLM-based synthetic data generation, curation, and evaluation. |
|[Text2Bricks: Fine-tuning Open-Sora in 1,000 GPU Hours.](https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky) | Lambda Labs trained the Open Sora video model on its 1-click cluster to create Lego movies.|
|[Laplace Neural Operator.](https://github.com/qianyingcao/Laplace-Neural-Operator) |One architecture for approximating PDEs that is based on neural networks is the Laplace operator. |
|[llama-agents.](https://github.com/run-llama/llama-agents) |llama-agents is an async-first framework for building, iterating, and productionizing multi-agent systems, including multi-agent communication, distributed tool execution, human-in-the-loop, and more! |
|[Suri: Multi-constraint Instruction Following for Long-form Text Generation.](https://chtmp223.github.io/suri/) |A collection of 20,000 lengthy documents and intricate instructions is called Suri. Its goal is to enhance AI's capacity to adhere to intricate writing requirements. The Suri development team has presented Instructional ORPO (I-ORPO), an alignment technique that provides feedback through artificially damaged instructions. |
|[Cambrian-1.](https://cambrian-mllm.github.io/) | High-performing, fully open vision model from NYU with significant improvements over text encoders and data mixtures.|
|[DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling on Time Variability.](https://arxiv.org/abs/2406.19135v1) |A novel expressive text-to-speech (TTS) model called DEX-TTS makes use of reference speech to enhance style representation and model generalization. |
|[Debugging in PyTorch.](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide3/Debugging_PyTorch.html) |PyTorch is an excellent modeling tool. Nonetheless, a few prevalent issues have the ability to significantly lower model performance. Examining this list will aid you when debugging your model code. |
|[vision-agent.](https://github.com/landing-ai/vision-agent) | Vision Agent is a library that helps you utilize agent frameworks to generate code to solve your vision task.|
|[What to do to scale up?](https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5) | An amazing and surprisingly understandable post about fine-tuning hyperparameters as model and dataset sizes increase.|
|[Web2Code.](https://mbzuai-llm.github.io/webpage2code/) | A novel procedure that researchers have created will enhance Web2Code instruction tweaking. It entails generating new text question-answer pairs, generating new webpage image-code pairs, improving webpage understanding data, and developing new webpage code generation pairs.|
|[Block Transformer: Global-to-Local Language Modeling for Fast Inference.](https://github.com/itsnamgyu/block-transformer) | This repository presents a brand-new Transformer type with a significantly smaller KV cache size. Although it hasn't been tested in large quantities, it should be able to perform on par with typical Transformers.|
|[Composio.](https://github.com/ComposioHQ/composio) | Equip your agent with high-quality tools & integrations without worrying about authentication, accuracy, and reliability in a single line of code!|
|[Segment Anything without Supervision.](https://github.com/frank-xwang/unsam) |Unsupervised SAM (UnSAM) is a 'segment anything' model for promptable and automatic whole-image segmentation which does not require human annotations. |
|[Following Length Constraints in Instructions.](https://github.com/facebookresearch/RAM/tree/main/projects/length_instruct) | Most models don't adhere to length specifications (less than 40 words, for example). This piece demonstrates how to tune them to do that.|
|[AI Overviews Research: Comparing pre and post-rollout results on 100K keywords.](https://seranking.com/blog/google-ai-overviews-research/) |The prevalence of Google's AI Overviews (AIO) feature, which typically links to the top 10 organic results, has significantly decreased from 64% pre-rollout to just 8.71% of SERPs for 100K keywords. Following the implementation, both the length of AIO material and the quantity of links have grown, demonstrating Google's focus on thorough responses and reliable sources. In this dynamic search environment, where user searches with longer inquiries, lower search volumes, and lower CPC are more likely to result in AI-generated results, SEO techniques must change to stay relevant. |
|[Meta 3D Gen.](https://ai.meta.com/research/publications/meta-3d-gen) | Meta has trained both a PBR texture creation system and an advanced 3D object generation model. It generates synthetic data by using the proprietary 2D picture generating model of the company.|
|[Mutahunter.](https://github.com/codeintegrity-ai/mutahunter) |An open-source, LLM-based mutation testing tool for automated software testing that is independent of language. |
|[LLaRA: Large Language and Robotics Assistant.](https://github.com/lostxine/llara) |LLaRA is a framework that leverages conversation-style instruction-response pairings and Large Language Models (LLMs) to enhance robot action policy. These Vision Language Models (VLMs) use visual inputs to evaluate state data and produce the best possible policy choices. |
|[MM-Instruct.](https://github.com/jihaonew/mm-instruct) |MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment |
|[Parable of the Parser.](https://drive.google.com/file/d/1VodGljuEhBKwZIXQwN-ApH6g2wBAVAdK/view) |Great keynote talk from CVPR. |
|[InstantStyle-Plus : Style Transfer with Content-Preserving in Text-to-Image Generation.](https://instantstyle-plus.github.io/) | Style transfer with modern diffusion models and content embedders.|
|[RSCaMa: Remote Sensing Image Change Captioning with State Space Model.](https://arxiv.org/abs/2404.18895v1) |A novel technique called RSCaMa has been presented by researchers to use natural language to describe changes in remote sensing photographs. |
|[Simple Diffusion Language Models.](https://www.youtube.com/watch?v=WjAUX23vgfg&ab_channel=SashaRush%F0%9F%A4%97) |Excellent talk about utilizing diffusion as a target for language modeling by Hugging Face researcher and Cornell Tech professor Sasha Rush. |
|[3D Reconstruction from Blurry Images.](https://arxiv.org/abs/2407.02174v1) |Researchers have created a technique that uses neural radiance fields (NeRF) and event streams to recreate three-dimensional sceneries from a single fuzzy image. This novel method eliminates the requirement for pre-computed camera poses by modeling camera motion and synthesizing brightness changes to produce high-quality, view-consistent images from hazy inputs. |
|[Agentless.](https://github.com/OpenAutoCoder/Agentless) |Agentless is an agentless approach to automatically solve software development problems. To solve each issue, Agentless follows a simple two phase process: localization and repair. |
|[MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention.](https://hqjiang.com/minference.html) | A novel technique called inference speeds up the processing of lengthy cues in big language models. To get around the considerable delays brought on by conventional approaches, it makes use of sparse computation techniques.|
|[torch.compile, the missing manual.](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab) | Manual for resolving torch.compile errors to make your code run faster.|
|[facebook/multi-token-prediction.](https://huggingface.co/facebook/multi-token-prediction) |Models for Meta's multi-token prediction model were provided, and they performed incredibly well. |
|[Maestro - A Framework for Claude Opus, GPT and local LLMs to Orchestrate Subagents.](https://github.com/Doriandarko/maestro) |This Python script demonstrates an AI-assisted task breakdown and execution workflow using the Anthropic API. It utilizes two AI models, Opus and Haiku, to break down an objective into sub-tasks, execute each sub-task, and refine the results into a cohesive final output. |
|[Magic Insert: Style-Aware Drag-and-Drop.](https://magicinsert.github.io/) |Method from Google to introduce meaningful items into photos with diffusion. Demo and dataset are accessible. |
|[Discrete Semantic Tokenization for Deep CTR Prediction.](https://arxiv.org/abs/2403.08206v1) | UIST is a unique method that transforms dense embeddings into discrete, compact tokens for user and item representations, therefore significantly improving click-through rate estimates.|
|[CELLO: Causal Evaluation of Large Vision-Language Models.](https://arxiv.org/abs/2406.19131v1) | With 14,094 causal questions, CELLO is a new dataset designed to help AI understand causality beyond common sense thinking.|
|[OpenStreetView-5M.](https://github.com/gastruc/osv5m) | With more than 5 million geotagged street photos from 225 countries, OpenStreetView-5M is a sizable open-access dataset aimed at evaluating computer vision techniques for picture localization.|
|[PTQ4SAM: Post-Training Quantization for Segment Anything .](https://github.com/chengtao-lv/ptq4sam) | A new framework called PTQ4SAM was created to lessen the memory and processing requirements of the large-scale Segment Anything Model (SAM).|
|[Boosting Smartphone Camera Clarity.](https://github.com/cszhilu1998/selfdzsr_plusplus) |In this study, a self-supervised learning model that enhances reference-based super-resolution (RefSR) is used to present a technique for improving smartphone image resolution. |
|[An Investigation of Incorporating Mamba for Speech Enhancement.](https://arxiv.org/abs/2405.06573v1) | SEMamba is a novel speech enhancement system that enhances voice signal clarity by utilizing the Mamba state-space model.|
|[Florence 2 on WebGPU.](https://github.com/xenova/transformers.js/tree/v3/examples/florence2-webgpu) | The tiny vision model is fully functional within the onnx and WebGPU-based browser.|
|[FlexiFilm: Long Video Generation with Flexible Conditions.](https://y-ichen.github.io/FlexiFilm-Page/) |A diffusion model called FlexiFilm was created expressly to produce long videos—more than 30 seconds—with excellent quality and consistency. |

## Perspectives
|Link|description|
|---|---|
|[Smudgy chins, weird hands, dodgy numbers: seven signs you’re watching a deepfake.](https://www.theguardian.com/technology/article/2024/jul/01/seven-signs-deepfake-artificial-intelligence-videos-photographs) | Look out for surplus fingers, compare mannerisms with real recordings and apply good old-fashioned common sense and scepticism, experts advise|
|[Training MoEs at Scale with PyTorch.](https://pytorch.org/blog/training-moes/) |In order to write about scaling their MoE models to thousands of GPUs, the Mosaic team has teamed up with PyTorch. |
|[Investing in the Age of Generative AI.](https://eastwind.substack.com/p/investing-in-the-age-of-generative) |Though there is currently a "euphoria" surrounding investment, the generative AI business is already showing signs of fragility. |
|[Can AI boom drive Nvidia to a $4tn valuation despite investor doubt?](https://www.theguardian.com/technology/article/2024/jul/02/can-ai-boom-drive-nvidia-to-a-4tn-valuation-despite-investor-doubt) |Powerful new chips are on the way but there are questions over whether tech firm’s growth can be sustained |
|[AI scaling myths.](https://www.aisnakeoil.com/p/ai-scaling-myths) |It is improbable that LLMs will ever be able to achieve AGI through scaling on its own. Although scaling has been found to improve model capabilities, it largely improves confusion instead of emergent skills. Getting hold of high-quality training data is getting harder and harder. |
|[A discussion of discussions on AI bias.](https://danluu.com/ai-bias/) | The nature of AI bias has come under more scrutiny, with detractors claiming that biases in machine learning are demonstrated by the way models like as Playground AI occasionally change a user's ethnicity in photos. Some users refute this as a flaw or pertinent prejudice, pointing to instances in which Asian traits are overrepresented. The discussion touches on the wider ramifications of AI bias in many businesses. There is no easy answer to this complicated problem.|
|[The shape of information.](https://kucharski.substack.com/p/the-shape-of-information) |This article describes how to use binary logic to maximize scarce resources. |
|[why we no longer use LangChain for building our AI agents.](https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents) |Octomind's codebase and team productivity increased after it eschewed the LangChain framework for AI test automation in favor of more straightforward, modular building parts. It found that the high-level abstractions of LangChain were rigid, making development and maintenance more difficult. Octomind now benefits from a leaner architecture and faster iteration for its AI agent duties as a result of changing strategy. |
|[The Five Stages Of AI Grief.](https://www.noemamag.com/the-five-stages-of-ai-grief) | Benjamin Bratton, a professor at the University of California, San Diego and director of the Antikythera program at the Berggruen Institute, refers to the global response to artificial intelligence as a "Copernican Trauma," comparing it to historical changes that have reshaped humanity's understanding of itself. Bratton offers the following five stages of "AI grief" to describe how society would react to AI's evolution: from skepticism to integration into our conception of intelligence: denial, rage, bargaining, depression, and acceptance. He contends that rather than being a uniquely human story, the integration of AI represents a larger biological and technological evolutionary process.|
|[How to win at Enterprise AI — A playbook.](https://medium.com/@sanguit/how-to-win-at-enterprise-ai-a-playbook-4bdd714cf47e) |This AI-focused playbook describes AI adoption methods for enterprises, emphasizing the move from human-performed services to software-driven workflows known as "Service-as-a-software." It explores how these changes may affect business models, including performance-based pricing, and stresses how crucial workflow capture and AI accuracy are to the implementation process's success. The handbook also covers threats such as lateral attacks and emphasizes that in enterprise contexts, AI must show real performance, not simply potential. |
|[AI is disrupting Customer Support. Salesforce is feeling the pinch[.]()](https://amritaroy.substack.com/p/ai-is-disrupting-customer-support) | Customer support software providers like Salesforce and Zendesk are facing challenges as enterprises redirect their IT spending toward AI proof-of-concept projects. For traditional software suppliers, the increasing integration of solutions such as ChatGPT in customer assistance has resulted in longer payback periods due to higher customer acquisition expenses. The creativity of these businesses and the overall macroeconomic climate will determine how much money is invested in customer support software in the future.|
|[Contra Acemoglu on AI.](https://www.maximum-progress.com/p/contra-acemoglu-on-ai) | In contrast to more positive projections, economist Daron Acemoglu's working paper on AI proposes a modest 0.06% annual rise in TFP growth. He identifies four distinct ways that AI affects productivity, but he ignores the development of new labor-intensive goods and the further automation of existing processes, perhaps underestimating the whole economic potential of AI. His method is criticized for being unduly restrictive and for perhaps distorting the wider socioeconomic effects of AI developments.|
|[Inside the maths that drives AI.](https://www.nature.com/articles/d41586-024-02185-z) | Loss functions measure algorithmic errors in artificial-intelligence models, but there’s more than one way to do that. Here’s why the right function is so important.|
|[‘The disruption is already happening!’ Is AI about to ruin your favourite TV show?](https://www.theguardian.com/tv-and-radio/article/2024/jul/05/the-disruption-is-already-happening-is-ai-about-to-ruin-your-favourite-tv-show) |It won’t be long till everything from Drag Race to Keeping Up With the Kardashians could be written without humans – and you might be able to write yourself as the hero of a new show. But will robot TV ever be up to snuff? |
|[Can the climate survive the insatiable energy demands of the AI arms race?](https://www.theguardian.com/business/article/2024/jul/04/can-the-climate-survive-the-insatiable-energy-demands-of-the-ai-arms-race) |New computing infrastructure means big tech is likely to miss emissions targets but they can’t afford to get left behind in a winner takes all market |
|[Our attitudes towards AI reveal how we really feel about human intelligence.](https://www.theguardian.com/technology/article/2024/jul/03/ai-human-intelligence) |We’re in the untenable position of regarding the AI as alien because we’re already in the position of alienating each other |


# ML news: Week 24 - 30 June

## Research
|Link|description|
|---|---|
|[Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?](https://arxiv.org/abs/2406.13121) |reports that long-context LLMs can compete with state-of-the-art retrieval and RAG systems without explicit training on the tasks; suggests that compositional reasoning (needed in SQL-like tasks) is still challenging for these LLMs; and encourages further research on advanced prompting strategies. performs a thorough performance analysis of long-context LLMs on in-context retrieval and reasoning. first presents a benchmark with real-world tasks requiring 1M token context. |
|[PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers.](https://arxiv.org/abs/2406.12430) | improves decision-making using the iterative plan-then-RAG (PlanRAG) technique, which consists of two steps: The last phase determines whether a new plan for additional analysis is required and repeats earlier steps or makes a decision based on the data. 1) An LM creates the plan for decision making by reviewing the questions and data schema, and 2) the retriever creates the queries for data analysis; It is discovered that PlanRAG performs better than iterative RAG on the suggested Decision QA tasks. |
|[Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs.](https://arxiv.org/abs/2406.10209) | demonstrates how the goldfish loss resists memorization and keeps the model useful, but it may need to train for longer to more effectively learn from the training data. It is a modification of the next-token prediction objective called goldfish loss, which helps mitigate the verbatim generation of memorized training data. It uses a simple technique that excludes a pseudorandom subset of training tokens at training time.|
|[Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B.](https://arxiv.org/abs/2406.07394v2) | report having used an approach that combines LLMs with Monte Carlo Tree Search to achieve a mathematical Olympiad solution at the GPT-4 level. This approach aims to improve the system's performance in mathematical reasoning by enabling features like systematic exploration, self-refinement, and self-evaluation.|
|[From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries.](https://arxiv.org/abs/2406.12824) |aims to better understand how LLMs use external knowledge in place of parametric information when responding to factual queries. It finds that in a RAG pipeline, LLMs take a "shortcut" and exhibit a strong bias toward using only the context information and their parametric memory to answer the question. |
|[Tree Search for Language Model Agents.](https://jykoh.com/search-agents/paper.pdf) | reveals that performance scales with increased test-time computing. It is tested on interactive online environments and applied to GPT-4o to dramatically enhance performance. It suggests an inference-time tree search technique for LM agents to do exploration and enable multi-step reasoning.|
|[Evidence of a log scaling law for political persuasion with large language models.](https://arxiv.org/abs/2406.14508) |Super persuasion is the worry that models may become noticeably more persuasive as they get bigger. The idea that larger models aren't significantly more compelling than smaller models isn't supported by strong data. They might, nevertheless, be able to be adjusted to be more convincing. |
|[MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading.](https://arxiv.org/abs/2406.14537v1) |Reinforcement learning is used in MacroHFT, a novel method of high-frequency trading (HFT) in cryptocurrency markets, to enhance profitability and decision-making. |
|[Soft-QMIX: Integrating Maximum Entropy For Monotonic Value Function Factorization.](https://arxiv.org/abs/2406.13930v1) |Researchers have included a local Q-value learning method within a maximum entropy framework to enhance QMIX, a well-liked multi-agent reinforcement learning technique. |
|[eaL: Efficient RLHF Training for LLMs with Parameter Reallocation.](https://github.com/openpsi-project/realhf) |ReaLHF is a unique method that optimizes parallelization during training and dynamically redistribute parameters to improve reinforcement learning from human input (RLHF). |
|[AlphaFold2 structures guide prospective ligand discovery.](https://www.science.org/doi/10.1126/science.adn6354) | AlphaFold2 (AF2) models have had wide impact but mixed success in retrospective ligand recognition. We prospectively docked large libraries against unrefined AF2 models of the σ2 and serotonin 2A (5-HT2A) receptors, testing hundreds of new molecules and ...|
|[GPTs are GPTs: Labor market impact potential of LLMs.](https://www.science.org/doi/10.1126/science.adj0998) |OWe propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs. When accounting for current and likely future software developments that complement LLM capabilities, this share jumps to just over 46% of jobs. |
|[Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models.](https://arxiv.org/abs/2406.14848v1) | PE-Rank is a novel passage ranking method that leverages context compression through single passage embeddings to increase performance.|
|[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression.](https://github.com/thu-nics/moa) |By customizing sparse attention configurations for each head and layer, the Mixture of Attention (MoA) method maximizes sparse attention in large language models. |
|[GeoMFormer: A General Architecture for Geometric Molecular Representation Learning.](https://arxiv.org/abs/2406.16853v1) | A new Transformer-based model called GeoMFormer learns both equivariant and invariant properties to enhance molecular modeling.|
|[Making my local LLM voice assistant faster and more scalable with RAG.](https://johnthenerd.com/blog/faster-local-llm-assistant/) | Researchers classified data, precomputed embeddings, and dynamically generated examples to improve the efficiency and scalability of an LLM voice assistant.|
|[Retrieval Augmented Instruction Tuning for Open NER with Large Language Models.](https://arxiv.org/abs/2406.17305v1) | Using big language models, Retrieval Augmented Instruction Tuning (RA-IT) enhances information extraction.|
|[Data curation via joint example selection further accelerates multimodal learning.](https://arxiv.org/abs/2406.17711) |In pre-training, actively choosing the next best batch is a difficult and open problem. This research from DeepMind investigates how to match SOTA for a variety of tasks while using only 10% of FLOPs and hard mining negative samples. |
|[Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text.](https://imlixinyang.github.io/director3d-page/) | A system called Director3D was created to improve camera trajectory modeling and 3D scene production in the real world. Director3D creates lifelike 3D scenes from text descriptions by using a Multi-view Latent Diffusion Model and a Trajectory Diffusion Transformer.|
|[Prompt Engineering Tool.](https://github.com/teknium1/Prompt-Engineering-Toolkit) |An excellent prompting toolset that helps evaluate the effectiveness of various prompts, nearly completely composed of Sonnet 3.5. |
|[Meta Large Language Model Compiler: Foundation Models of Compiler Optimization.](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/) | Two language models that can decompile to LLVM IR and compile code to assembly have been made available by Meta. They received additional training after being trained on 546 billion tokens of superior quality data. They can accomplish 45% round trip disassembly performance and 77% optimized assembling performance.|

## News
|Link|description|
|---|---|
|[Geologists raise concerns over possible censorship and bias in Chinese chatbot.](https://www.theguardian.com/technology/article/2024/jun/24/geologists-censorship-bias-chinese-chatbot-geogpt) | GeoGPT developed as part of Chinese-funded earth sciences programme aimed at researchers in global south|
|[OpenAI acquires Rockset.](https://openai.com/index/openai-acquires-rockset) |Rockset is a robust database that supports both indexing and querying. The startup was acquired by OpenAI in order to enhance its infrastructure for retrieval. |
|[Snapchat AI turns prompts into new lens.](https://www.theverge.com/2024/6/19/24181965/snapchat-ai-prompt-custom-lens) | Snapchat’s upcoming on-device AI model could transform your background — and your clothing — in real time.|
|[HeyGen Raises $60M Series A to Scale Visual Storytelling for Businesses.](https://www.heygen.com/article/announcing-our-series-a) |HeyGen, an AI video generating platform, has raised $60 million in Series A funding to improve its studio-quality video creation and localization capabilities quickly and affordably. HeyGen, which just generated $35 million in ARR, strives to democratize visual storytelling for companies of all sizes. |
|[AI candidate running for Parliament in the U.K. says AI can humanize politics.](https://www.nbcnews.com/tech/tech-news/ai-candidate-running-parliament-uk-says-ai-can-humanize-politics-rcna156991) |Voters can talk to AI Steve, whose name will be on the ballot for the U.K.'s general election next month, to ask policy questions or raise concerns. |
|[Anthropic has a fast new AI model — and a clever new way to interact with chatbots .](https://www.theverge.com/2024/6/20/24181961/anthropic-claude-35-sonnet-model-ai-launch) |Claude 3.5 Sonnet is apparently Anthropic’s smartest, fastest, and most personable model yet. |
|[AIs are coming for social networks.](https://www.theverge.com/2024/6/18/24181196/butterflies-app-ai-chatbots-social-media) | An app called Butterflies puts a new spin on how we interact with AI. With Meta and others making similar moves, social media is about to get a lot weirder.|
|[OpenAI walks back controversial stock sale policies, will treat current and former employees the same.](https://www.cnbc.com/2024/06/24/openai-changes-secondary-stock-sale-rules-treats-ex-staffers-equally.html) |OpenAI has changed its policies toward secondary share sales to allow current and former employees to participate equally in its annual tender offers, CNBC has learned. All current and former staffers “will have the same sales limit” and be able to participate at the same time, OpenAI said in documents shared with stakeholders.|
|[Report: Amazon developing AI chatbot that would compete with ChatGPT and others.](https://www.geekwire.com/2024/report-amazon-developing-ai-chatbot-that-would-compete-with-chatgpt-and-others/) |Amazon is developing its own consumer-focused AI chatbot that would compete with OpenAI’s ChatGPT and could be revealed later this year, according to a report from Business Insider. |
|[Multi is joining OpenAI.](https://multi.app/blog/multi-is-joining-openai) |OpenAI continues its purchase binge by purchasing additional desktop-related infrastructure. |
|[Artificial Marketing Intelligence at your fingertips: MarTech startup Ability AI secures $1.1M pre-seed round funding to automate the process.](https://www.linkedin.com/pulse/artificial-marketing-intelligence-your-fingertips-martech-startup-avnye/) | |
|[Claude 3.5 suggests AI’s looming ubiquity could be a good thing.](https://www.theguardian.com/technology/article/2024/jun/25/anthropic-claude-ai-chatbot) |If you don’t like chatbots popping up everywhere, get ready to be peeved. But the latest version of Anthropic’s shows AI is becoming more useful – and, crucially, affordable |
|[Apple found in breach of EU competition rules.](https://www.theguardian.com/technology/article/2024/jun/24/apple-breach-eu-competition-rules-digital-markets-act) | European Commission finds iPhone maker broke new laws designed to protect smaller competitors against big tech platforms|
|[Etched is building an AI chip that only runs one type of model.](https://techcrunch.com/2024/06/25/etched-is-building-an-ai-chip-that-only-runs-transformer-models/) |Etched is among the many, many alternative chip companies vying for a seat at the table — but it’s also among the most intriguing.  |
|[Stability AI Secures Significant New Investment.](https://stability.ai/news/stability-ai-secures-significant-new-investment) |Stability AI was able to obtain a "significant infusion of capital" from both new and existing investors in addition to hiring a new CEO. |
|[Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings.](https://imbue.com/research/70b-intro/) | Earlier this year, we pre-trained and fine-tuned a 70B-parameter model that outperforms GPT-4o zero-shot on a range of reasoning and coding-related benchmarks and datasets. Our fine-tuned model, pre-trained on 2T tokens, roughly matches a fine-tuned Llama 3 70B, which was pre-trained on more than seven times as much data.|
|[OpenAI Pushes Back Voice Mode.](https://threadreaderapp.com/thread/1805716393524183136.html) | The sophisticated Voice Mode that OpenAI showcased in its Spring Update will go live in alpha form in late July for a limited group of ChatGPT Plus subscribers.|
|[Meta’s AI translation model embraces overlooked languages.](https://www.nature.com/articles/d41586-024-00964-2) |More than 7,000 languages are in use throughout the world, but popular translation tools cannot deal with most of them. A translation model that was tested on under-represented languages takes a key step towards a solution. |
|[Researchers fool university markers with AI-generated exam papers.](https://www.theguardian.com/education/article/2024/jun/26/researchers-fool-university-markers-with-ai-generated-exam-papers) |University of Reading project poses questions for integrity of coursework and take-home student assignments |
|[YouTube tries convincing record labels to license music for AI song generator.](https://arstechnica.com/ai/2024/06/youtube-tries-convincing-record-labels-to-license-music-for-ai-song-generator/) | Video site needs labels’ content to legally train AI song generators.|
|[Evolutionary Scale Raises $142m series A.](https://www.evolutionaryscale.ai/blog/esm3-release) | A biology startup called Evolutionary Scale has came out of stealth with significant funding. Additionally, it declared the release of ESM 3, its foundation model, a 98B parameter model trained for 10^24 Flops on 771B biological tokens. Using the model, it found a new luminous green protein that is not found in nature.|
|[Waymo One is now open to everyone in San Francisco.](https://waymo.com/blog/2024/06/waymo-one-is-now-open-to-everyone-in-san-francisco/) |With its driverless cars, Waymo One now makes it possible for anybody in San Francisco to request a ride. After providing tens of thousands of trips per week, the company is expanding. Its all-electric fleet helps it achieve its sustainability goals and boosts the local economy. Waymo claims that its cars are much less likely to be involved in collisions than those driven by humans, citing increased safety. |
|[ChatGPT on your desktop.](https://openai.com/chatgpt/mac/) |Users can now download the ChatGPT desktop software for macOS. |
|[https://www.theguardian.com/technology/article/2024/jun/27/ai-bill-gates-climate-targets-datacentres-energy.](https://www.theguardian.com/technology/article/2024/jun/27/ai-bill-gates-climate-targets-datacentres-energy) | Microsoft co-founder says efficiencies for technology and electricity grids will outweigh energy use by data centres|
|[Snap Lense Studio 5.0.](https://ar.snap.com/blog/genai-suite-lens-studio-5.0) | The GenAI suite, which Snap introduced with Lens Studio 5.0, is a fantastic development and a huge help for creating augmented reality apps.|
|[Instagram Launching An AI Studio.](https://www.theverge.com/2024/6/27/24187502/instagram-ai-studio-versions-chatbot) |Instagram's "AI Studio" enables developers to create self-aware AI chatbots. In the US, an early test of it is presently underway. |
|[Dust raises $16m series A.](https://blog.dust.tt/dust-seriesa-sequoia-leading-ai-platform/) |Dust, one of the first modern-day chaining and agency companies, raised more money after surpassing $1 million in annual revenue. |
|[ElevenLabs launches iOS app that turns ‘any’ text into audio narration with AI.](https://venturebeat.com/ai/elevenlabs-launches-ios-app-that-turns-any-text-into-audio-narration-with-ai/) |"ElevenLabs Reader: AI Audio," the company's debut iOS app, enables users to listen on the go by turning text files or web links into audio narration. |

## Resources
|Link|description|
|---|---|
|[Open-Sora 1.2 Report.](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_03.md) |a 1.1B parameter model trained on over 30 million data points, this open-source video generation model can produce 16-second 720p videos. It also features an improved diffusion model and video compression network for both temporal and spatial compression, which lowers training costs and improves controllability of the generations. |
|[LLM101n: Let's build a Storyteller.](https://github.com/karpathy/LLM101n) |An outline for a new course that Andrej Karpathy is working on can be found in a new repository. It entails creating a narrative-capable aligned language model. Code, video lectures, and other learning resources are included in the course. |
|[AutoCodeRover: Autonomous Program Improvement.](https://github.com/nus-apr/auto-code-rover) | AutoCodeRover is a new technology that combines sophisticated code search methods with big language models to automate software enhancements, such as feature additions and problem fixes.|
|[NLUX.](https://github.com/nlkitai/nlux) | NLUX is React and JavaScript open-source library for building conversational AI interfaces. It makes it super simple to build web applications powered by Large Language Models (LLMs) and AI. With just a few lines of code, you can add conversational AI capabilities and interact with your favourite AI models.|
|[Claudette.](https://claudette.answer.ai/) |Claudette is a higher-level and easier-to-use way to interact with Claude. |
|[top CVPR 2024 papers.](https://github.com/SkalskiP/top-cvpr-2024-papers) |Computer Vision and Pattern Recognition is a massive conference. In 2024 alone, 11,532 papers were submitted, and 2,719 were accepted. I created this repository to help you search for crème de la crème of CVPR publications. |
|[TTS in 7000 Languages.](https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v3.0) |Recently, Toucan published a collection of new text-to-speech models that are now compatible with all ISO-639-3 standard languages. |
|[ParaLLM: 1300+ tok/s on a MacBook.](https://willcb.com/blog/parallm/) |When batch parallel KV cache is implemented in MLX, inference times for the creation of synthetic data and model completions are significantly sped up. |
|[Train vision models in TRL .](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) | Transformers can be trained using reinforcement learning with the help of TRL, a Hugging Face library. You may apply the same procedure for vision-based language models, such as LLaVA, using this example.|
|[Rethinking Remote Sensing Change Detection With A Mask View.](https://arxiv.org/abs/2406.15320v1) | Two new models for remote sensing change detection—CDMask and CDMaskFormer—are presented in this study.|
|[llama.ttf.](https://fuglede.github.io/llama.ttf/) | This article explains how to use a font file to run a little Llama language model.|
|[june.](https://github.com/mezbaul-h/june) |june is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers. |
|[Building a personalized code assistant with open-source LLMs using RAG Fine-tuning.](https://www.together.ai/blog/rag-fine-tuning) | AI and Morph Labs collaborated to create an excellent blog post about optimizing models for retrieval enhanced generation. They also demonstrate a few applications of generated data.|
|[EvalAlign: Evaluating Text-to-Image Models through Precision Alignment of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations.](https://arxiv.org/abs/2406.16562v1) |A novel metric called EvalAlign was created to enhance the assessment of generative models that convert text to images. EvalAlign provides fine-grained accuracy and stability in contrast to current measures. It emphasizes text-image alignment and image faithfulness. |
|[Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models.](https://huggingface.co/blog/finetune-florence2) | Florence-2, released by Microsoft in June 2024, is a foundation vision-language model. This model is very attractive because of its small size (0.2B and 0.7B) and strong performance on a variety of computer vision and vision-language tasks. Florence supports many tasks out of the box: captioning, object detection, OCR, and more.|
|[Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity.](https://pytorch.org/blog/accelerating-neural-network-training/) |Specifically designed kernels have been created by the PyTorch team to utilize sparse cores, which are typically exclusively used for inference. |
|[FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models.](http://haonanqiu.com/projects/FreeTraj.html) |Diffusion models are used in FreeTraj, a tuning-free technique for controlling motion trajectories in video creation. To direct the generated content, it adjusts the attention mechanisms and noise sampling. |
|[OpenGlass - Open Source Smart Glasses.](https://github.com/BasedHardware/OpenGlass) | Turn any glasses into hackable smart glasses with less than $25 of off-the-shelf components. Record your life, remember people you meet, identify objects, translate text, and more.|
|[An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability.](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) |The Golden Gate Claude served as a potent illustration of how to influence and evaluate models using SAEs. This work includes some sample code for training these models and an easy-to-understand explanation of how it operates. |
|[RES-Q.](https://github.com/qurrent-ai/res-q) | A new benchmark called RES-Q is designed to evaluate how well huge language models can modify code repositories using instructions in natural language.|
|[Balancing Old Tricks with New Feats: AI-Powered Conversion From Enzyme to React Testing Library at Slack.](https://slack.engineering/balancing-old-tricks-with-new-feats-ai-powered-conversion-from-enzyme-to-react-testing-library-at-slack/) |Using a hybrid method, Slack developers used AI Large Language Models with Abstract Syntax Tree transformations to automate the translation of more than 15,000 unit tests from Enzyme to React Testing Library. The team utilized Anthropic's Claude 2.1 AI model in conjunction with DOM tree capture for React components to achieve an 80% success rate in automatic conversions. This ground-breaking project demonstrates Slack's dedication to using AI to improve developer productivity and experience. It's a part of the continuous attempts to remain ahead of the always changing frontend scene. |
|[R2R.](https://github.com/SciPhi-AI/R2R) | R2R was designed to bridge the gap between local LLM experimentation and scalable, production-ready Retrieval-Augmented Generation (RAG). R2R provides a comprehensive and SOTA RAG system for developers, built around a RESTful API for ease of use.|
|[Internist.ai 7b.](https://huggingface.co/internistai/base-7b-v0.2) |Internist.ai 7b is a medical domain large language model trained by medical doctors to demonstrate the benefits of a physician-in-the-loop approach. The training data was carefully curated by medical doctors to ensure clinical relevance and required quality for clinical practice. |
|[Finding GPT-4’s mistakes with GPT-4.](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/) |CriticGPT, a model based on GPT-4, writes critiques of ChatGPT responses to help human trainers spot mistakes during RLHF |
|[ALPBench: A Benchmark for Active Learning Pipelines on Tabular Data.](https://arxiv.org/abs/2406.17322v1) |A program called ALPBench was created to standardize active learning query benchmarks. |
|[Introducing AuraSR - An open reproduction of the GigaGAN Upscaler.](https://blog.fal.ai/introducing-aurasr-an-open-reproduction-of-the-gigagan-upscaler-2/) |FAL recently made AuraSR, a high resolution picture upscaler, open-sourced. Even with repeated applications, it may upscale by 4x with just one forward pass. AuraSR performs admirably with created photos. |
|[Point-SAM: Promptable 3D Segmentation Model for Point Clouds.](https://point-sam.github.io/) |Point-SAM, a transformer-based 3D segmentation model, has been introduced by researchers in response to the increasing demand for comprehensive 3D data. |
|[GenIR-Survey.](https://github.com/ruc-nlpir/genir-survey) | This survey explores generative information retrieval (GenIR), a novel approach to information retrieval that shifts from conventional search techniques to ones that generate results dynamically.|
|[Gemma 2.](https://www.kaggle.com/models/google/gemma-2) | Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.|
|[MatText: Do Language Models Need More than Text & Scale for Materials Modeling?](https://arxiv.org/abs/2406.17295v1) |MatText is a collection of benchmarking tools and datasets intended to assess the effectiveness of language models in the field of materials science. |
|[mamba2.](https://github.com/okarthikb/state-space-models/blob/main/models/mamba2.py) |A quick implementation of Mamba 2 |

## Perspectives
|Link|description|
|---|---|
|[The Long View on AI.](https://www.maximum-progress.com/p/the-long-view-on-ai) | AI has the potential to cause tremendous growth rates and technological improvements, according to historical statistics. Society will probably be able to adjust to these rapid changes just as it has in the past.|
|[AI’s Hidden Opportunities: Shawn "swyx" Wang on New Use Cases and Career.](https://www.heavybit.com/library/article/ai-hidden-opportunities-for-software-developers-swyx) |Well-known developer Shawn "swyx" Wang discusses untapped potential for conventional software professionals wishing to go into artificial intelligence. In particular, examining how to enhance existing tools, use AI to summarization, and more. |
|[Apple Intelligence.](https://daringfireball.net/2024/06/wwdc24_apple_intelligence) | Rather than developing stand-alone AI products, Apple has incorporated generative AI into its core apps, improving services like Mail classification, Safari summaries, and Siri's functioning. This demonstrates the company's focus on user control and privacy.|
|[Apple intelligence and AI maximalism.](https://www.ben-evans.com/benedictevans/2024/06/20/apple-intelligence) |Apple has showed a bunch of cool ideas for generative AI, but much more, it is pointing to most of the big questions and proposing a different answer - that LLMs are commodity infrastructure, not platforms or products. |
|[How To Solve LLM Hallucinations.](https://morethanmoore.substack.com/p/how-to-solve-llm-hallucinations) | Lamini has created Memory Tuning, which effectively embeds particular facts into models without sacrificing general knowledge and reduces hallucinations by 95%.|
|[AI machine translation tools must be taught cultural differences too.](https://www.nature.com/articles/d41586-024-02091-4) |But to successfully preserve or revitalize minority languages, the scope of large-language-model (LLM) training needs to be broadened. |
|[Misinformation might sway elections — but not in the way that you think.](https://www.nature.com/articles/d41586-024-01696-z) | Rampant deepfakes and false news are often blamed for swaying votes. Research suggests it’s hard to change people’s political opinions, but easier to nudge their behaviour.|
|[How I’m using AI tools to help universities maximize research impacts.](https://www.nature.com/articles/d41586-024-02081-6) |Artificial-intelligence algorithms could identify scientists who need support with translating their work into real-world applications and more. Leaders must step up. |
|[The Future of LLM-Based Agents: Making the Boxes Bigger.](https://www.arcus.co/blog/ai-agents-pt-2) |Long-term planning and system-level resilience are two essential strategies that assist move Agents from the playground into the real world, and they are discussed in this post. These introduce the ability to create plans of a higher level for the Agents, allowing for adaptability in the middle of an episode. They also introduce systems techniques to intelligently orchestrate the models, resulting in increased performance and accuracy. |
|[Apple, Microsoft Shrink AI Models to Improve Them.](https://spectrum.ieee.org/small-language-models-apple-microsoft) | Large language models are becoming less popular as IT companies shift their focus to more efficient small language models (SLMs). Apple and Microsoft have introduced models with far fewer parameters that nonetheless perform comparably or even better in benchmarks. According to the CEO of OpenAI, we're past the LLM era since SLMs have benefits including greater accessibility for smaller entities, local device operation, and potential insights into human language acquisition. Even though SLMs are narrower in scope, their performance is enhanced by training them on high-quality, or "textbook-quality" data.|
|[Are Tech-Enabled Vertical Roll-Ups the Future or the Past?.](https://www.tidemarkcap.com/post/are-tech-enabled-vertical-roll-ups-the-future-or-the-past) | The ability to generate excess cash flows through operational efficiencies is a prerequisite for roll-up methods. It's possible that the development of AI offers a new lever that fully unlocks the rollup strategy. Are rollups for SMBs and verticals the future? Two different perspectives on this issue are presented in this post.|

# ML news: 16 - 23  June

## Research
|Link|description|
|---|---|
|[Discovering Preference Optimization Algorithms with and for Large Language Models.](https://arxiv.org/abs/2406.08414) |suggests an algorithm that adaptively combined logistic and exponential losses; this approach eliminates the need for human intervention by prompting an LLM to suggest and implement preference optimization loss functions based on previously assessed performance metrics. It also suggests an LLM-driven objective discovery of state-of-the-art preference optimization.  |
|[SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals.](https://arxiv.org/abs/2406.04784) |a framework to increase the high-level goal-achieving capabilities of an LLM-based agent; during interaction with the environment, the framework adaptively decomposes a high-level goal into a tree structure of useful subgoals; enhances performance on a variety of tasks, including cooperative, competitive, and deferred feedback environments. |
|[Mixture-of-Agents Enhances Large Language Model Capabilities.](https://arxiv.org/abs/2406.04692) | a strategy that beats GPT-4o on AlpacaEval 2.0, MT-Bench, and FLASK by utilizing the combined strengths of several LLMs through a Mixture-of-Agents methodology; layers are constructed with numerous LLM agents, and each agent builds on the outputs of other agents in the previous levels.|
|[Transformers meet Neural Algorithmic Reasoners.](https://arxiv.org/abs/2406.09308) | Tokens in the LLM can now cross-attend to node embeddings from a GNN-based neural algorithmic reasoner (NAR) thanks to a new hybrid design; the resulting model, named TransNAR, shows gains in OOD reasoning across algorithmic challenges.|
|[Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching.](https://arxiv.org/abs/2406.06326) | increases an LLM's capacity to learn new information from raw documents through self-teaching; the process consists of three steps: 1) a self-teaching component that enhances documents with a series of knowledge-intensive tasks emphasizing comprehension, memorization, and self-reflection; 2) the model is configured to continuously learn using only the new documents, aiding in the thorough acquisition of new knowledge; and 3) the deployed model is used to learn new information from new documents while evaluating its QA skills.|
|[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.](https://arxiv.org/abs/2406.09403) |a framework that gives a multimodal LLM access to a visual sketchpad and drawing tools; it can give a model, such as GPT-4, the ability to create intermediate sketches in order to reason over complex tasks; over strong base models without sketching, it performs better on many tasks; on all the tasks tested, GPT-4 equipped with SketchPad sets a new state of the art. |
|[Mixture of Memory Experts.](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf) |claims to enable scaling to a high number of parameters while keeping the inference cost fixed. It suggests a method to significantly reduce hallucination (10x) by tuning millions of expert adapters (e.g., LoRAs) to learn exact facts and retrieve them from an index at inference time. The memory experts are specialized to ensure faithful and factual accuracy on the data it was tuned on. |
|[Multimodal Table Understanding.](https://arxiv.org/abs/2406.08100) | presents Table-LLaVa 7B, a multimodal LLM for multimodal table understanding; it produces a large-scale dataset MMTab, comprising table images, instructions, and tasks; it is comparable with GPT-4V and greatly outperforms existing MLLMs on numerous benchmarks. |
|[Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement.](https://arxiv.org/abs/2406.07138) |suggests a training-efficient way to extend LLMs to longer context lengths (e.g., 4K -> 256K); it uses a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning; the approach helps to alleviate the so-called "Lost-in-the-Middle" problem in long-context LLMs. suggests a method to tune an LLM to effectively utilize information from the middle part of the context. |
|[Simple and Effective Masked Diffusion Language Models.](https://s-sahoo.com/mdlm/) | Easy diffusion model to model language. It functions fairly well and generates out of order.|
|[MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding.](https://arxiv.org/abs/2406.09297v1) |A novel technique that dramatically lowers memory consumption during auto-regressive inference in transformers is called Multi-Layer Key-Value (MLKV) sharing. |
|[Understanding Hallucinations in Diffusion Models through Mode Interpolation.](https://arxiv.org/abs/2406.09358v1) | This study looks into the reasons behind "hallucinations"—images that never were in the training set—that are produced by diffusion-based picture generation models.|
|[Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs.](https://arxiv.org/abs/2406.09136v1) |A technique called Chain of Preference Optimization (CPO) helps large language models (LLMs) become more adept at logical reasoning. CPO matches the reasoning steps of Chain-of-Thought (CoT) decoding with the optimal routes of ToT by fine-tuning LLMs using search trees from the Tree-of-Thought (ToT) technique. |
|[Language Modeling with Editable External Knowledge.](https://arxiv.org/abs/2406.11830v1) |ERASE is a novel approach to updating language models. Unlike conventional methods that emphasize enhancing retrieval during prediction, ERASE incrementally deletes or rewrites entries in the knowledge base as new documents are incorporated. |
|[Duoduo CLIP: Efficient 3D Understanding with Multi-View Images.](https://3dlg-hcvc.github.io/DuoduoCLIP) |Duoduo CLIP is a 3D representation learning model that utilizes multi-view images rather than point-clouds for its training and analysis. |
|[CAMixerSR: Only Details Need More "Attention".](https://arxiv.org/abs/2402.19289v2) |CAMixerSR enhances image resolution by intelligently applying convolution to simpler areas and using deformable window-attention for intricate textures. |
|[‘Fighting fire with fire’ — using LLMs to combat LLM hallucinations.](https://www.nature.com/articles/d41586-024-01641-0) | The number of errors produced by an LLM can be reduced by grouping its outputs into semantically similar clusters. Remarkably, this task can be performed by a second LLM, and the method’s efficacy can be evaluated by a third. Associate article is [here](https://www.nature.com/articles/s41586-024-07421-0)|
|[Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.](https://huggingface.co/microsoft/Florence-2-large-ft) | Microsoft has published a collection of tiny VLMs under an MIT license that perform noticeably better in captioning, bounding, and classification than much larger models.|
|[Logit Prisms: Decomposing Transformer Outputs for Mechanistic Interpretability.](https://neuralblog.github.io/logit-prisms/) |The logit lens approach has been improved by decomposing logit outputs into contributions from different model components. This aids in comprehending the decision-making process of transformer models. This method, which employs "prisms" for residual streams, attention layers, and MLP layers, demonstrates how these components affect predictions and offers insights into the tasks that the gemma-2b model does, such as factual retrieval and arithmetic. |
|[PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers.](https://arxiv.org/abs/2406.12430v1) |Using sophisticated data analysis, decision QA is a new role for LLMs that identifies the optimal decisions. |
|[ChangeViT: Unleashing Plain Vision Transformers for Change Detection.](https://arxiv.org/abs/2406.12847v1) | A methodology called ChangeViT makes use of vision transformers (ViTs) to identify significant environmental changes in remote sensing photos.|
|[LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging.](https://arxiv.org/abs/2406.12837v1) |LayerMerge is a novel technique that simultaneously prunes activation functions and convolution layers to increase neural network efficiency. |
|[Adversarial Attacks on Multimodal Agents.](https://chenwu.io/attack-agent/) |Vision-enabled language models (VLMs) such as Gemini and GPT-4o enable autonomous agents to perform activities like code editing and buying. This investigation demonstrates how susceptible these agents are to malevolent attacks. |
|[TimeSieve: Extracting Temporal Dynamics through Information Bottlenecks.](https://arxiv.org/abs/2406.05036v1) | A novel model called TimeSieve was created to address typical problems in time series forecasting.|

## News
|Link|description|
|---|---|
|[Apple to ‘Pay’ OpenAI for ChatGPT Through Distribution, Not Cash.](https://www.bloomberg.com/news/articles/2024-06-12/apple-to-pay-openai-for-chatgpt-through-distribution-not-cash) | The collaboration between Apple and OpenAI isn't anticipated to bring in a significant amount of money for either company, at least not right away. Apple is not paying OpenAI as part of the agreement because it feels that integrating OpenAI's technology and brand into its products is as valuable as or more valuable than financial compensation. The agreement isn't exclusive; Apple is already talking about providing additional chatbot choices. In the long run, Apple intends to profit from AI by entering into revenue-sharing contracts with AI partners.|
|[AI will make money sooner than you’d think, says Cohere CEO Aidan Gomez.](https://www.theverge.com/24173858/ai-cohere-aidan-gomez-money-revenue-llm-transformers-enterprise-stochastic-parrot) |Enterprise is the pathway to profit, Gomez says, but maybe don’t ask it to do medicine quite yet. |
|[Fake beauty queens charm judges at the Miss AI pageant.](https://www.npr.org/2024/06/09/nx-s1-4993998/the-miss-ai-beauty-pageant-ushers-in-a-new-type-of-influencer) |An AI model from Romania named Aiyana Rainbow is a finalist in the first Miss AI pageant, which showcases AI-generated models on social media. The event is a part of "The FanVue World AI Creator Awards," which is organized by FanVue and highlights the talent of AI creators who can create captivating content without having to be the face of the work. The $5,000 prize package for Miss AI will include mentorship and support from the public relations community. At the end of June, the outcomes will be made public. |
|[Elon Musk reconsiders phone project after Apple Intelligence OpenAI integration.](https://www.teslarati.com/elon-musk-reconsiders-phone-apple-intelligence-openai-chatgpt-integration/) | Elon Musk threatened to forbid any Apple devices from being used on the properties of his firms in response to Apple integrating OpenAI ChatGPT on a few of its devices.|
|[Microsoft’s star AI chief peers into OpenAI’s code, highlighting an unusual rivalry.](https://www.semafor.com/article/06/14/2024/microsoft-ai-ceo-mustafa-suleyman-audits-openais-code) |Primarily, OpenAI was established as a safety net against DeepMind, the AI startup that Google purchased in 2014. However, Mustafa Suleyman, a co-founder of DeepMind, has recently been taking on an unimaginable task: delving into OpenAI's crown jewels, the proprietary algorithms that power foundation models like GPT-4, according to people familiar with the situation. This is due to the fact that Suleyman is currently Microsoft's head of AI initiatives. As part of Microsoft's multibillion-dollar investment in OpenAI, the corporation possesses the intellectual property rights to its software. |
|[Amazon says it’ll spend $230 million on generative AI startups.](https://techcrunch.com/2024/06/13/amazon-says-itll-spend-230-million-on-generative-ai-startups/) |Amazon says that it will commit up to $230 million to startups building generative AI-powered applications. |
|[McDonald’s ends AI drive-thru trial as fast-food industry tests automation.](https://www.theguardian.com/business/article/2024/jun/17/mcdonalds-ends-ai-drive-thru) |Companies have touted AI as future of the industry, but technology has also resulted in viral videos of wrong orders |
|[Balance effects of AI with profits tax and green levy, says IMF.](https://www.theguardian.com/business/article/2024/jun/17/ai-profits-tax-green-levy-imf-carbon-emissions) | Governments faced with economic upheaval caused by artificial intelligence should consider fiscal policies including taxes on excess profits and a green levy to atone for AI-related carbon emissions, according to the International Monetary Fund.|
|[Introducing Gen-3 Alpha.](https://runwayml.com/blog/introducing-gen-3-alpha/) | Runway has developed a brand-new, incredibly potent video generation model. Many of the current functions on its platform will be powered by it. You can find examples at the given URL.|
|[DeepMind’s new AI generates soundtracks and dialogue for videos.](https://techcrunch.com/2024/06/17/deepminds-new-ai-generates-soundtracks-and-dialog-for-videos) | V2A is an AI system that DeepMind is developing to create synchronized soundtracks for videos. It generates music, sound effects, and dialogue using diffusion models trained on audio, dialogue transcripts, and video clips.|
|[Giant Chips Give Supercomputers a Run for Their Money .](https://spectrum.ieee.org/cerebras-wafer-scale-engine) |The California-based business Cerebras has proven in molecular dynamics calculations that their second-generation wafer-scale engine outperforms the fastest supercomputer in the world by a large margin. Additionally, it can infer sparse huge language models with no loss of accuracy at one-third of the energy cost of a complete model. The hardware of Cerebras allows for quick memory access and interconnects, which make both accomplishments possible. Cerebras aims to expand the scope of its wafer-scale engine applications to encompass a broader range of issues, such as airflow models surrounding cars and molecular dynamics simulations of biological processes. |
|[Nvidia becomes world’s most valuable company amid AI boom.](https://www.theguardian.com/technology/article/2024/jun/18/nvidia-valuation-most-valuable) | Chipmaker dethrones Microsoft and Apple as stock market surge boosts valuation above $3.34tn|
|[The ‘Godfather of AI’ quit Google a year ago. Now he’s emerged out of stealth to back a startup promising to use AI for carbon capture.](https://fortune.com/europe/2024/06/18/godfather-ai-geoffrey-hinton-quit-google-year-ago-emerged-stealth-back-startup-cuspai-ai-carbon-capture/) |Renowned AI researchers Geoff Hinton and Max Welling have gathered a talented team to develop AI systems aimed at advancing material science for carbon capture. |
|[Nvidia Conquers Latest AI Tests​.](https://spectrum.ieee.org/mlperf-nvidia-conquers) |Nvidia's Hopper architecture-based systems excelled in two recent MLPerf AI benchmark tests, which assess the fine-tuning of large language models and the training of graph neural networks. |
|[Perplexity AI searches for users in Japan, via SoftBank deal.](https://techcrunch.com/2024/06/17/softbank-ties-up-with-perplexity/) |Perplexity is capitalizing on its strategic partnership with SoftBank to broaden its presence in Japan. As part of this initiative, it is providing a free year of its premium AI-powered search engine, Perplexity Pro. SoftBank's goal is to draw users by offering AI services without creating internal solutions. With a valuation of $1 billion, Perplexity is expanding its funding and investor base, which features prominent tech leaders and venture firms. |
|[Introducing Local III.](https://changes.openinterpreter.com/log/local-iii) | The open-source local agent, Open Interpreter, has recently received a significant upgrade. It now has the capability to control the computer seamlessly and operates entirely offline and locally.|
|[Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs.](https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms) | LlamaIndex has launched the Property Graph Index, significantly improving knowledge graph capabilities with enhanced modeling, storage, and querying features. This new index enables flexible graph construction and supports schema-guided, implicit, and free-form entity extraction. It also integrates with vector databases for hybrid searches and offers querying options through keyword expansion, vector similarity, Cypher queries, and custom traversal.|
|[Decagon launches with $35m raised from Accel and a16z.](https://decagon.ai/blog/series-a) |Decagon is developing human-like AI agents for customer support and has recently secured $30 million in Series A funding from Accel, along with $5 million in seed funding from a16z. Decagon's product manages global support for companies such as Eventbrite, Rippling, Webflow, BILT, and Substack. |
|[London premiere of movie with AI-generated script cancelled after backlash.](https://www.theguardian.com/film/article/2024/jun/20/premiere-movie-ai-generated-script-cancelled-backlash-the-last-screenwriter-prince-charles-cinema) | Plans to show The Last Screenwriter, whose script is credited to ‘ChatGPT 4.0’, prompted complaints although the film-makers insist the feature is ‘a contribution to the cause’|
|[OpenAI’s former chief scientist is starting a new AI company.](https://www.theverge.com/2024/6/19/24181870/openai-former-chief-scientist-ilya-sutskever-ssi-safe-superintelligence) |Ilya Sutskever is launching Safe Superintelligence Inc., an AI startup that will prioritize safety over ‘commercial pressures.’ |
|[Claude 3.5 Sonnet.](https://www.anthropic.com/news/claude-3-5-sonnet) |At a fifth of the cost, Claude 3.5 Sonnet outperforms Opus in performance. Plus, it's the greatest vision model out there right now. This demonstrates how much the frontier models have progressed. |
|[Apple researchers add 20 more open-source models to improve text and image AI.](https://appleinsider.com/articles/24/06/19/apple-researchers-add-20-more-open-source-models-to-improve-text-and-image-ai) |With 20 Core Machine Learning models that Apple has added to the Hugging Face open-source AI repository, the repository now includes a wider selection of public models with improved image classification and depth segmentation. These donations come after Apple earlier in the year released the four OpenELMs to Hugging Face and the Ferret big language model. The action shows Apple's dedication to developing AI capabilities and its growing involvement with the AI research community. |
|[Factory Raises $15M Series A from Sequoia.](https://www.factory.ai/news/series-a-announcement) |Led by Sequoia Capital, Factory has raised $15 million in Series A funding to grow its workforce and improve its Droids software development toolset, which leverages artificial intelligence. Its products are rapidly expanding its customer base and setting new benchmarks on the SWE-bench AI coding benchmark. With Factory, software engineering will be increasingly automated, cutting down on laborious processes and speeding up development cycles. |
|[Optimizing AI Inference at Character.AI.](https://research.character.ai/optimizing-inference/) |Twenty percent of Google's search volume, or 20,000 questions per second, are answered by Character AI. It operates this effectively thanks to several advancements. |
|[Apple delays launch of AI-powered features in Europe, blaming EU rules.](https://www.theguardian.com/technology/article/2024/jun/21/apple-ai-europe-regulation) | Apple says competition rules that require functionality with rival products would compromise privacy and security|

## Resources
|Link|description|
|---|---|
|[Nemotron-4 340B.](https://research.nvidia.com/publication/2024-06_nemotron-4-340b) |offers a reward model to filter data based on many qualities and an instruct model to generate high-quality data; exhibits impressive results on widely-used benchmarks such as MMLU and GSM8K; It competes with GPT-4 in a number of activities, such as scoring highly in multi-turn chat; Together with the base model, a preference data is also made available. |
|[Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs.](https://arxiv.org/abs/2406.09324v1) |Determining ways to incorporate search into language model creation is now the Holy Grail of study. This work is quite encouraging as it demonstrates that on math performance, tiny models with search can match considerably more powerful models. |
|[MCTSr: Mathematic as a Blackbox for LLM.](https://github.com/trotsky1997/MathBlackBox) | |
|[VideoGPT.](https://github.com/mbzuai-oryx/videogpt-plus) | To improve video understanding, a model called VideoGPT+ combines image and video encoders. While video encoders offer temporal context, image encoders capture finely detailed spatial information.|
|[Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach.](https://linlin-dev.github.io/project/RSG.html) | In order to enhance Scene Graph Generation (SGG) for very-high-resolution satellite imaging (VHR SAI), this research introduces a new dataset and methodology.|
|[LLM.Mojo.](https://github.com/dorjeduck/llm.mojo) |This project is a port of Andrej Karpathy's llm.c to Mojo, currently in beta and subject to changes. |
|[Depth Anything V2.](https://arxiv.org/abs/2406.09414) | With the use of artificial data, the new Depth Anything model was trained, and its performance on intricate scenes has significantly increased.|
|[DeepSeek-Coder-V2.](https://github.com/deepseek-ai/DeepSeek-Coder-V2) |Robust DeepSeek Coder achieves scores of 90+ on HumanEval and matches GPT-4 Turbo on numerous other difficult benchmarks. It is free for business usage and accessible via an API. |
|[HelpSteer2: Open-source dataset for training top-performing reward models.](https://arxiv.org/abs/2406.08673) |Along with an excellent paper about training reward models to match model output to human preferences, Nvidia has made available a dataset and procedure. |
|[Differentiable rasterization.](https://srush.github.io/DiffRast) | Given a program that produces a vector representation of an image (think SVG), rasterization turns it into a pixel representation (think PNG). Everything ought to be adjustable. This article explains how to write SVG light that is differentiable.|
|[LARS - The LLM & Advanced Referencing Solution.](https://github.com/abgulati/LARS) |LARS is an application that enables you to run LLM's (Large Language Models) locally on your device, upload your own documents and engage in conversations wherein the LLM grounds its responses with your uploaded content. |
|[Beyond the Basics of Retrieval for Augmenting Generation.](https://parlance-labs.com/education/rag/ben.html) |The RAGatouille creator delivered a great discussion about COLBERT, some of the open issues, and how to significantly increase RAG performance. |
|[TokenCost.](https://github.com/AgentOps-AI/tokencost) |Tokencost helps calculate the USD cost of using major Large Language Model (LLMs) APIs by calculating the estimated cost of prompts and completions. |
|[GaiaNet node.](https://github.com/GaiaNet-AI/gaianet-node) | Install and run your own AI agent service.|
|[Meta Chameleon.](https://github.com/facebookresearch/chameleon) |Chameleon is an early fusion model that processes images and text tokens concurrently. The team published the paper a few weeks ago and has now released model checkpoints along with inference code. |
|[OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations.](https://github.com/princeton-vl/ogni-dc) | OGNI-DC is a new framework for depth completion that employs "Optimization-Guided Neural Iterations" (OGNI). This method refines a depth gradient field and incorporates the depth gradients into a depth map.|
|[Subobject-level Image Tokenization.](https://arxiv.org/abs/2402.14327v2) |Subobject tokenization is a novel approach for vision models to interpret images. Rather than dividing images into fixed square patches, this method allows models to analyze images by identifying meaningful segments, such as parts of objects. |
|[Introduction to Granite Code Models.](https://github.com/ibm-granite/granite-code-models/tree/main) |We introduce the Granite series of decoder-only code models for code generative tasks (e.g., fixing bugs, explaining code, documenting code), trained with code written in 116 programming languages. A comprehensive evaluation of the Granite Code model family on diverse tasks demonstrates that our models consistently reach state-of-the-art performance among available open source code LLMs.  |
|[FireFunction V2: Fireworks Function Calling Model.](https://huggingface.co/fireworks-ai/firefunction-v2) |Open model that matches GPT4-o on function calling benchmarks trained on top of Llama 3 70B. |
|[Argilla.](https://github.com/argilla-io/argilla) |For AI developers and subject matter experts who need complete data ownership, high-quality outputs, and overall efficiency, Argilla offers a platform for cooperation. |
|[TroL: Traversal of Layers for Large Language and Vision Models.](https://github.com/byungkwanlee/trol) |Large language and vision models (LLVMs) with sizes of 1.8B, 3.8B, and 7B parameters are part of the new TroL family of efficient LLVMs. |
|[Dot.](https://github.com/alexpinel/Dot) |A stand-alone open-source program designed to be simple to use for local LLMs, and specifically RAG, to interact with files and documents in a manner similar to Nvidia's Chat with RTX. |
|[WebCanvas: Benchmarking Web Agents in Online Environments.](https://github.com/imeanai/webcanvas) |WebCanvas is a pioneering online evaluation framework designed to address the dynamic nature of web interactions. It provides a realistic assessment of autonomous web agents by utilizing live web environments and emphasizing task completion through the identification of key nodes. |
|[CIFAR-10 Airbench.](https://github.com/KellerJordan/cifar10-airbench) |A benchmark for image classification is CIFAR-10. In a remarkably short amount of time, this algorithm offers a training setting that yields good performance. |
|[Cost Of Self Hosting Llama-3 8B-Instruct.](https://blog.lytix.co/posts/self-hosting-llama-3) |Compared to using ChatGPT, self-hosting an LLM such as Llama-3 8B-Instruct can be much more expensive, costing approximately $17 per million tokens, while ChatGPT just costs $1 per million tokens. It is possible to lower the cost of self-hosted hardware to less than $0.01 per million tokens, but it would take about 5.5 years for the initial investment to pay for itself. |
|[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models.](https://yongtaoge.github.io/projects/geobench/) |Modern surface normal estimate and depth models are assessed using a new benchmark. |
|[An Empirical Study of Mamba-based Language Models.](https://huggingface.co/nvidia/mamba2-hybrid-8b-3t-4k) |The Nvidia research that previously showcased the hybrid basic Mamba model is now available. |

## Perspectives
|Link|description|
|---|---|
|[Computer says yes: how AI is changing our romantic lives.](https://www.theguardian.com/technology/article/2024/jun/16/computer-says-yes-how-ai-is-changing-our-romantic-lives) | Artificial intelligence is creating companions who can be our confidants, friends, therapists and even lovers. But are they an answer to loneliness or merely another way for big tech to make money?|
|[Nvidia’s New Sales Booster: The Global Push for National AI Champions.](https://www.wsj.com/tech/ai/nvidias-new-sales-booster-the-global-push-for-domestic-ai-champions-6d005ab7) |Governments everywhere are increasing their spending to entice corporations and multinationals to construct new data centers and renovate existing ones so that AI can be developed locally and massive language models can be trained in the original languages using data from their own inhabitants. According to Nvidia, these independent AI initiatives should generate over $10 billion in revenue this year. The potential economic effects of generative AI are a source of concern for several governments. For their sensitive data and AI infrastructure, they want sovereign clouds, and US IT companies are happy to construct them for them. |
|[General Intelligence (2024).](https://nonint.com/2024/06/03/general-intelligence-2024/) | What is lacking and what would it take to create a generally intelligent agent? This essay suggests that we will be here in a few years and examines the three concepts required to create an agent. The writer is an OpenAI researcher.|
|[Human neuroscience is entering a new era — it mustn’t forget its human dimension.](https://www.nature.com/articles/d41586-024-02022-3) | The field is taking a leap forward thanks to innovative technologies, such as artificial intelligence. Researchers must improve consent procedures and public involvement.|
|[AI and Euro 2024: VAR is shaking up football — and it’s not going away.](https://www.nature.com/articles/d41586-024-01764-4) |Sports physicist Eric Goff explains how updates to the technology can help referees to make the toughest calls. |
|[How cutting-edge computer chips are speeding up the AI revolution.](https://www.nature.com/articles/d41586-024-01544-0) |Engineers are harnessing the powers of graphics processing units (GPUs) and more, with a bevy of tricks to meet the computational demands of artificial intelligence. |
|[Apple’s Intelligent Strategy.](https://sdw.space/apples-intelligent-strategy/) | Apple showed off an incredible strategic edge in the AI arms race - but some might have missed that the company hints at using its biggest weakness as a formidable weapon against competitors.|
|[How to Fix “AI’s Original Sin”.](https://www.oreilly.com/radar/how-to-fix-ais-original-sin/) |The copyright issues raised by AI models trained on protected content without authorization are discussed in this article. It advises AI developers to adhere to copyright signals, put in place safeguards to stop producing content that violates intellectual property rights, and design business plans that guarantee just compensation for content creators. These strategies include retrieval-augmented generation (RAG) and the development of collaborative AI content ecosystems. |
|[Takeaways from OpenAI and Google's May announcements.](https://www.tanayj.com/p/takeaways-from-openai-and-googles) |With the introduction of sophisticated AI models by OpenAI and Google, real-time multimodal understanding and answers are now possible, and enhanced AI assistants and advancements in speech agents are promised. Google's Gemini 1.5 Flash offers a notable reduction in latency and cost, while OpenAI's GPT-4o promises double the speed and half the cost of its predecessor. Both digital behemoths are incorporating AI into their ecosystems, with OpenAI focusing on consumer markets with partnerships and products that could potentially reach up to a billion consumers. |
|[Collection of AI Side Business Money-Making Information.](https://github.com/bleedline/aimoneyhunter/blob/main/README_en.md) | There are some respectable AI projects on this list that even beginners can work on.|
|[paramount.](https://github.com/ask-fini/paramount) |Paramount lets your expert agents evaluate AI chats |
























