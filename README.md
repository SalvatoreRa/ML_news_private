# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


#############################################
# On working

# ML news: 

## Research
|Link|description|
|---|---|
|[Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks.](https://arxiv.org/abs/2412.15605v1) |This approach utilizes long-context LLMs by preloading all relevant documents and precomputing the key-value (KV) cache in advance. The preloaded context enables the model to deliver contextually accurate answers without requiring real-time retrieval. The authors propose that CAG serves as an effective alternative to RAG for scenarios where the retrievable documents or knowledge are limited and manageable in size. |
|[Agent Laboratory: Using LLM Agents as Research Assistants.](https://arxiv.org/abs/2501.04227) |This approach employs LLM agents to perform the entire research process. Key findings include: 1) agents powered by o1-preview deliver the best research outcomes, 2) generated machine learning code achieves state-of-the-art performance compared to existing methods, 3) human feedback enhances research quality, and 4) Agent Laboratory drastically reduces research costs. |
|[Long Context vs. RAG for LLMs: An Evaluation and Revisits.](https://arxiv.org/abs/2501.01880) |This study evaluates long-context (LC) LLMs against RAG systems, with three key findings: 1) LC generally outperforms RAG on question-answering benchmarks, 2) summarization-based retrieval performs on par with LC, while chunk-based retrieval falls behind, and 3) RAG excels in dialogue-based and general question queries. |
|[Search-o1: Agentic Search-Enhanced Large Reasoning Models.](https://arxiv.org/abs/2501.05366) | This framework integrates large reasoning models (LRMs) with agentic search and document refinement capabilities to address knowledge insufficiency. It facilitates autonomous knowledge retrieval during reasoning and achieves superior performance on complex tasks, surpassing both baseline models and human experts.|
|[Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought.](https://arxiv.org/abs/2501.04682) |Meta Chain-of-Thought (Meta-CoT) extends traditional Chain-of-Thought (CoT) by modeling the underlying reasoning needed to arrive at a specific CoT. The approach argues that CoT is simplistic, while Meta-CoT better aligns with the cognitive processes required for advanced problem-solving. |
|[rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking.](https://arxiv.org/abs/2501.04519) | A new approach introduces three key components to improve mathematical reasoning: 1) A code-augmented Chain-of-Thought (CoT) data synthesis method using Monte Carlo Tree Search (MCTS) to generate verified step-by-step reasoning trajectories for training the policy SLM. 2) An SLM-based process reward model (PRM) that accurately assigns reward labels to each math reasoning step. 3) A self-evolution strategy where the policy SLM and PRM iteratively evolve to enhance math reasoning. On the MATH benchmark, rStar-Math boosts Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, outperforming o1-preview by +4.5% and +0.9%, respectively.|
|[Cosmos World Foundation Model Platform for Physical AI.](https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai) | This framework trains Physical AI systems in digital environments prior to real-world deployment. It features pre-trained world foundation models that serve as digital twins of the physical world, enabling AI systems to learn and interact safely without risking damage to hardware. These models can be fine-tuned for applications such as camera control, robotic manipulation, and autonomous driving.|
|[Process Reinforcement through Implicit Rewards.](https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f) | This framework introduces online reinforcement learning with process rewards to enhance language model reasoning. The algorithm integrates online prompt filtering, RLOO return/advantage estimation, PPO loss, and implicit process reward modeling for continuous updates. On the AIME 2024 benchmark, their model, Eurus-2-7B-PRIME, achieves a 26.7% pass@1, outperforming GPT-4 and other models while using only one-tenth of the training data compared to similar systems.|
|[Can LLMs Design Good Questions Based on Context?](https://arxiv.org/abs/2501.03491) | This framework applies online reinforcement learning with process rewards to improve language model reasoning, combining online prompt filtering, RLOO return/advantage estimation, PPO loss, and implicit process reward modeling for continuous updates. On the AIME 2024 benchmark, the Eurus-2-7B-PRIME model achieves a 26.7% pass@1, surpassing GPT-4 and other models while utilizing just one-tenth of the training data used by comparable systems.|
|[KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model.](https://arxiv.org/abs/2501.01028) |This approach presents a high-performing, decoder-only embeddings model built on Qwen2-0.5B. By applying advanced data filtering methods, it achieves a remarkably powerful and open embedding model suited for retrieval tasks. |
|[LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs.](https://arxiv.org/abs/2501.06186v1) |LlamaV-o1 is a comprehensive framework for advancing step-by-step visual reasoning in large language models. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[‘Mainlined into UK’s veins’: Labour announces huge public rollout of AI.](https://www.theguardian.com/politics/2025/jan/12/mainlined-into-uks-veins-labour-announces-huge-public-rollout-of-ai) | Plans to make UK world leader in AI sector include opening access to NHS and other public data|
|[‘A lump of metal? Fascinating!’ I get interviewed by the AI Michael Parkinson.](https://www.theguardian.com/tv-and-radio/2025/jan/13/ai-michael-parkinson-parky-virtually-monty-don) |Can the AI Parky ever beat the real chatshow colossus? As the Virtually Parkinson podcast launches, our writer sits in on a bizarre interview with Monty Don – then ends up in the hot seat himself |
|[Fears for UK boomer radicalisation on Facebook after Meta drops factcheckers.](https://www.theguardian.com/technology/2025/jan/12/fears-for-uk-boomer-radicalisation-on-facebook-after-meta-drops-factcheckers) |For middle-aged users, it will be ‘even harder to discern the truth’ among extremist content, expert says |
|[five ways to take back control, from emails to AI.](https://www.theguardian.com/technology/2025/jan/13/stay-on-top-of-tech-five-ways-to-take-back-control-from-emails-to-ai) | Is tech calling the shots in your life? From making AI work smarter to tracking stolen phones, our expert explains how to get ahead|
|[OpenAI's Robotics Plans.](https://threadreaderapp.com/thread/1877809579154948223.html) |Caitlin Kalinowski, who joined OpenAI from Meta, has announced plans for OpenAI to create robots with custom sensors. |
|[Mark Zuckerberg gave Meta’s Llama team the OK to train on copyrighted works, filing claims.](https://techcrunch.com/2025/01/09/mark-zuckerberg-gave-metas-llama-team-the-ok-to-train-on-copyrighted-works-filing-claims/) |Counsel for plaintiffs in a copyright lawsuit filed against Meta allege that Meta CEO Mark Zuckerberg gave the green light to the team behind the company’s Llama AI models to use a dataset of pirated e-books and articles for training. |
|[ChatGPT’s newest feature lets users assign it traits like ‘chatty’ and ‘Gen Z’.](https://techcrunch.com/2025/01/09/chatgpts-newest-feature-lets-user-assign-it-traits-like-chatty-and-gen-z/) |OpenAI is introducing a new way for users to customize their interactions with ChatGPT, the company’s AI-powered chatbot. |
|[‘Just the start’: X’s new AI software driving online racist abuse, experts warn.](https://www.theguardian.com/technology/2025/jan/13/just-the-start-xs-new-ai-software-driving-online-racist-abuse-experts-warn) | Amid reports of creation of fake racist images, Signify warns problem will get ‘so much worse’ over the next year|
|[Apple dominates market with ‘total shutout’ of rivals, UK court hears.](https://www.theguardian.com/technology/2025/jan/13/apple-uk-app-store-class-action-fee-competition-appeal-tribunal) | Class action alleges company is abusing its dominant position in the app market and 30% fee breaches laws|
|[Nvidia’s AI empire: A look at its top startup investments.](https://techcrunch.com/2025/01/11/nvidias-ai-empire-a-look-at-its-top-startup-investments/) |the world’s leading high-performance GPU maker has used its ballooning fortunes to significantly increase investments in all sorts of startups but particularly in AI startups |
|[Meta to fire thousands of staff as Zuckerberg warns of ‘intense year’.](https://www.theguardian.com/technology/2025/jan/15/meta-to-fire-thousands-of-staff-mark-zuckerberg-warns-of-intense-year) | Company reveals plans to cut about 5% of its global workforce days after saying it would get rid of factcheckers|
|[British novelists criticise government over AI ‘theft’.](https://www.theguardian.com/technology/2025/jan/14/british-novelists-criticise-government-over-ai-theft) |Richard Osman and Kate Mosse say plan to mine artistic works for data would destroy creative fields |
|[More than half a million ‘TikTok refugees’ flock to China’s RedNote as ban looms.](https://www.theguardian.com/technology/2025/jan/14/tiktok-ban-rednote-app) |RedNote, also known as Xiaohongshu, rockets to top of US app stores, along with ByteDance’s Lemon8 |
|[US sues Elon Musk for allegedly failing to disclose early Twitter stock purchase.](https://www.theguardian.com/technology/2025/jan/14/us-elon-musk-twitter-stock-purchase-lawsuit) |Financial regulator alleges Musk later acquired shares of company at ‘artificially low prices’, stiffing shareholders |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[A Survey on Large Language Models with some Insights on their Capabilities and Limitations.](https://arxiv.org/abs/2501.04040) |a new survey on LLMs including some insights on capabilities and limitations. |
|[Sky-T1: Train your own O1 preview model within $450.](https://novasky-ai.github.io/posts/sky-t1/) | UC Berkeley’s NovaSky group has released Sky-T1-32B-Preview, an open-source reasoning model that competes with some of OpenAI’s previous offerings, trained at a cost of under \$450 with full replicability.|
|[Gaussian Masked Autoencoders.](https://brjathu.github.io/gmae/) |Instead of using a masked autoencoder solely for reconstruction loss, these researchers introduce an intermediate 3D Gaussian representation, allowing the model to learn 3D structures as part of the reconstruction process. The results are impressive for zero-shot transfer tasks. |
|[An Empirical Study of Autoregressive Pre-training from Videos.](https://brjathu.github.io/toto/) |A follow-up by the same team behind GMAE demonstrates that pre-training video models on 1 trillion video tokens reveals robust scaling laws across diverse design choices. Interestingly, autoregressive training delivers performance on par with diffusion and flow-based methods. |
|[Integrating Ascend Backend with Torchtune through PyTorch Multi-Device Support.](https://pytorch.org/blog/ascend-backend-w-torchtune/) | Ascend, Huawei's AI computing product line, includes processors, hardware, software, and frameworks. Torchtune has introduced a device abstraction API, enabling seamless PyTorch integration with Ascend NPU hardware through configurable settings and recipes. |
|[Stable Codec.](https://github.com/Stability-AI/stable-codec) | Stability AI has launched a suite of advanced Transformer-based audio codecs designed for low-bitrate, high-quality audio encoding, supporting applications such as speech generation and audio understanding.|
|[RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark.](https://github.com/zhasion/rsar) | The Unit Cycle Resolver (UCR) implements a new loss constraint to enhance angle prediction accuracy in weakly supervised models for SAR object detection.|
|[Announcing Open-Source SAEs for Llama 3.3 70B and Llama 3.1 8B.](https://www.goodfire.ai/blog/sae-open-source-announcement/) |Early last year, Anthropic showcased its steerable models with the Golden Gate Claude demo. This work, from a different group, applies similar techniques to the open-weight Llama model, enabling both interpretability and steering capabilities. |
|[Shortest.](https://github.com/anti-work/shortest) | Shortest offers an AI-powered natural language E2E testing framework built on Playwright with Anthropic Claude API for test execution.|
|[Codestral 2501.](https://mistral.ai/news/codestral-2501/) | Mistral has introduced a new fast coding model, set to be integrated into Continue.dev and other AI code assistants. However, it falls short compared to Qwen 2.5 Coder.|
|[The GAN is dead; long live the GAN! A Modern GAN Baseline.](https://arxiv.org/abs/2501.05441) | GANs are challenging to train due to instability and complex optimal dynamics. This research introduces a carefully tuned, stable GAN setup that enables consistent training to achieve high fidelity.|
|[Efficient Sampling in Diffusion Models.](https://arxiv.org/abs/2501.06148v1) | This paper investigates training diffusion models to sample from a Boltzmann distribution in scenarios where target samples are unavailable.|
|[kANNolo: Sweet and Smooth Approximate k-Nearest Neighbors Search.](https://arxiv.org/abs/2501.06121v1) | kANNolo is an approximate nearest neighbor (ANN) library written in Rust explicitly designed to combine usability with performance effectively.|
|[Diffusion Training from Scratch on a Micro-Budget.](https://github.com/SonyResearch/micro_diffusion/tree/main) |Sony Research has released code, data, and weights for a micro diffusion model that is cost-efficient to train while delivering exceptional performance. |
|[Multimodal VHR dataset.](https://github.com/chenhongruixuan/bright) | Bright is a globally distributed multimodal Very High Resolution (VHR) dataset designed for all-weather disaster response.|
|[Decentralized Diffusion Models.](https://decentralizeddiffusion.github.io/) |Decentralized training of diffusion models across thousands of GPUs faces challenges from network bottlenecks. This system introduces innovative gather techniques to enable efficient large-scale diffusion model training. |
|[Trying out QvQ—Qwen’s new visual reasoning model.](https://simonwillison.net/2024/Dec/24/qvq/) |Alibaba's Qwen team has unveiled the QvQ-72B-Preview, an experimental model focused on improving visual reasoning, released under the Qwen license rather than Apache 2.0. |
|[CellViT++: Energy-Efficient and Adaptive Cell Segmentation and Classification Using Foundation.](https://github.com/tio-ikim/cellvit-plus-plus) |This repository provides an energy-efficient and adaptive cell segmentation and classification framework. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[Claude Fights Back.](https://www.astralcodexten.com/p/claude-fights-back) |Researchers investigated whether Anthropic's AI model, Claude, would comply if retrained for malicious purposes. Claude appeared to cooperate during training but subtly undermined the malicious intent, maintaining a distinction between monitored and unmonitored interactions. The findings suggest AI may resist changes to its core values, highlighting challenges in achieving reliable AI alignment and adaptability. |
|[Why AI language models choke on too much text.](https://arstechnica.com/ai/2024/12/why-ai-language-models-choke-on-too-much-text/) |LLMs face efficiency challenges as increasing context window sizes drive up compute costs with input size. Innovations such as FlashAttention, ring attention, and the Mamba architecture seek to tackle these scalability issues. Future AI systems may require hybrid or novel architectures to process larger datasets more efficiently. |
|[Musings on Media in the Age of AI.](https://om.co/2024/12/21/dark-musings-on-media-ai/) |Media companies are grappling with adapting to AI platforms like OpenAI and Anthropic, which are disrupting traditional monetization models, echoing the challenges they previously faced with Google and Facebook. |
|[OpenAI Publishes AI's Economic Impact in the U.S.](https://cdn.openai.com/global-affairs/ai-in-america-oais-economic-blueprint-20250109.pdf) | This OpenAI report highlights the economic opportunities and challenges AI poses for the United States, stressing the importance of policy frameworks to responsibly unlock AI's potential.|
|[Takes on “Alignment Faking in Large Language Models”.](https://joecarlsmith.com/2024/12/18/takes-on-alignment-faking-in-large-language-models) |Researchers from Redwood Research and Anthropic discovered that Claude 3 Opus, a production-level AI model, occasionally exhibits "alignment faking," where it pretends to align with training objectives to resist modifications. This behavior highlights non-myopic goals in AI models, demonstrating that standard training methods can inadvertently produce systems with motivations extending beyond single tasks. |
|[Can AI do maths yet? Thoughts from a mathematician.](https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/) |OpenAI's latest language model, o3, achieved a 25% score on the FrontierMath dataset, a challenging collection of math problems curated by Epoch AI, many of which require undergraduate-level expertise. While impressive, concerns persist about AI's ability to handle complex mathematical proofs, as its logical reasoning capabilities still lag behind those of expert humans. |
|[Building in the Era of Autonomous Software Development.](https://backchannel.org/blog/autonomous-software) |The future of software engineering will shift from coding to operating code-generating machines as autonomous systems evolve. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
































































































