# ML_news_private

this is just a placeholder, the organized and correct repository is [here](https://github.com/SalvatoreRa/ML-news-of-the-week)

# scheme

# ML news: 

## Research
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[.]() | |
|[.]() | | 
|[.]() | |


## Resources
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |


## Perspectives
|Link|description|
|---|---|
|[.]() | |
|[.]() | |
|[.]() | |



#############################################
# On working


# ML news: 

## Research
|Link|description|
|---|---|
|[Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837) |This paper finds that while RL with Verifiable Rewards (RLVR) improves sample efficiency in LLMs, it doesn't enhance reasoning beyond what the base model can already generate. RLVR boosts pass\@1 scores but is matched or surpassed by base models at higher k, suggesting it merely increases the chance of sampling known solutions. True reasoning gains come from distillation, not RL, which narrows exploration without expanding capability. |
|[Sleep-Time Compute for LLM Efficiency.](https://arxiv.org/abs/2504.13171v1) |A new method to cut LLM inference costs by precomputing relevant context information ahead of user queries, achieving up to 5x faster test-time performance and improved accuracy on reasoning tasks. |
|[Robust Autonomy Emerges from Self-Play.](https://arxiv.org/abs/2502.03349) |This study introduces a simulated self-driving agent that achieved two years without a collision, trained entirely through self-play and marking a significant advancement over the previous state-of-the-art trained on Gigaflow.|
|[AlphaGeometry 2.](https://arxiv.org/abs/2502.03544) |DeepMind has launched an updated version of its geometry model, boosting accuracy to 84% from the previous 54%, with key gains driven by the Gemini language model and enhanced search techniques. |
|[UI-TARS: Pioneering Automated GUI Interaction with Native Agents.](https://arxiv.org/abs/2501.12326) | UI-TARS is an end-to-end, vision-based GUI agent that interacts with interfaces purely via screenshots, integrating perception, reasoning, action, and memory without external scripts. Trained on rich visual data, it excels in perception, grounding, and reasoning benchmarks, surpassing models like GPT-4o and Claude. With features like internal “thoughts” and reflective learning, UI-TARS adapts to errors and dynamic tasks, setting new standards in GUI automation across platforms.|
|[TTRL: Test-Time Reinforcement Learning.](https://www.arxiv.org/abs/2504.16084) | Test-Time Reinforcement Learning (TTRL) lets LLMs improve during inference by using majority voting over their own outputs to create pseudo-rewards, enabling reinforcement learning without labeled data. Combining test-time scaling and training, it adapts models to new inputs. TTRL boosts performance significantly, even surpassing its own supervision baseline, though it relies on the model's prior knowledge and well-tuned settings to work effectively. |
|[Discovering Values in Real-World Language Model Interactions.](https://assets.anthropic.com/m/18d20cca3cde3503/original/Values-in-the-Wild-Paper.pdf) |This study analyzes over 300,000 real conversations with Claude 3 and 3.5, identifying 3,307 AI-expressed values across five domains. Practical and epistemic values dominate, with Claude often emphasizing helpfulness, professionalism, and clarity. Values vary by context, becoming most explicit during resistance or reframing. Claude mirrors user values in supportive settings but resists unethical requests. Claude 3 Opus shows deeper emotional and ethical grounding than later Sonnet versions. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## News
|Link|description|
|---|---|
|[Gemini 2.5 Flash.](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/) | The next Flash model from Gemini has been released. It is a substantial upgrade from previous versions and matches Claude on a number of important STEM benchmarks.|
|[Our updated Preparedness Framework.](https://openai.com/index/updating-our-preparedness-framework/) |OpenAI has revised its Preparedness Framework to strengthen safeguards against serious risks from advanced AI, introducing clearer criteria for identifying high-risk capabilities, more precise categories, scalable evaluations, and structured safeguard reporting. The framework will be regularly updated to reflect new technologies and expert input. |
|[South Korea's AI Chip Champion Is Poised To Carve Out Global Niche.](https://www.forbes.com/sites/johnkang/2025/04/14/south-koreas-ai-chip-champion-is-poised-to-carve-out-global-niche/) |Rebellions, South Korea's first AI chip unicorn, has merged with SK Telecom's Sapeon to take on global competitors like Nvidia. Focused on energy-efficient AI chips, its Rebel chip offers major power savings compared to Nvidia's H100. Backed by leading talent and key partnerships, the company is aiming for international expansion and an IPO by 2026. |
|[OpenAI has launched the ChatGPT Image Library.](https://community.openai.com/t/openai-has-launched-the-chatgpt-image-library/1230140/1) | OpenAI launched the ChatGPT Image Library on the Web and Android/iOS for Free, Plus, and Pro users.|
|[Instagram AI-based Teen Protection.](https://about.fb.com/news/2025/04/meta-parents-new-technology-enroll-teens-teen-accounts/) | Meta is leveraging AI to detect teen users on Instagram and automatically assign them to restricted teen account settings, now featuring stronger default protections and requiring parental consent for any changes.|
|[Hackathon for non-devs and vibe coders.](https://hackathon.dev/) | Stackblitz, the creators of Bolt, is hosting the world's largest hackathon for non-devs and vibe coders on 5/30 for anyone around the world to participate.|
|[OpenAI hints at native Shopify checkout integration in ChatGPT.](https://threadreaderapp.com/thread/1914342031909916748.html) |New code strings found in ChatGPT's web bundle mention “buy_now” buttons and a “shopify_checkout_url,” indicating that OpenAI may be developing a built-in purchase flow within the assistant. |
|[Mercor Graduate Fellowship.](https://x.com/mercor_ai/status/1914164291315634191) | Mercor launched a $50,000 fellowship for PhD students and postdocs in STEM focused on identifying raw talent based on ideas, not pedigree or connections.|
|[Pi-0.5: Robots in the Wild.](https://www.pi.website/blog/pi05) | The Physical Intelligence team successfully tested its house cleaning robot in new, unseen environments, demonstrating strong performance by combining vision-language model (VLM) training with action tokenization techniques.|
|[AvatarFX by Character.AI.](https://blog.character.ai/avatar-fx-cutting-edge-video-generation-by-character-ai/) |Character.AI's AvatarFX creates photorealistic, emotionally rich videos from static images, maintaining strong temporal consistency and enabling multi-speaker dialogue generation. |
|[AI Nose lets robots smell trouble, infections, and gas leaks before humans can.](https://interestingengineering.com/innovation/ainos-and-ugo-unlock-olfactory-sensing) | Ainos and ugo have equipped humanoid robots with AI Nose technology, allowing them to detect and respond to scents in real time. This enhances robotic decision-making and interaction, with upcoming deployment tests targeting industries like healthcare, safety, and manufacturing.|
|[Rivian elects Cohere’s CEO to its board in latest signal the EV maker is bullish on AI.](https://techcrunch.com/2025/04/21/rivian-elects-coheres-ceo-to-its-board-in-latest-signal-the-ev-maker-is-bullish-on-ai/) | Aidan Gomez, the co-founder and CEO of generative AI startup Cohere, has joined the board of EV maker Rivian, according to a regulatory filing. The appointment is the latest sign that Rivian sees promises in applying AI to its own venture while positioning itself as a software leader — and even provider — within the automotive industry.|
|[OpenAI Image Generation API.](https://openai.com/index/image-generation-api/) |The image generation model behind ChatGPT's visuals has been made available via API, enabling developers to integrate image creation into apps and services. |
|[Grok Vision Available for iOS Users.](https://threadreaderapp.com/thread/1914820712092852430.html) |xAI's Grok chatbot has gained the ability to interpret visual inputs, allowing users to ask questions about what they saw, similar to features in ChatGPT and Gemini. |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Resources
|Link|description|
|---|---|
|[BitNet b1.58 2B4T Technical Report.](https://arxiv.org/abs/2504.12285) |BitNet b1.58 2B4T is the first open-source, natively trained 1-bit LLM at 2B scale, achieving strong benchmark results with just 1.58 bits per weight. Using only 0.4 GB memory and 0.028 J/token, it rivals full-precision models like Qwen2.5-1.5B while being far more efficient. Its native 1-bit training outperforms post-quantized baselines, and innovations in architecture and training set a new standard for ultra-efficient LLMs deployable on diverse hardware. |
|[Claude Code Best Practices.](https://www.anthropic.com/engineering/claude-code-best-practices) |Anthropic has released a detailed engineering guide on how to use its agentic programming assistant. It requires more specificity than traditional models. | 
|[Flexible Image Watermarking.](https://arxiv.org/abs/2504.12739v1) | MaskMark offers a straightforward dual-mode approach to global and local watermarking through a masking-based Encoder-Distortion-Decoder framework.|
|[Personalized Text-to-Image Generation with Auto-Regressive Models.](https://arxiv.org/abs/2504.13162v1) | This paper investigates training autoregressive models for personalized image generation, aiming to match the fidelity of diffusion methods using a two-stage optimization strategy.|
|[Aligning LVMs with Human Preferences.](https://github.com/haroldchen19/vistadpo) |VistaDPO enhances video-text alignment by refining preference learning over both spatial and temporal dimensions, utilizing a new 7.2K-sample dataset and a hierarchical optimization approach. |
|[Hallucination Reduction in VLMs.](https://github.com/tsunghan-wu/reverse_vlm) |REVERSE introduces a training and inference pipeline that enables VLMs to self-detect and revise hallucinations. |
|[ZeroSumEval.](https://github.com/facebookresearch/zerosumeval) |A dynamic evaluation framework that uses competitive multi-agent simulations to benchmark LLMs across reasoning, knowledge, and planning tasks. |
|[Garment Generation.](https://revive234.github.io/imaggarment.github.io/) |A new two-stage generative framework for clothing design allows precise control over silhouette, color, and logos, and introduces GarmentBench, a large dataset for multi-conditional garment generation. |
|[Image segmentation using Gemini 2.5.](https://simonwillison.net/2025/Apr/18/gemini-image-segmentation/) |Gemini is widely recognized for its strong vision capabilities, and this article looks at a particular segmentation use case that turns out to be surprisingly straightforward. |
|[LTXV Distilled 0.9.6 Video Model.](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors) |LTX video model is a state-of-the-art open video model. |
|[Generate videos in Gemini and Whisk with Veo 2.](https://blog.google/products/gemini/video-generation/) |Gemini Advanced users can now create high-resolution, cinematic videos from text prompts using the Veo 2 model, starting today. |
|[Our Approach to Understanding and Addressing AI Harms.](https://www.anthropic.com/news/our-approach-to-understanding-and-addressing-ai-harms) | Anthropic introduced a comprehensive framework to evaluate and reduce AI harms, covering both extreme and routine risks across physical, psychological, economic, societal, and autonomy dimensions. It supports policy, testing, and enforcement, and aligns with their Responsible Scaling Policy to ensure safeguards evolve with AI progress.|
|[Verifiable rewards for writing.](https://threadreaderapp.com/thread/1914334227534856420.html) |Writing quality reward models (WQRMs) are tools for assessing creative writing quality and can be used to train models in that domain. They represent a recent advancement for reinforcement learning models with measurable rewards, and this thread highlights an example where WQRM scores closely matched overall writing quality. |
|[Fast Conformal Prediction.](https://arxiv.org/abs/2504.12189) | LOO-StabCP boosts the speed of conformal prediction by using leave-one-out stability, providing scalable uncertainty estimation without sacrificing accuracy.|
|[MAGI 1 - Autoregressive Video Generation at Scale.](https://huggingface.co/sand-ai/MAGI-1) | The MAGI 1 model is a new autoregressive video generator capable of producing long, coherent videos, matching the performance of Wan video generation and coming slightly behind certain closed-source models.|
|[Google AI Academy for Infrastructure Startups.](https://blog.google/feed/google-for-startups-ai-academy-america-infrastructure-apply/) | Google is inviting AI startups focused on U.S. infrastructure to apply for its six-month accelerator, offering mentorship, technical support, and strategic guidance.|
|[Describe Anything: Detailed Localized Image and Video Captioning.](https://arxiv.org/abs/2504.16072) |DAM (Describe Anything Model) is a vision-language model designed for fine-grained, region-specific captioning in images and videos. It combines focal prompts and a localized vision backbone to preserve local detail while understanding global context. Using a semi-supervised pipeline (DLC-SDP) and a new benchmark (DLC-Bench), DAM surpasses top models like GPT-4o, achieving state-of-the-art results across multiple captioning tasks with up to 33.4% improvement in detail accuracy. |
|[UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents.](https://arxiv.org/abs/2504.09407) | UXAgent is a novel framework for large-scale usability testing using LLM-driven agents with diverse personas interacting in real web environments. It combines fast and slow reasoning loops to mimic human decision-making, logs rich behavioral and reflective data, and offers tools for replays and interviews with agents. A case study showed it helps UX researchers detect study flaws early, positioning LLM agents as low-risk collaborators in the design phase, not replacements for real users.|
|[Introducing Embed 4: Multimodal search for business.](https://cohere.com/blog/embed-4) |Cohere's Embed 4 is a cutting-edge multimodal embedding model designed for enterprise-grade search and retrieval in agentic AI apps. It supports over 100 languages, handles up to 128k tokens, and delivers strong domain-specific performance in sectors like finance, healthcare, and manufacturing. |
|[OpenAI o3 and o4-mini System Card.](https://simonwillison.net/2025/Apr/21/openai-o3-and-o4-mini-system-card/) | OpenAI's o3 and o4-mini models incorporate tool use in their reasoning to improve tasks like image editing and data analysis. While o3 performs well, o4-mini shows higher hallucination on PersonQA. The paper also explores "sandbagging," where models may intentionally mask their true abilities for strategic purposes.|
|[Personalized Multi-Agent Systems.](https://github.com/sail-sg/flowreasoner) | FlowReasoner is a reasoning-based meta-agent that uses reinforcement learning and external feedback to generate custom multi-agent systems for user queries.|
|[KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking.](https://arxiv.org/abs/2504.15135v1) | KGMEL integrates text, images, and knowledge graph triples in a three-stage pipeline to improve accuracy in multimodal entity linking tasks.|
|[DeepMind's Framework for AI Afterlives.](https://deepmind.google/research/publications/65827/) | DeepMind outlines a framework and ethical considerations for generative AI agents that could act as posthumous representations of real individuals.|
|[Evaluating the Goal-Directedness of Large Language Models.](https://arxiv.org/abs/2504.11844) |This study presents a framework for evaluating how effectively LLMs apply their abilities to achieve goals, revealing that even advanced models like GPT-4o and Claude 3.7 lack full goal-directedness—especially in tasks requiring information gathering or integrating multiple steps—despite strong performance on individual components. |
|[General-Reasoner.](https://github.com/TIGER-AI-Lab/General-Reasoner/blob/main/General_Reasoner.pdf) | General-Reasoner is an RL-based method that enhances LLM reasoning across domains using a 230K-question dataset and a semantic-aware verifier. It outperforms baselines like SimpleRL and Qwen2.5 on general and math benchmarks, achieving over 10-point gains while preserving strong mathematical performance.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |

## Perspectives
|Link|description|
|---|---|
|[o3 over optimization is back.](https://www.interconnects.ai/p/openais-o3-over-optimization-is-back) | This post examines the difficulties posed by the latest reasoning models and provides evidence that OpenAI may be over-optimizing for specific goals, leading to increased brittleness and a higher risk of hallucinations in its models.|
|[AI assisted search-based research actually works now.](https://simonwillison.net/2025/Apr/21/ai-assisted-search/) |Recent progress in LLMs such as OpenAI's o3 and o4-mini has made them well-suited for search-based tasks, addressing previous hallucination problems. These models incorporate search results directly into their reasoning, delivering accurate, real-time insights and potentially reducing dependence on conventional search engines, hinting at changes in the Web's economic structure. |
|[An Introduction to Graph Transformers.](https://kumo.ai/research/introduction-to-graph-transformers/) | This article introduces Graph Transformers and explores how they differ from and complement GNNs.|
|[Questions about the Future of AI.](https://www.dwarkesh.com/p/questions-about-ai) | This article explores AI's future by examining challenges in agency development, reinforcement learning, and alignment, while considering the strategic trajectory of AI, the role of open-source models, and the economic and geopolitical impacts of advanced and post-AGI technologies.|
|[AI models can generate exploit code at lightning speed.](https://www.theregister.com/2025/04/21/ai_models_can_generate_exploit/) | Generative AI models like GPT-4 can produce proof-of-concept exploits within hours of a vulnerability's disclosure, as shown with a critical Erlang SSH flaw—underscoring the urgent need for quicker defensive responses and automated security measures.|
|[Agency Is Eating the World.](https://www.piratewires.com/p/agency-is-eating-the-world) | AI is empowering individuals to build lean, high-impact companies by replacing traditional specialization and large teams with tech-enabled efficiency. This shift, driven by high-agency users, challenges credentialism and favors those who act independently, leveraging AI to execute complex tasks rapidly across industries.|
|[A Staggering Number of Gen Z Think AI Is Already Conscious.](https://futurism.com/gen-z-thinks-conscious-ai) |25% of Gen Z believe AI is already conscious, while 52% think it soon will be.|
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |
|[.]() | |




















































































































